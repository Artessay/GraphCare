{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling direct ehr nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9717/9717 [00:00<00:00, 14781.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset...\n",
      "Getting embedding...\n"
     ]
    }
   ],
   "source": [
    "from graphcare import *\n",
    "\n",
    "kg = \"GPT-KG\"\n",
    "dataset = \"mimic3\"\n",
    "task = \"mortality\"\n",
    "\n",
    "# load dataset\n",
    "sample_dataset, G, ent2id, rel2id, ent_emb, rel_emb, \\\n",
    "            map_cluster, map_cluster_inv, map_cluster_rel, map_cluster_rel_inv, \\\n",
    "                ccscm_id2clus, ccsproc_id2clus, atc3_id2clus = load_everything(dataset, task, kg)\n",
    "\n",
    "# label direct ehr node\n",
    "print(\"Labeling direct ehr nodes...\")\n",
    "sample_dataset = label_ehr_nodes(task, sample_dataset, len(map_cluster), ccscm_id2clus, ccsproc_id2clus, atc3_id2clus)\n",
    "print(\"Splitting dataset...\")\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)\n",
    "G_tg = from_networkx(G)\n",
    "\n",
    "# get embedding\n",
    "print(\"Getting embedding...\")\n",
    "rel_emb = get_rel_emb(map_cluster_rel)\n",
    "node_emb = G_tg.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 4598, 4598, 4598],\n",
       "        [ 275, 1997,    0,  ..., 3388, 3924, 4283]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_tg.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(G=G_tg, dataset=train_dataset, task=task)\n",
    "val_set = Dataset(G=G_tg, dataset=val_dataset, task=task)\n",
    "test_set = Dataset(G=G_tg, dataset=test_dataset, task=task)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4599, 1536])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "node_embedding = nn.Embedding.from_pretrained(node_emb, freeze=False)\n",
    "relation_embedding = nn.Embedding.from_pretrained(rel_emb, freeze=False)\n",
    "alpha_attn = nn.Linear(node_emb.shape[0], node_emb.shape[0])\n",
    "beta_attn = nn.Linear(node_emb.shape[0], 1)\n",
    "leakyrelu = nn.LeakyReLU(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(4599, 1536), Embedding(1077, 1536))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embedding, relation_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[90729, 1536], edge_index=[2, 54985], y=[90729], relation=[54985], label=[64], visit_padded_node=[1792, 4599], ehr_nodes=[294336], batch=[90729], ptr=[65])\n",
      "torch.Size([54985])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Softmax\n",
    "\n",
    "for batch in train_loader:\n",
    "    # print(batch.ehr_nodes)\n",
    "    # print(node_embedding(torch.tensor(batch.ehr_nodes[0])))\n",
    "    node_ids = batch.y\n",
    "    visit_node = batch.visit_padded_node.reshape(int(train_loader.batch_size), int(len(batch.visit_padded_node)/train_loader.batch_size), batch.visit_padded_node.shape[1]).double()\n",
    "    x = node_embedding(node_ids)\n",
    "    alpha = torch.softmax((leakyrelu(alpha_attn(visit_node.float()))), dim=1)\n",
    "    beta = torch.softmax((leakyrelu(beta_attn(visit_node.float()))), dim=0)\n",
    "    j = torch.arange(visit_node.shape[1], device=x.device).float()\n",
    "    lambda_j = torch.exp(0.03 * (visit_node.shape[1] - j)).unsqueeze(0).reshape(1, visit_node.shape[1], 1)\n",
    "    attn = alpha*beta*lambda_j\n",
    "    attn = torch.sum(attn, dim=1)\n",
    "    ehr_nodes = batch.ehr_nodes.reshape(int(train_loader.batch_size), int(len(batch.ehr_nodes)/train_loader.batch_size)).float()\n",
    "    xj_batch = batch.batch[batch.edge_index[0]]\n",
    "    xj_node_ids = batch.y[batch.edge_index[0]]\n",
    "    print(batch)\n",
    "    print(attn[xj_batch, xj_node_ids].shape)\n",
    "    # print(batch.batch.shape)\n",
    "    # print(attn.shape)\n",
    "    # print(batch.batch[batch.edge_index[0]].shape)\n",
    "    # print(batch.y[batch.edge_index[0]].shape)\n",
    "    # print(ehr_nodes.shape)\n",
    "    # print(ehr_nodes[1].view(1, -1) @ node_embedding.weight / torch.sum(ehr_nodes[1]))\n",
    "    # attn = attn[batch.edge_index[0]]\n",
    "    # print(attn.shape)\n",
    "    \n",
    "    # print(x.shape)\n",
    "    # print(visit_node.shape)\n",
    "    # visit_emb = x.view(visit_node.shape).sum(dim=2) / visit_node.sum(dim=2).clamp(min=1).view(visit_node.shape[:2] + (1,))\n",
    "    # print(visit_emb.shape)\n",
    "    # print(node_emb[:4599].shape)\n",
    "    # print(visit_node.shape)\n",
    "    # print(x.shape)\n",
    "    # print((visit_node @ node_emb[:4599]).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from pyhealth.models import RETAINLayer\n",
    "from torch_geometric.nn.inits import reset\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptPairTensor,\n",
    "    OptTensor,\n",
    "    Size,\n",
    "    SparseTensor,\n",
    ")\n",
    "from torch_geometric.utils import spmm\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from torch.nn import LeakyReLU\n",
    "\n",
    "\n",
    "class BiAttentionGNNConv(MessagePassing):\n",
    "    def __init__(self, nn: torch.nn.Module, eps: float = 0.,\n",
    "                 train_eps: bool = False, edge_dim: Optional[int] = None,\n",
    "                 **kwargs):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "        self.nn = nn\n",
    "        self.initial_eps = eps\n",
    "        self.W_R = torch.nn.Linear(edge_dim, edge_dim)\n",
    "\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.nn.reset_parameters()\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "        if self.W_R is not None:\n",
    "            self.W_R.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None, attn: Tensor = None) -> Tensor:\n",
    "\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size, attn=attn)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if x_r is not None:\n",
    "            out = out + (1 + self.eps) * x_r\n",
    "\n",
    "        return self.nn(out)\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor, attn: Tensor) -> Tensor:\n",
    "\n",
    "        h_R = self.W_R(edge_attr)\n",
    "        out = (x_j * attn + h_R).relu()\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(nn={self.nn})'\n",
    "\n",
    "\n",
    "def masked_softmax(src: Tensor, mask: Tensor, dim: int = -1) -> Tensor:\n",
    "    out = src.masked_fill(~mask, float('-inf'))\n",
    "    out = torch.softmax(out, dim=dim)\n",
    "    out = out.masked_fill(~mask, 0)\n",
    "    return out\n",
    "\n",
    "class GraphCare(nn.Module):\n",
    "    def __init__(self, num_nodes, num_rels, max_visit, embedding_dim, hidden_dim, out_channels, layers=3, dropout=0.5, decay_rate=0.03, node_emb=None, rel_emb=None):\n",
    "        super(GraphCare, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "        j = torch.arange(max_visit).float()\n",
    "        self.lambda_j = torch.exp(self.decay_rate * (max_visit - j)).unsqueeze(0).reshape(1, max_visit, 1).float()\n",
    "\n",
    "        if node_emb is None:\n",
    "            self.node_emb = nn.Embedding(num_nodes, embedding_dim)\n",
    "        else:\n",
    "            self.node_emb = nn.Embedding.from_pretrained(node_emb, freeze=False)\n",
    "\n",
    "        if rel_emb is None:\n",
    "            self.rel_emb = nn.Embedding(num_rels, embedding_dim)\n",
    "        else:\n",
    "            self.rel_emb = nn.Embedding.from_pretrained(rel_emb, freeze=False)\n",
    "\n",
    "        self.lin = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.alpha_attn = nn.ModuleDict()\n",
    "        self.beta_attn = nn.ModuleDict()\n",
    "        self.conv = nn.ModuleDict()\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "        for layer in range(1, layers+1):\n",
    "            self.alpha_attn[str(layer)] = nn.Linear(num_nodes, num_nodes)\n",
    "            self.beta_attn[str(layer)] = nn.Linear(num_nodes, 1)\n",
    "            self.conv[str(layer)] = BiAttentionGNNConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=hidden_dim)\n",
    "\n",
    "        self.MLP = nn.Linear(hidden_dim * 2, out_channels)\n",
    "        \n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.lambda_j = self.lambda_j.float().to(device)\n",
    "\n",
    "\n",
    "    def forward(self, node_ids, rel_ids, edge_index, batch, visit_node, ehr_nodes):\n",
    "        x = self.node_emb(node_ids).float()\n",
    "        edge_attr = self.rel_emb(rel_ids).float()\n",
    "\n",
    "        x = self.bn1(self.lin(x))\n",
    "        edge_attr = self.bn1(self.lin(edge_attr))\n",
    "\n",
    "\n",
    "        for layer in range(1, self.layers+1):\n",
    "            alpha = masked_softmax((self.leakyrelu(self.alpha_attn[str(layer)](visit_node.float()))), mask=visit_node>1, dim=1)\n",
    "            beta = masked_softmax((self.leakyrelu(self.beta_attn[str(layer)](visit_node.float()))), mask=visit_node>1, dim=0) * self.lambda_j\n",
    "\n",
    "            attn = alpha * beta\n",
    "            attn = torch.sum(attn, dim=1)\n",
    "            xj_node_ids = node_ids[edge_index[0]]\n",
    "            xj_batch = batch[edge_index[0]]\n",
    "            attn = attn[xj_batch, xj_node_ids].reshape(-1, 1)\n",
    "\n",
    "            x = F.relu(self.conv[str(layer)](x, edge_index, edge_attr, attn=attn))\n",
    "            x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        # patient graph embedding through global mean pooling\n",
    "        x_graph = global_mean_pool(x, batch)\n",
    "        x_graph = F.dropout(x_graph, p=self.dropout, training=self.training)\n",
    "\n",
    "        # patient node embedding through local (direct EHR) mean pooling\n",
    "        x_node = torch.stack([ehr_nodes[i].view(1, -1) @ self.node_emb.weight / torch.sum(ehr_nodes[i]) for i in range(batch.max().item() + 1)])\n",
    "        x_node = self.lin(x_node).squeeze(1)\n",
    "        x_node = F.dropout(x_node, p=self.dropout, training=self.training)\n",
    "\n",
    "        # concatenate patient graph embedding and patient node embedding\n",
    "        x_concat = torch.cat((x_graph, x_node), dim=1)\n",
    "        x_concat = F.dropout(x_concat, p=self.dropout, training=self.training)\n",
    "\n",
    "        # MLP for prediction\n",
    "        logits = self.MLP(x_concat)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, jaccard_score\n",
    "    \n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    tot_loss = 0\n",
    "    pbar= tqdm(enumerate(train_loader))\n",
    "    for i, data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        node_ids = data.y\n",
    "        rel_ids = data.relation\n",
    "\n",
    "        out = model(\n",
    "                node_ids = node_ids, \n",
    "                rel_ids = rel_ids,\n",
    "                edge_index = data.edge_index,\n",
    "                batch = data.batch,\n",
    "                visit_node = data.visit_padded_node.reshape(int(train_loader.batch_size), int(len(data.visit_padded_node)/train_loader.batch_size), data.visit_padded_node.shape[1]).float(), \n",
    "                ehr_nodes = data.ehr_nodes.reshape(int(train_loader.batch_size), int(len(data.ehr_nodes)/train_loader.batch_size)).float()\n",
    "                \n",
    "            )\n",
    "        try:\n",
    "            label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "        except:\n",
    "            continue\n",
    "        # print(out.shape, label.shape)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.float())\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return tot_loss\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():    \n",
    "            \n",
    "            node_ids = data.y\n",
    "            rel_ids = data.relation\n",
    "\n",
    "            logits = model(\n",
    "                    node_ids = node_ids, \n",
    "                    rel_ids = rel_ids,\n",
    "                    edge_index = data.edge_index,\n",
    "                    batch = data.batch,\n",
    "                    visit_node = data.visit_padded_node.reshape(int(loader.batch_size), int(len(data.visit_padded_node)/loader.batch_size), data.visit_padded_node.shape[1]).float(), \n",
    "                    ehr_nodes = data.ehr_nodes.reshape(int(loader.batch_size), int(len(data.ehr_nodes)/loader.batch_size)).float()               \n",
    "                )\n",
    "\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "            try:\n",
    "                y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "            except:\n",
    "                continue\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "\n",
    "    return y_true_all, y_prob_all\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs):\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer)\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader)\n",
    "\n",
    "        y_pred_all = (y_prob_all >= 0.5).astype(int)\n",
    "        \n",
    "        val_pr_auc = average_precision_score(y_true_all, y_prob_all)\n",
    "        val_roc_auc = roc_auc_score(y_true_all, y_prob_all)\n",
    "        val_jaccard = jaccard_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_acc = accuracy_score(y_true_all, y_pred_all)\n",
    "        val_f1 = f1_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_precision = precision_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_recall = recall_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "\n",
    "        if val_acc >= best_acc and val_f1 >= best_f1:\n",
    "            # torch.save(model.state_dict(), '../../../data/pj20/exp_data/saved_weights_gin_mimic3_readmission_dynamic.pkl')\n",
    "            # print(\"best model saved\")\n",
    "            best_acc = val_acc\n",
    "            best_f1 = val_f1\n",
    "\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}, Val ROC_AUC: {val_roc_auc:.4f}, Val acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val precision: {val_precision:.4f}, Val recall: {val_recall:.4f}, Val jaccard: {val_jaccard:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0]['visit_padded_node'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GraphCare(\n",
    "    num_nodes=node_emb.shape[0],\n",
    "    num_rels=rel_emb.shape[0],\n",
    "    max_visit=sample_dataset[0]['visit_padded_node'].shape[0],\n",
    "    embedding_dim=node_emb.shape[1],\n",
    "    hidden_dim=512,\n",
    "    out_channels=1,\n",
    "    layers=3,\n",
    "    dropout=0.5,\n",
    "    decay_rate=0.01,\n",
    "    node_emb=node_emb,\n",
    "    rel_emb=rel_emb\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphCare(\n",
       "  (node_emb): Embedding(4599, 1536)\n",
       "  (rel_emb): Embedding(1077, 1536)\n",
       "  (lin): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (alpha_attn): ModuleDict(\n",
       "    (1): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "    (2): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "    (3): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "  )\n",
       "  (beta_attn): ModuleDict(\n",
       "    (1): Linear(in_features=4599, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=4599, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=4599, out_features=1, bias=True)\n",
       "  )\n",
       "  (conv): ModuleDict(\n",
       "    (1): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "    (2): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "    (3): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  )\n",
       "  (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
       "  (MLP): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2067977339029312: : 120it [00:50,  2.37it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 31.50913429260254, Val PRAUC: 0.1073, Val ROC_AUC: 0.6298, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.15553808212280273: : 120it [00:49,  2.43it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training loss: 29.789306640625, Val PRAUC: 0.1221, Val ROC_AUC: 0.6443, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2628225088119507: : 120it [00:54,  2.21it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training loss: 29.22536277770996, Val PRAUC: 0.1219, Val ROC_AUC: 0.6374, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2506409287452698: : 120it [00:49,  2.41it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training loss: 28.71989631652832, Val PRAUC: 0.1247, Val ROC_AUC: 0.6289, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.17904116213321686: : 120it [00:50,  2.40it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training loss: 28.603946685791016, Val PRAUC: 0.1274, Val ROC_AUC: 0.6521, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.18198521435260773: : 120it [00:46,  2.59it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training loss: 28.40140151977539, Val PRAUC: 0.1251, Val ROC_AUC: 0.6398, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.34278881549835205: : 120it [00:46,  2.56it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training loss: 28.36269187927246, Val PRAUC: 0.1255, Val ROC_AUC: 0.6459, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.39694470167160034: : 120it [00:48,  2.47it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training loss: 28.33040428161621, Val PRAUC: 0.1216, Val ROC_AUC: 0.6462, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22200188040733337: : 120it [00:46,  2.58it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training loss: 28.119611740112305, Val PRAUC: 0.1213, Val ROC_AUC: 0.6465, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.05911890044808388: : 120it [00:46,  2.59it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training loss: 28.490995407104492, Val PRAUC: 0.1307, Val ROC_AUC: 0.6407, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19967162609100342: : 120it [00:43,  2.73it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training loss: 27.972232818603516, Val PRAUC: 0.1286, Val ROC_AUC: 0.6400, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.18729358911514282: : 120it [00:43,  2.74it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training loss: 27.9657039642334, Val PRAUC: 0.1269, Val ROC_AUC: 0.6454, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.12111091613769531: : 120it [00:44,  2.72it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training loss: 27.88951301574707, Val PRAUC: 0.1273, Val ROC_AUC: 0.6405, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1285816729068756: : 120it [00:44,  2.70it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Training loss: 27.78513526916504, Val PRAUC: 0.1312, Val ROC_AUC: 0.6513, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2933382987976074: : 120it [00:45,  2.64it/s] \n",
      "100%|██████████| 15/15 [00:03<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Training loss: 27.727758407592773, Val PRAUC: 0.1284, Val ROC_AUC: 0.6362, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.18093577027320862: : 120it [00:45,  2.62it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Training loss: 27.63042449951172, Val PRAUC: 0.1296, Val ROC_AUC: 0.6434, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3249710202217102: : 120it [00:45,  2.63it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Training loss: 27.789810180664062, Val PRAUC: 0.1277, Val ROC_AUC: 0.6407, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.15263307094573975: : 120it [00:42,  2.81it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Training loss: 27.643404006958008, Val PRAUC: 0.1266, Val ROC_AUC: 0.6387, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3869945704936981: : 120it [00:44,  2.68it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training loss: 27.582056045532227, Val PRAUC: 0.1255, Val ROC_AUC: 0.6400, Val acc: 0.9208, Val F1: 0.4794, Val precision: 0.4619, Val recall: 0.4983, Val jaccard: 0.4604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3811080753803253: : 120it [00:43,  2.75it/s]  \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Training loss: 27.32731819152832, Val PRAUC: 0.1264, Val ROC_AUC: 0.6396, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.07026718556880951: : 14it [00:05,  2.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pj20/experiment/exp.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m train_loop(train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, device\u001b[39m=\u001b[39;49mdevice, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/home/pj20/experiment/exp.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, val_loader, model, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m best_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     y_true_all, y_prob_all \u001b[39m=\u001b[39m evaluate(model, device, val_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     y_pred_all \u001b[39m=\u001b[39m (y_prob_all \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "\u001b[1;32m/home/pj20/experiment/exp.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00mtraining_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     node_ids \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39my\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/data.py:262\u001b[0m, in \u001b[0;36mBaseData.to\u001b[0;34m(self, device, non_blocking, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, device: Union[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39m*\u001b[39margs: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    259\u001b[0m        non_blocking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    260\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    263\u001b[0m         \u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice, non_blocking\u001b[39m=\u001b[39;49mnon_blocking), \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/data.py:245\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m store \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstores:\n\u001b[0;32m--> 245\u001b[0m     store\u001b[39m.\u001b[39;49mapply(func, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    246\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/storage.py:183\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems(\u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m recursive_apply(value, func)\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/storage.py:679\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecursive_apply\u001b[39m(data: Any, func: Callable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m--> 679\u001b[0m         \u001b[39mreturn\u001b[39;00m func(data)\n\u001b[1;32m    680\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mPackedSequence):\n\u001b[1;32m    681\u001b[0m         \u001b[39mreturn\u001b[39;00m func(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/data.py:263\u001b[0m, in \u001b[0;36mBaseData.to.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, device: Union[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39m*\u001b[39margs: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    259\u001b[0m        non_blocking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    260\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice, non_blocking\u001b[39m=\u001b[39;49mnon_blocking), \u001b[39m*\u001b[39margs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
