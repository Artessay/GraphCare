{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pyhealth.datasets import SampleDataset\n",
    "from pyhealth.datasets import split_by_patient\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "with open('../../../data/pj20/exp_data/icd9cm_icd9proc/drugrec_dataset_umls_th015.pkl', 'rb') as f:\n",
    "    sample_dataset = pickle.load(f)\n",
    "\n",
    "with open('../../../data/pj20/exp_data/icd9cm_icd9proc/graph_umls_th015_drugrec.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "G_tg = from_networkx(G) \n",
    "\n",
    "# filt_dataset = []\n",
    "\n",
    "# for patient in sample_dataset:\n",
    "#     if len(patient['node_set']) != 0:\n",
    "#         filt_dataset.append(patient)\n",
    "# filt_dataset = SampleDataset(samples=filt_dataset)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 29\n"
     ]
    }
   ],
   "source": [
    "c_v, p_v, d_v = [], [], []\n",
    "\n",
    "for patient in sample_dataset:\n",
    "    c_v.append(len(patient['conditions']))\n",
    "    p_v.append(len(patient['procedures']))\n",
    "print(max(c_v), max(p_v))\n",
    "max_visits = max(c_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "\n",
    "def get_subgraph(G, dataset, idx):\n",
    "    patient = dataset[idx]\n",
    "    while len(patient['node_set']) == 0:\n",
    "        idx -= 1\n",
    "        patient = dataset[idx]\n",
    "    # L = G.edge_subgraph(torch.tensor([*patient['node_set']]))\n",
    "    P = G.subgraph(torch.tensor([*patient['node_set']]))\n",
    "    P.label = patient['drugs_ind']\n",
    "    P.visits_cond = patient['visit_node_set_condition']\n",
    "    P.visits_proc = patient['visit_node_set_procedure']\n",
    "    \n",
    "    return P\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, G, dataset):\n",
    "        self.G = G\n",
    "        self.dataset=dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        return get_subgraph(G=self.G, dataset=self.dataset, idx=idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GINConv, HGTConv\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.loader import DataListLoader\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1)\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # print(x.shape)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # print(x.shape)\n",
    "        logits = self.fc(x)\n",
    "        # print(logits.shape)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Linear(in_channels, hidden_channels))\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GINX(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, hidden_channels, out_channels, word_emb=None):\n",
    "        super(GINX, self).__init__()\n",
    "        \n",
    "        if word_emb == None:\n",
    "            self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "            self.conv1 = GINConv(Linear(embedding_dim, hidden_channels))\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(word_emb, freeze=False)\n",
    "            self.conv1 = GINConv(Linear(word_emb.shape[1], hidden_channels))\n",
    "\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, node_ids, edge_index, batch):\n",
    "        x = self.embedding(node_ids)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from pyhealth.models import RETAINLayer\n",
    "\n",
    "class GraphCare(nn.Module):\n",
    "    def __init__(self, num_nodes, feature_keys, embedding_dim, hidden_dim, out_channels, dropout=0.5, max_visits=None, word_emb=None, use_attn=True):\n",
    "        super(GraphCare, self).__init__()\n",
    "        self.max_visits = max_visits\n",
    "        self.max_nodes = len(word_emb)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_attn = use_attn\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "        if word_emb == None:\n",
    "            self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(word_emb, freeze=True)\n",
    "        \n",
    "        self.retain = nn.ModuleDict()\n",
    "        for feature_key in feature_keys:\n",
    "            self.retain[feature_key] = RETAINLayer(feature_size=self.max_nodes, dropout=dropout)\n",
    "        \n",
    "        self.conv1 = GINEConv(nn.Linear(embedding_dim, hidden_dim), edge_dim=1)\n",
    "        self.conv2 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=1)\n",
    "        self.conv3 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, node_ids, edge_index, batch, visits_cond, visits_proc):\n",
    "        x = self.embedding(node_ids)\n",
    "\n",
    "        if self.use_attn == True:\n",
    "\n",
    "            cond_attn = self.retain['cond'](visits_cond)\n",
    "            proc_attn = self.retain['proc'](visits_proc)\n",
    "            cross_attn = self.retain['cross'](visits_cond + visits_proc)\n",
    "\n",
    "            attn = cond_attn.add_(proc_attn).add_(cross_attn)    # (batch_size, max_nodes)\n",
    "\n",
    "            # Create a batch index tensor to map the batch index to the corresponding attention weight\n",
    "            batch_index = torch.arange(attn.size(0), device=node_ids.device).repeat_interleave(torch.bincount(batch))   \n",
    "            # print(\"batch index shape: \", batch_index.shape)\n",
    "            # print(\"edge index shape: \", edge_index.shape)\n",
    "            # Fill the attn_weights matrix with the correct weights using batch_index and node_ids\n",
    "            attn_weights = attn[batch_index, node_ids]\n",
    "            # Multiply the embeddings with the corresponding attention weights\n",
    "            # x = x * attn_weights\n",
    "            # x = (1 - self.alpha) * x + self.alpha * x\n",
    "            row, col = edge_index\n",
    "            # Define a small constant value epsilon\n",
    "            epsilon = 1e-6\n",
    "\n",
    "            # Calculate the geometric mean with added epsilon\n",
    "            # Normalize the attn_weights and replace NaNs with 0s\n",
    "            attn_weights = attn_weights / torch.max(attn_weights)\n",
    "            attn_weights = torch.where(torch.isnan(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n",
    "\n",
    "            # Calculate the geometric mean with added epsilon\n",
    "            edge_attr = ((attn_weights[row] + epsilon) + (attn_weights[col] + epsilon)).unsqueeze(-1)\n",
    "\n",
    "            \n",
    "            # print(\"row shape: \", row.shape) \n",
    "            # print(\"col shape: \", col.shape)\n",
    "            # print(\"attn shape: \", attn.shape)\n",
    "            # print(\"attn_weights shape: \", attn_weights.shape)\n",
    "            # print(\"edge_attr shape: \", edge_attr.shape)\n",
    "            # print(\"x shape: \", x.shape)\n",
    "\n",
    "\n",
    "        # Apply the first GIN layer\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        # Apply the second GIN layer\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, jaccard_score\n",
    "\n",
    "def train(model, device, train_loader, optimizer, model_):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    tot_loss = 0\n",
    "    pbar = tqdm(train_loader)\n",
    "    for data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if model_ == \"GIN\":\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "        elif model_ == \"GINX\":\n",
    "            out = model(data.y, data.edge_index, data.batch)\n",
    "        else:\n",
    "            out = model(\n",
    "                    data.y, \n",
    "                    data.edge_index, \n",
    "                    data.batch, \n",
    "                    data.visits_cond.reshape(int(train_loader.batch_size), int(len(data.visits_cond)/train_loader.batch_size), data.visits_cond.shape[1]).double(), \n",
    "                    data.visits_proc.reshape(int(train_loader.batch_size), int(len(data.visits_proc)/train_loader.batch_size), data.visits_proc.shape[1]).double(), \n",
    "                )\n",
    "\n",
    "        \n",
    "        label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label)\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return tot_loss\n",
    "\n",
    "def evaluate(model, device, loader, model_):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if model_ == \"GIN\":\n",
    "                logits = model(data.x, data.edge_index, data.batch)\n",
    "            elif model_ == \"GINX\":\n",
    "                logits = model(data.y, data.edge_index, data.batch)\n",
    "            else:\n",
    "                logits = model(\n",
    "                    data.y, \n",
    "                    data.edge_index, \n",
    "                    data.batch, \n",
    "                    data.visits_cond.reshape(int(loader.batch_size), int(len(data.visits_cond)/loader.batch_size), data.visits_cond.shape[1]).double(), \n",
    "                    data.visits_proc.reshape(int(loader.batch_size), int(len(data.visits_proc)/loader.batch_size), data.visits_proc.shape[1]).double(), \n",
    "                )\n",
    "\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "            try:\n",
    "                y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "            except:\n",
    "                continue\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "    # pr_auc = multilabel_metrics_fn(y_true=y_true_all, y_prob=y_true_all, metrics=\"pr_auc_macro\")\n",
    "\n",
    "    return y_true_all, y_prob_all\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs, model_):\n",
    "    best_pr_auc = 0\n",
    "    best_roc_auc = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer, model_)\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader, model_)\n",
    "\n",
    "        y_pred_all = y_prob_all.copy()\n",
    "        y_pred_all[y_pred_all >= 0.5] = 1\n",
    "        y_pred_all[y_pred_all < 0.5] = 0\n",
    "\n",
    "        val_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "        val_roc_auc = roc_auc_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "        val_f1 = f1_score(y_true_all, y_pred_all, average='samples')\n",
    "        val_jaccard = jaccard_score(y_true_all, y_pred_all, average='samples')\n",
    "\n",
    "        if val_pr_auc >= best_pr_auc and val_roc_auc >= best_roc_auc:\n",
    "            torch.save(model.state_dict(), '../../../data/pj20/exp_data/saved_weights_graph_mimic3_drugrec_th02.pkl')\n",
    "            print(\"best model saved\")\n",
    "            best_pr_auc = val_pr_auc\n",
    "            best_roc_auc = val_roc_auc\n",
    "\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}, Val ROC_AUC: {val_roc_auc:.4f}, Val F1-score: {val_f1:.4f}, Val Jaccard: {val_jaccard:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_tg.x = torch.randn(G_tg.num_nodes, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_set = Dataset(G=G_tg, dataset=train_dataset)\n",
    "val_set = Dataset(G=G_tg, dataset=val_dataset)\n",
    "test_set = Dataset(G=G_tg, dataset=test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GIN(\n",
       "  (conv1): GINConv(nn=Linear(in_features=1536, out_features=512, bias=True))\n",
       "  (conv2): GINConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  (conv3): GINConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  (fc): Linear(in_features=512, out_features=197, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = \"GIN\"\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "out_channels = len(train_set[0].label)\n",
    "\n",
    "if model_ == \"GIN\":\n",
    "    in_channels = train_set[0].x.shape[1]\n",
    "    model = GIN(in_channels=in_channels, out_channels=out_channels, hidden_channels=512).to(device)\n",
    "    # model = GAT(in_channels=in_channels, out_channels=1, hidden_channels=256, heads=3).to(device)\n",
    "    # model = HGT(in_channels=in_channels, out_channels=out_channels, hidden_channels=512, heads=2).to(device)\n",
    "elif model_ == \"GINX\":\n",
    "    model = GINX(num_nodes=G_tg.num_nodes, embedding_dim=512, hidden_channels=512, out_channels=out_channels, word_emb=G_tg.x).to(device)\n",
    "\n",
    "elif model_ == \"GraphCare\":\n",
    "    # model = GINX(num_nodes=G_tg.num_nodes, embedding_dim=512, hidden_channels=512, out_channels=out_channels, word_emb=G_tg.x).to(device)\n",
    "    model = GraphCare(\n",
    "        num_nodes=G_tg.num_nodes,\n",
    "        feature_keys=['cond', 'proc', 'cross'], \n",
    "        embedding_dim=len(G_tg.x[0]), \n",
    "        hidden_dim=512, \n",
    "        out_channels=out_channels, \n",
    "        dropout=0.5, \n",
    "        max_visits=max_visits,\n",
    "        word_emb=G_tg.x,\n",
    "        use_attn=True\n",
    "    ).to(device)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.264652420131181: 100%|██████████| 2223/2223 [02:28<00:00, 14.97it/s]  \n",
      "100%|██████████| 271/271 [00:16<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 1, Training loss: 620.9488562615705, Val PRAUC: 0.6562, Val ROC_AUC: 0.9117, Val F1-score: 0.4713, Val Jaccard: 0.3203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23851399449628402: 100%|██████████| 2223/2223 [03:03<00:00, 12.11it/s]\n",
      "100%|██████████| 271/271 [00:17<00:00, 15.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 2, Training loss: 565.3708916109855, Val PRAUC: 0.6958, Val ROC_AUC: 0.9200, Val F1-score: 0.5363, Val Jaccard: 0.3806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.25836394406931: 100%|██████████| 2223/2223 [02:44<00:00, 13.49it/s]   \n",
      "100%|██████████| 271/271 [00:19<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 3, Training loss: 546.5450063532412, Val PRAUC: 0.7062, Val ROC_AUC: 0.9245, Val F1-score: 0.5276, Val Jaccard: 0.3739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2656918345643904: 100%|██████████| 2223/2223 [02:48<00:00, 13.19it/s] \n",
      "100%|██████████| 271/271 [00:16<00:00, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 4, Training loss: 539.1200801766764, Val PRAUC: 0.7128, Val ROC_AUC: 0.9264, Val F1-score: 0.5365, Val Jaccard: 0.3839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23456541218674248: 100%|██████████| 2223/2223 [02:42<00:00, 13.66it/s]\n",
      "100%|██████████| 271/271 [00:18<00:00, 14.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training loss: 530.5595686406699, Val PRAUC: 0.7159, Val ROC_AUC: 0.9262, Val F1-score: 0.5734, Val Jaccard: 0.4198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3102274889538489: 100%|██████████| 2223/2223 [02:37<00:00, 14.08it/s] \n",
      "100%|██████████| 271/271 [00:16<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 6, Training loss: 524.6495366238371, Val PRAUC: 0.7156, Val ROC_AUC: 0.9268, Val F1-score: 0.5671, Val Jaccard: 0.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21745822299046955: 100%|██████████| 2223/2223 [02:38<00:00, 14.07it/s]\n",
      "100%|██████████| 271/271 [00:18<00:00, 14.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 7, Training loss: 521.8044102076269, Val PRAUC: 0.7231, Val ROC_AUC: 0.9294, Val F1-score: 0.5700, Val Jaccard: 0.4173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23728790588903828: 100%|██████████| 2223/2223 [02:40<00:00, 13.84it/s]\n",
      "100%|██████████| 271/271 [00:17<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 8, Training loss: 518.4774973883899, Val PRAUC: 0.7242, Val ROC_AUC: 0.9298, Val F1-score: 0.5570, Val Jaccard: 0.4059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19735614937025947: 100%|██████████| 2223/2223 [02:41<00:00, 13.80it/s]\n",
      "100%|██████████| 271/271 [00:15<00:00, 17.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training loss: 517.4744460167844, Val PRAUC: 0.7184, Val ROC_AUC: 0.9279, Val F1-score: 0.5788, Val Jaccard: 0.4253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23712275730851895: 100%|██████████| 2223/2223 [02:45<00:00, 13.43it/s]\n",
      "100%|██████████| 271/271 [00:16<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 10, Training loss: 520.2664088063736, Val PRAUC: 0.7265, Val ROC_AUC: 0.9299, Val F1-score: 0.5671, Val Jaccard: 0.4151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.28236734866247293: 100%|██████████| 2223/2223 [02:44<00:00, 13.48it/s]\n",
      "100%|██████████| 271/271 [00:14<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 11, Training loss: 514.0924009080613, Val PRAUC: 0.7298, Val ROC_AUC: 0.9313, Val F1-score: 0.5653, Val Jaccard: 0.4135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2023531802907622: 100%|██████████| 2223/2223 [02:26<00:00, 15.17it/s] \n",
      "100%|██████████| 271/271 [00:16<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training loss: 513.4551000749922, Val PRAUC: 0.7278, Val ROC_AUC: 0.9300, Val F1-score: 0.5693, Val Jaccard: 0.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2252312063652598: 100%|██████████| 2223/2223 [02:36<00:00, 14.25it/s] \n",
      "100%|██████████| 271/271 [00:15<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training loss: 512.6274367714983, Val PRAUC: 0.7294, Val ROC_AUC: 0.9309, Val F1-score: 0.5790, Val Jaccard: 0.4261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2771169464038104: 100%|██████████| 2223/2223 [02:27<00:00, 15.11it/s] \n",
      "100%|██████████| 271/271 [00:16<00:00, 16.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Training loss: 511.374240786065, Val PRAUC: 0.7289, Val ROC_AUC: 0.9304, Val F1-score: 0.5663, Val Jaccard: 0.4141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.303339931228752: 100%|██████████| 2223/2223 [02:26<00:00, 15.19it/s]  \n",
      "100%|██████████| 271/271 [00:14<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 15, Training loss: 511.4807423247251, Val PRAUC: 0.7318, Val ROC_AUC: 0.9319, Val F1-score: 0.5625, Val Jaccard: 0.4108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2384440962679373: 100%|██████████| 2223/2223 [02:18<00:00, 16.02it/s] \n",
      "100%|██████████| 271/271 [00:13<00:00, 19.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 16, Training loss: 510.5601419647611, Val PRAUC: 0.7331, Val ROC_AUC: 0.9323, Val F1-score: 0.5775, Val Jaccard: 0.4254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2207560984998378: 100%|██████████| 2223/2223 [02:15<00:00, 16.36it/s] \n",
      "100%|██████████| 271/271 [00:13<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Training loss: 509.86237250197934, Val PRAUC: 0.7305, Val ROC_AUC: 0.9316, Val F1-score: 0.5632, Val Jaccard: 0.4111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2333292302457498: 100%|██████████| 2223/2223 [02:13<00:00, 16.70it/s] \n",
      "100%|██████████| 271/271 [00:13<00:00, 19.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Training loss: 509.634421370784, Val PRAUC: 0.7294, Val ROC_AUC: 0.9310, Val F1-score: 0.5620, Val Jaccard: 0.4102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2699796150366273: 100%|██████████| 2223/2223 [02:17<00:00, 16.11it/s] \n",
      "100%|██████████| 271/271 [00:13<00:00, 20.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training loss: 516.1450444874873, Val PRAUC: 0.7324, Val ROC_AUC: 0.9318, Val F1-score: 0.5783, Val Jaccard: 0.4261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.28399227903704094: 100%|██████████| 2223/2223 [02:36<00:00, 14.20it/s]\n",
      "100%|██████████| 271/271 [00:18<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Training loss: 509.32720693649316, Val PRAUC: 0.7313, Val ROC_AUC: 0.9323, Val F1-score: 0.5559, Val Jaccard: 0.4047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2266822409795965: 100%|██████████| 2223/2223 [02:40<00:00, 13.89it/s] \n",
      "100%|██████████| 271/271 [00:14<00:00, 19.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Training loss: 510.92679103652785, Val PRAUC: 0.7287, Val ROC_AUC: 0.9312, Val F1-score: 0.5695, Val Jaccard: 0.4174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2084033884479648: 100%|██████████| 2223/2223 [02:52<00:00, 12.87it/s] \n",
      "100%|██████████| 271/271 [00:18<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Training loss: 513.1270967918309, Val PRAUC: 0.7287, Val ROC_AUC: 0.9309, Val F1-score: 0.5742, Val Jaccard: 0.4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.233681675821645: 100%|██████████| 2223/2223 [02:54<00:00, 12.75it/s]  \n",
      "100%|██████████| 271/271 [00:16<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Training loss: 511.117312153225, Val PRAUC: 0.7312, Val ROC_AUC: 0.9317, Val F1-score: 0.5578, Val Jaccard: 0.4066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20919307109037283: 100%|██████████| 2223/2223 [03:01<00:00, 12.22it/s]\n",
      "100%|██████████| 271/271 [00:14<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Training loss: 509.8599535965487, Val PRAUC: 0.7324, Val ROC_AUC: 0.9317, Val F1-score: 0.5766, Val Jaccard: 0.4247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24528638202076386: 100%|██████████| 2223/2223 [02:42<00:00, 13.66it/s]\n",
      "100%|██████████| 271/271 [00:15<00:00, 16.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 25, Training loss: 508.64697666377094, Val PRAUC: 0.7333, Val ROC_AUC: 0.9330, Val F1-score: 0.5813, Val Jaccard: 0.4296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21262393636107757: 100%|██████████| 2223/2223 [02:31<00:00, 14.71it/s]\n",
      "100%|██████████| 271/271 [00:14<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Training loss: 507.14072430252537, Val PRAUC: 0.7335, Val ROC_AUC: 0.9328, Val F1-score: 0.5770, Val Jaccard: 0.4251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20974041947672353: 100%|██████████| 2223/2223 [02:20<00:00, 15.82it/s]\n",
      "100%|██████████| 271/271 [00:13<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 27, Training loss: 506.3439952924706, Val PRAUC: 0.7347, Val ROC_AUC: 0.9331, Val F1-score: 0.5788, Val Jaccard: 0.4269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22488158366622177: 100%|██████████| 2223/2223 [02:17<00:00, 16.19it/s]\n",
      "100%|██████████| 271/271 [00:16<00:00, 16.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 28, Training loss: 506.0166451120067, Val PRAUC: 0.7353, Val ROC_AUC: 0.9333, Val F1-score: 0.5716, Val Jaccard: 0.4202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24217492389990528: 100%|██████████| 2223/2223 [02:25<00:00, 15.26it/s]\n",
      "100%|██████████| 271/271 [00:14<00:00, 19.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Training loss: 506.12700061344873, Val PRAUC: 0.7343, Val ROC_AUC: 0.9324, Val F1-score: 0.5851, Val Jaccard: 0.4329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2608394724586076: 100%|██████████| 2223/2223 [02:29<00:00, 14.84it/s] \n",
      "100%|██████████| 271/271 [00:17<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Training loss: 504.98880040206615, Val PRAUC: 0.7333, Val ROC_AUC: 0.9327, Val F1-score: 0.5751, Val Jaccard: 0.4230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21214668744232168: 100%|██████████| 2223/2223 [02:33<00:00, 14.51it/s]\n",
      "100%|██████████| 271/271 [00:14<00:00, 19.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Training loss: 504.78390528750305, Val PRAUC: 0.7351, Val ROC_AUC: 0.9336, Val F1-score: 0.5864, Val Jaccard: 0.4344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.16835746878889973: 100%|██████████| 2223/2223 [02:44<00:00, 13.55it/s]\n",
      "100%|██████████| 271/271 [00:19<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 32, Training loss: 504.27173944700746, Val PRAUC: 0.7358, Val ROC_AUC: 0.9335, Val F1-score: 0.5773, Val Jaccard: 0.4258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24326478438445132: 100%|██████████| 2223/2223 [02:41<00:00, 13.75it/s]\n",
      "100%|██████████| 271/271 [00:18<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Training loss: 503.2805603499776, Val PRAUC: 0.7310, Val ROC_AUC: 0.9320, Val F1-score: 0.5755, Val Jaccard: 0.4233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20465569731650074: 100%|██████████| 2223/2223 [02:43<00:00, 13.56it/s]\n",
      "100%|██████████| 271/271 [00:19<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Training loss: 503.292123100584, Val PRAUC: 0.7337, Val ROC_AUC: 0.9320, Val F1-score: 0.5784, Val Jaccard: 0.4266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23988472870351688: 100%|██████████| 2223/2223 [02:51<00:00, 12.94it/s]\n",
      "100%|██████████| 271/271 [00:16<00:00, 16.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Training loss: 503.2673965112291, Val PRAUC: 0.7354, Val ROC_AUC: 0.9335, Val F1-score: 0.5763, Val Jaccard: 0.4245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2062125938388466: 100%|██████████| 2223/2223 [03:01<00:00, 12.26it/s] \n",
      "100%|██████████| 271/271 [00:16<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Training loss: 502.6411985118604, Val PRAUC: 0.7356, Val ROC_AUC: 0.9339, Val F1-score: 0.5750, Val Jaccard: 0.4233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.25239967689868986: 100%|██████████| 2223/2223 [02:40<00:00, 13.82it/s]\n",
      "100%|██████████| 271/271 [00:15<00:00, 17.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 37, Training loss: 502.62556518607164, Val PRAUC: 0.7373, Val ROC_AUC: 0.9343, Val F1-score: 0.5742, Val Jaccard: 0.4229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19268260674661697: 100%|██████████| 2223/2223 [02:50<00:00, 13.01it/s]\n",
      "100%|██████████| 271/271 [00:18<00:00, 14.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Training loss: 501.9360889644533, Val PRAUC: 0.7373, Val ROC_AUC: 0.9343, Val F1-score: 0.5828, Val Jaccard: 0.4310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19220278833534504: 100%|██████████| 2223/2223 [02:59<00:00, 12.41it/s]\n",
      "100%|██████████| 271/271 [00:20<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Training loss: 501.81665132676704, Val PRAUC: 0.7364, Val ROC_AUC: 0.9336, Val F1-score: 0.5721, Val Jaccard: 0.4207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2388874401674598: 100%|██████████| 2223/2223 [03:10<00:00, 11.70it/s] \n",
      "100%|██████████| 271/271 [00:19<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Training loss: 501.4019642126533, Val PRAUC: 0.7357, Val ROC_AUC: 0.9338, Val F1-score: 0.5749, Val Jaccard: 0.4231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1950800610722115: 100%|██████████| 2223/2223 [03:15<00:00, 11.37it/s] \n",
      "100%|██████████| 271/271 [00:19<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Training loss: 500.89516088711434, Val PRAUC: 0.7369, Val ROC_AUC: 0.9337, Val F1-score: 0.5933, Val Jaccard: 0.4408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2600185637682354: 100%|██████████| 2223/2223 [03:13<00:00, 11.47it/s] \n",
      "100%|██████████| 271/271 [00:20<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Training loss: 500.7281724472741, Val PRAUC: 0.7368, Val ROC_AUC: 0.9342, Val F1-score: 0.5667, Val Jaccard: 0.4155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24994443603869715: 100%|██████████| 2223/2223 [03:08<00:00, 11.77it/s]\n",
      "100%|██████████| 271/271 [00:17<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 43, Training loss: 500.9458651389644, Val PRAUC: 0.7386, Val ROC_AUC: 0.9346, Val F1-score: 0.5782, Val Jaccard: 0.4268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2692422061525881: 100%|██████████| 2223/2223 [03:02<00:00, 12.15it/s] \n",
      "100%|██████████| 271/271 [00:22<00:00, 12.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Training loss: 500.23906527011775, Val PRAUC: 0.7339, Val ROC_AUC: 0.9334, Val F1-score: 0.5757, Val Jaccard: 0.4241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1966615091898438: 100%|██████████| 2223/2223 [02:51<00:00, 12.93it/s] \n",
      "100%|██████████| 271/271 [00:15<00:00, 17.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Training loss: 499.8853326913479, Val PRAUC: 0.7343, Val ROC_AUC: 0.9324, Val F1-score: 0.5938, Val Jaccard: 0.4409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.25591749245114886: 100%|██████████| 2223/2223 [02:47<00:00, 13.27it/s]\n",
      "100%|██████████| 271/271 [00:22<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Training loss: 499.5433346207292, Val PRAUC: 0.7366, Val ROC_AUC: 0.9342, Val F1-score: 0.5864, Val Jaccard: 0.4340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2588962777731507: 100%|██████████| 2223/2223 [02:46<00:00, 13.37it/s] \n",
      "100%|██████████| 271/271 [00:18<00:00, 14.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Training loss: 499.202745295591, Val PRAUC: 0.7360, Val ROC_AUC: 0.9337, Val F1-score: 0.5921, Val Jaccard: 0.4395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19312815908735756: 100%|██████████| 2223/2223 [02:46<00:00, 13.39it/s]\n",
      "100%|██████████| 271/271 [00:19<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 48, Training loss: 499.68388534362253, Val PRAUC: 0.7394, Val ROC_AUC: 0.9347, Val F1-score: 0.5886, Val Jaccard: 0.4370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.25292904059296445: 100%|██████████| 2223/2223 [02:56<00:00, 12.60it/s]\n",
      "100%|██████████| 271/271 [00:22<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Training loss: 499.45574927639666, Val PRAUC: 0.7384, Val ROC_AUC: 0.9345, Val F1-score: 0.5927, Val Jaccard: 0.4405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23888105063542406: 100%|██████████| 2223/2223 [03:14<00:00, 11.43it/s]\n",
      "100%|██████████| 271/271 [00:19<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Training loss: 499.08935754453796, Val PRAUC: 0.7386, Val ROC_AUC: 0.9347, Val F1-score: 0.5869, Val Jaccard: 0.4348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2214860826783488: 100%|██████████| 2223/2223 [03:14<00:00, 11.45it/s] \n",
      "100%|██████████| 271/271 [00:22<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, Training loss: 498.93953396546306, Val PRAUC: 0.7378, Val ROC_AUC: 0.9345, Val F1-score: 0.5970, Val Jaccard: 0.4448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24503920857640643: 100%|██████████| 2223/2223 [03:01<00:00, 12.25it/s]\n",
      "100%|██████████| 271/271 [00:20<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52, Training loss: 498.99925550237987, Val PRAUC: 0.7384, Val ROC_AUC: 0.9342, Val F1-score: 0.5881, Val Jaccard: 0.4363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20921497779866546: 100%|██████████| 2223/2223 [03:00<00:00, 12.29it/s]\n",
      "100%|██████████| 271/271 [00:20<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53, Training loss: 498.48129641795003, Val PRAUC: 0.7378, Val ROC_AUC: 0.9343, Val F1-score: 0.5842, Val Jaccard: 0.4322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24957955463703496: 100%|██████████| 2223/2223 [02:56<00:00, 12.63it/s]\n",
      "100%|██████████| 271/271 [00:15<00:00, 17.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, Training loss: 498.82705942120526, Val PRAUC: 0.7370, Val ROC_AUC: 0.9335, Val F1-score: 0.5897, Val Jaccard: 0.4372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1951572555367995: 100%|██████████| 2223/2223 [03:00<00:00, 12.32it/s] \n",
      "100%|██████████| 271/271 [00:14<00:00, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 55, Training loss: 498.32914848464105, Val PRAUC: 0.7397, Val ROC_AUC: 0.9350, Val F1-score: 0.5807, Val Jaccard: 0.4295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.253510790274362: 100%|██████████| 2223/2223 [02:47<00:00, 13.26it/s]  \n",
      "100%|██████████| 271/271 [00:19<00:00, 14.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56, Training loss: 498.3820612854395, Val PRAUC: 0.7379, Val ROC_AUC: 0.9342, Val F1-score: 0.5752, Val Jaccard: 0.4231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21018016546946724: 100%|██████████| 2223/2223 [02:54<00:00, 12.76it/s]\n",
      "100%|██████████| 271/271 [00:16<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57, Training loss: 497.9989936056451, Val PRAUC: 0.7399, Val ROC_AUC: 0.9350, Val F1-score: 0.5961, Val Jaccard: 0.4437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22829240612955068: 100%|██████████| 2223/2223 [02:52<00:00, 12.88it/s]\n",
      "100%|██████████| 271/271 [00:19<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58, Training loss: 498.02106887904614, Val PRAUC: 0.7374, Val ROC_AUC: 0.9340, Val F1-score: 0.5911, Val Jaccard: 0.4390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2566155296914928:  67%|██████▋   | 1495/2223 [01:50<00:53, 13.51it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m train_loop(train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, device\u001b[39m=\u001b[39;49mdevice, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, model_\u001b[39m=\u001b[39;49mmodel_)\n",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, val_loader, model, optimizer, device, epochs, model_)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m best_roc_auc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer, model_)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     y_true_all, y_prob_all \u001b[39m=\u001b[39m evaluate(model, device, val_loader, model_)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     y_pred_all \u001b[39m=\u001b[39m y_prob_all\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb Cell 10\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, model_)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m tot_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(train_loader)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00mtraining_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_umls_cp.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py:20\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[1;32m     21\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys)\n\u001b[1;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/batch.py:76\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[1;32m     66\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[1;32m     77\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m     78\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[1;32m     79\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     80\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[1;32m     81\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[1;32m     82\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[1;32m     86\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/collate.py:108\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m (add_batch \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(stores[\u001b[39m0\u001b[39m], NodeStorage)\n\u001b[1;32m    106\u001b[0m             \u001b[39mand\u001b[39;00m stores[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcan_infer_num_nodes):\n\u001b[1;32m    107\u001b[0m         repeats \u001b[39m=\u001b[39m [store\u001b[39m.\u001b[39mnum_nodes \u001b[39mfor\u001b[39;00m store \u001b[39min\u001b[39;00m stores]\n\u001b[0;32m--> 108\u001b[0m         out_store\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m repeat_interleave(repeats, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    109\u001b[0m         out_store\u001b[39m.\u001b[39mptr \u001b[39m=\u001b[39m cumsum(torch\u001b[39m.\u001b[39mtensor(repeats, device\u001b[39m=\u001b[39mdevice))\n\u001b[1;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m out, slice_dict, inc_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/collate.py:247\u001b[0m, in \u001b[0;36mrepeat_interleave\u001b[0;34m(repeats, device)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat_interleave\u001b[39m(\n\u001b[1;32m    244\u001b[0m     repeats: List[\u001b[39mint\u001b[39m],\n\u001b[1;32m    245\u001b[0m     device: Optional[torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 247\u001b[0m     outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mfull((n, ), i, device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m i, n \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(repeats)]\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(outs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/collate.py:247\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat_interleave\u001b[39m(\n\u001b[1;32m    244\u001b[0m     repeats: List[\u001b[39mint\u001b[39m],\n\u001b[1;32m    245\u001b[0m     device: Optional[torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 247\u001b[0m     outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mfull((n, ), i, device\u001b[39m=\u001b[39;49mdevice) \u001b[39mfor\u001b[39;00m i, n \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(repeats)]\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(outs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100, model_=model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader:\n",
    "    print(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gat_mimic3_drugrec.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gin_mimic3_drugrec_random.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_hgt_mimic3_drugrec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('./exp_data/saved_weights_gat_mimic3_drugrec.pkl'))\n",
    "model.load_state_dict(torch.load('../../../data/pj20/exp_data/saved_weights_graph_mimic3_drugrec_th02.pkl'))\n",
    "model.double()\n",
    "\n",
    "y_true_all, y_prob_all = evaluate(model, device, val_loader, static)\n",
    "\n",
    "y_pred_all = y_prob_all.copy()\n",
    "y_pred_all[y_pred_all >= 0.5] = 1\n",
    "y_pred_all[y_pred_all < 0.5] = 0\n",
    "\n",
    "test_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "test_roc_auc = roc_auc_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "test_f1 = f1_score(y_true_all, y_pred_all, average='samples')\n",
    "test_jaccard = jaccard_score(y_true_all, y_pred_all, average='samples')\n",
    "\n",
    "print(f'test PRAUC: {test_pr_auc:.4f}, test ROC_AUC: {test_roc_auc:.4f}, test F1-score: {test_f1:.4f}, test Jaccard: {test_jaccard:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
