{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_dir = \"./graphs/condition/CCSCM\"\n",
    "\n",
    "file_id2ent = f\"{file_dir}/id2ent.json\"\n",
    "file_ent2id = f\"{file_dir}/ent2id.json\"\n",
    "file_id2rel = f\"{file_dir}/id2rel.json\"\n",
    "file_rel2id = f\"{file_dir}/rel2id.json\"\n",
    "\n",
    "with open(file_id2ent, 'r') as file:\n",
    "    cond_id2ent = json.load(file)\n",
    "with open(file_ent2id, 'r') as file:\n",
    "    cond_ent2id = json.load(file)\n",
    "with open(file_id2rel, 'r') as file:\n",
    "    cond_id2rel = json.load(file)\n",
    "with open(file_rel2id, 'r') as file:\n",
    "    cond_rel2id = json.load(file)\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "condition_mapping_file = \"./resources/CCSCM.csv\"\n",
    "procedure_mapping_file = \"./resources/CCSPROC.csv\"\n",
    "drug_file = \"./resources/ATC.csv\"\n",
    "\n",
    "condition_dict = {}\n",
    "with open(condition_mapping_file, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        condition_dict[row['code']] = row['name'].lower()\n",
    "\n",
    "procedure_dict = {}\n",
    "with open(procedure_mapping_file, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        procedure_dict[row['code']] = row['name'].lower()\n",
    "\n",
    "drug_dict = {}\n",
    "with open(drug_file, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if row['level'] == '5.0':\n",
    "            drug_dict[row['code']] = row['name'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyhealth.datasets import MIMIC3Dataset\n",
    "# from GraphCare.task_fn import drug_recommendation_fn\n",
    "\n",
    "# mimic3_ds = MIMIC3Dataset(\n",
    "#     root=\"../../../data/physionet.org/files/mimiciii/1.4/\", \n",
    "#     tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"],      \n",
    "#     code_mapping={\n",
    "#         \"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}}),\n",
    "#         \"ICD9CM\": \"CCSCM\",\n",
    "#         \"ICD9PROC\": \"CCSPROC\"\n",
    "#         },\n",
    "# )\n",
    "\n",
    "# sample_dataset = mimic3_ds.set_task(drug_recommendation_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(flatten(item))\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def multihot(label, num_labels):\n",
    "    multihot = np.zeros(num_labels)\n",
    "    for l in label:\n",
    "        multihot[l] = 1\n",
    "    return multihot\n",
    "\n",
    "def prepare_label(drugs):\n",
    "    label_tokenizer = Tokenizer(\n",
    "        sample_dataset.get_all_tokens(key='drugs')\n",
    "    )\n",
    "\n",
    "    labels_index = label_tokenizer.convert_tokens_to_indices(drugs)\n",
    "    # print(labels_index)\n",
    "    # convert to multihot\n",
    "    num_labels = label_tokenizer.get_vocabulary_size()\n",
    "    # print(num_labels)\n",
    "    labels = multihot(labels_index, num_labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# for patient in tqdm(sample_dataset):\n",
    "#     # patient['drugs_all'] = flatten(patient['drugs'])\n",
    "#     # print(patient['drugs_all'])\n",
    "#     patient['drugs_ind'] = torch.tensor(prepare_label(patient['drugs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./exp_data/ccscm_ccsproc/sample_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(sample_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./exp_data/ccscm_ccsproc/sample_dataset.pkl', 'rb') as f:\n",
    "    sample_dataset= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import split_by_patient\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['101', '106', '98', '138']], [['44', '47', '50']])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['conditions'], train_dataset[0]['procedures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "with open('./graphs/cond_proc/CCSCM_CCSPROC/ent2id.json', 'r') as file:\n",
    "    ent2id = json.load(file)\n",
    "with open('./graphs/cond_proc/CCSCM_CCSPROC/rel2id.json', 'r') as file:\n",
    "    rel2id = json.load(file)\n",
    "with open('./graphs/cond_proc/CCSCM_CCSPROC/entity_embedding.pkl', 'rb') as file:\n",
    "    ent_emb = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02357265,  0.002313  ,  0.02204529, ..., -0.01157682,\n",
       "        0.01255   ,  0.00188047])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44399/44399 [01:54<00:00, 388.58it/s]\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for i in range(len(ent_emb)):\n",
    "    G.add_nodes_from([\n",
    "        (i, {'y': i, 'x': ent_emb[i]})\n",
    "    ])\n",
    "\n",
    "triples_all = []\n",
    "for patient in tqdm(sample_dataset):\n",
    "    triples = []\n",
    "    triple_set = set()\n",
    "    # node_set = set()\n",
    "    conditions = flatten(patient['conditions'])\n",
    "    for condition in conditions:\n",
    "        cond_file = f'./graphs/condition/CCSCM/{condition}.txt'\n",
    "        with open(cond_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items = line.split('\\t')\n",
    "            if len(items) == 3:\n",
    "                h, r, t = items\n",
    "                t = t[:-1]\n",
    "                h = int(ent2id[h])\n",
    "                r = int(rel2id[r])\n",
    "                t = int(ent2id[t])\n",
    "                triple = (h, r, t)\n",
    "                if triple not in triple_set:\n",
    "                    triples.append((h, t))\n",
    "                    triple_set.add(triple)\n",
    "                    # node_set.add(h)\n",
    "                    # node_set.add(r)\n",
    "\n",
    "    G.add_edges_from(\n",
    "        triples,\n",
    "        # label=prepare_label(patient['drugs'])\n",
    "    )\n",
    "    \n",
    "    # triples.append(prepare_label(patient['drugs']))\n",
    "    # triples_all.append(np.array(triples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/utils/convert.py:250: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import pickle\n",
    "\n",
    "G_tg = from_networkx(G)\n",
    "\n",
    "# with open('./exp_data/ccscm_ccsproc/graph_tg.pkl', 'wb') as f:\n",
    "#     pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_subgraph(dataset):\n",
    "    \n",
    "    subgraph_list = []\n",
    "    for patient in tqdm(dataset):\n",
    "        triple_set = set()\n",
    "        node_set = set()\n",
    "        conditions = flatten(patient['conditions'])\n",
    "        for condition in conditions:\n",
    "            cond_file = f'./graphs/condition/CCSCM/{condition}.txt'\n",
    "            with open(cond_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            for line in lines:\n",
    "                items = line.split('\\t')\n",
    "                if len(items) == 3:\n",
    "                    h, r, t = items\n",
    "                    t = t[:-1]\n",
    "                    h = int(ent2id[h])\n",
    "                    r = int(rel2id[r])\n",
    "                    t = int(ent2id[t])\n",
    "                    triple = (h, r, t)\n",
    "                    if triple not in triple_set:\n",
    "                        triple_set.add(triple)\n",
    "                        node_set.add(h)\n",
    "                        node_set.add(r)\n",
    "\n",
    "        P = G_tg.subgraph(torch.tensor([*node_set]))\n",
    "        P.label = patient['drugs_ind']\n",
    "        subgraph_list.append(P)\n",
    "\n",
    "    return subgraph_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35578/35578 [02:17<00:00, 259.39it/s]\n",
      "100%|██████████| 4346/4346 [00:16<00:00, 263.44it/s]\n",
      "100%|██████████| 4475/4475 [00:18<00:00, 247.05it/s]\n"
     ]
    }
   ],
   "source": [
    "train_graph_list = get_subgraph(train_dataset)\n",
    "val_graph_list = get_subgraph(val_dataset)\n",
    "test_graph_list = get_subgraph(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, graph_list):\n",
    "        self.graph_list=graph_list\n",
    "    def __len__(self):\n",
    "        return len(self.graph_list)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graph_list[idx]\n",
    "\n",
    "train_set = Dataset(train_graph_list)\n",
    "val_set = Dataset(val_graph_list)\n",
    "test_set = Dataset(test_graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.loader import DataListLoader\n",
    "import pickle\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1)\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # print(x.shape)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # print(x.shape)\n",
    "        logits = self.fc(x)\n",
    "        # print(logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Linear(in_channels, hidden_channels))\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "class HGT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=2):\n",
    "        super(HGT, self).__init__()\n",
    "        self.conv1 = HGTConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = HGTConv(hidden_channels*heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = HGTConv(hidden_channels*heads, hidden_channels, heads=1)\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    pbar = tqdm(train_loader)\n",
    "    for data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        try:\n",
    "            label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "        except:\n",
    "            continue\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label)\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return training_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     print(train_loader.batch_size)\n",
    "#     print(len(data.label))\n",
    "#     print(data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size)).shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(data.x, data.edge_index, data.batch)\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "            try:\n",
    "                y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "            except:\n",
    "                continue\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "    # pr_auc = multilabel_metrics_fn(y_true=y_true_all, y_prob=y_true_all, metrics=\"pr_auc_macro\")\n",
    "\n",
    "    return y_true_all, y_prob_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer)\n",
    "        # y_true_all, y_prob_all = evaluate(model, device, train_loader)\n",
    "        # train_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"macro\")\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader)\n",
    "        val_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 197)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].x.shape[1], len(train_set[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = train_set[0].x.shape[1]\n",
    "out_channels = len(train_set[0].label)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GAT(in_channels=in_channels, out_channels=out_channels, hidden_channels=256, heads=3).to(device)\n",
    "# model = GIN(in_channels=in_channels, out_channels=out_channels, hidden_channels=512).to(device)\n",
    "model = HGT(in_channels=in_channels, out_channels=out_channels, hidden_channels=512, heads=2).to(device)\n",
    "\n",
    "model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gat_mimic3_drugrec.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gin_mimic3_drugrec.pkl')\n",
    "torch.save(model.state_dict(), './exp_data/saved_weights_hgt_mimic3_drugrec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 544/544 [00:39<00:00, 13.72it/s]\n"
     ]
    }
   ],
   "source": [
    "y_true_all, y_prob_all = evaluate(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.757423070584265\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# len(y_true_all), len(y_prob_all)\n",
    "pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "print(pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# graph_all_patients = np.array(triples_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./graph_mimic3_patients.pkl', 'wb') as f:\n",
    "#     pickle.dump(graph_all_patients, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./graph_mimic3_patients.pkl', 'rb') as f:\n",
    "#     graph_all_patients = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./graphs/cond_proc/CCSCM_CCSPROC/id2ent.json', 'r') as file:\n",
    "#     id2ent = json.load(file)\n",
    "# with open('./graphs/cond_proc/CCSCM_CCSPROC/id2rel.json', 'r') as file:\n",
    "#     id2rel = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import dgl\n",
    "# import numpy as np\n",
    "\n",
    "# patient = graph_all_patients[0]\n",
    "# nodes = set()\n",
    "# for triple in patient:\n",
    "#     h, r, t = triple[0], triple[1], triple[2]\n",
    "#     nodes.add(h)\n",
    "#     nodes.add(t)\n",
    "\n",
    "# node_idx_ori2new = {node : idx for idx, node in enumerate(nodes)}\n",
    "# node_idx_new2ori = {idx: node for node, idx in node_idx_ori2new.items()}\n",
    "# heads = torch.tensor([node_idx_ori2new[head] for head in patient[:, 0]])\n",
    "# tails = torch.tensor([node_idx_ori2new[tail] for tail in patient[:, 2]])\n",
    "\n",
    "# g = dgl.graph((heads, tails), num_nodes=len(nodes))\n",
    "\n",
    "# g.edata['edge_ids'] = torch.from_numpy(patient[:, 1])\n",
    "# g.ndata['node_ids'] = torch.from_numpy(np.array([node_idx_new2ori[i] for i in range(len(nodes))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10904), tensor(3966), tensor(14935), array([10904,  3966, 14935]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g.ndata['node_ids'][165], g.edata['edge_ids'][0], g.ndata['node_ids'][147], patient[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9090), tensor(3966), tensor(7639), array([9090, 3966, 7639]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g.ndata['node_ids'][226], g.edata['edge_ids'][1], g.ndata['node_ids'][385], patient[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Torch modules for graph attention networks with fully valuable edges (EGAT).\"\"\"\n",
    "# # pylint: disable= no-member, arguments-differ, invalid-name\n",
    "# import torch as th\n",
    "# from torch import nn\n",
    "# from torch.nn import init\n",
    "\n",
    "# from dgl import function as fn\n",
    "# from dgl.nn.functional import edge_softmax\n",
    "# from dgl.base import DGLError\n",
    "# from dgl.utils import expand_as_pair\n",
    "\n",
    "# # pylint: enable=W0235\n",
    "# class EGATConv(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  in_node_feats,\n",
    "#                  in_edge_feats,\n",
    "#                  out_node_feats,\n",
    "#                  out_edge_feats,\n",
    "#                  num_heads,\n",
    "#                  bias=True):\n",
    "\n",
    "#         super().__init__()\n",
    "#         self._num_heads = num_heads\n",
    "#         self._in_src_node_feats, self._in_dst_node_feats = expand_as_pair(in_node_feats)\n",
    "#         self._out_node_feats = out_node_feats\n",
    "#         self._out_edge_feats = out_edge_feats\n",
    "#         if isinstance(in_node_feats, tuple):\n",
    "#             self.fc_node_src = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_node_feats * num_heads, bias=False)\n",
    "#             self.fc_ni = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_edge_feats*num_heads, bias=False)\n",
    "#             self.fc_nj = nn.Linear(\n",
    "#                 self._in_dst_node_feats, out_edge_feats*num_heads, bias=False)\n",
    "#         else:\n",
    "#             self.fc_node_src = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_node_feats * num_heads, bias=False)\n",
    "#             self.fc_ni = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_edge_feats*num_heads, bias=False)\n",
    "#             self.fc_nj = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_edge_feats*num_heads, bias=False)\n",
    "\n",
    "#         self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats*num_heads, bias=False)\n",
    "#         self.attn = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "#         if bias:\n",
    "#             self.bias = nn.Parameter(th.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "#         else:\n",
    "#             self.register_buffer('bias', None)\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         \"\"\"\n",
    "#         Reinitialize learnable parameters.\n",
    "#         \"\"\"\n",
    "#         gain = init.calculate_gain('relu')\n",
    "#         init.xavier_normal_(self.fc_node_src.weight, gain=gain)\n",
    "#         init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "#         init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "#         init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "#         init.xavier_normal_(self.attn, gain=gain)\n",
    "#         init.constant_(self.bias, 0)\n",
    "\n",
    "#     def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "#         with graph.local_scope():\n",
    "#             if (graph.in_degrees() == 0).any():\n",
    "#                 raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "#                                'output for those nodes will be invalid. '\n",
    "#                                'This is harmful for some applications, '\n",
    "#                                'causing silent performance regression. '\n",
    "#                                'Adding self-loop on the input graph by '\n",
    "#                                'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "#                                'the issue.')\n",
    "\n",
    "#             # calc edge attention\n",
    "#             # same trick way as in dgl.nn.pytorch.GATConv, but also includes edge feats\n",
    "#             # https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/gatconv.py\n",
    "#             if isinstance(nfeats, tuple):\n",
    "#                 nfeats_src, nfeats_dst = nfeats\n",
    "#             else:\n",
    "#                 nfeats_src = nfeats_dst = nfeats\n",
    "\n",
    "#             f_ni = self.fc_ni(nfeats_src)\n",
    "#             f_nj = self.fc_nj(nfeats_dst)\n",
    "#             f_fij = self.fc_fij(efeats)\n",
    "\n",
    "#             graph.srcdata.update({'f_ni': f_ni})\n",
    "#             graph.dstdata.update({'f_nj': f_nj})\n",
    "#             # add ni, nj factors\n",
    "#             graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "#             # add fij to node factor\n",
    "#             f_out = graph.edata.pop('f_tmp') + f_fij\n",
    "#             if self.bias is not None:\n",
    "#                 f_out = f_out + self.bias\n",
    "#             f_out = nn.functional.leaky_relu(f_out)\n",
    "#             f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "#             # compute attention factor\n",
    "#             e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "#             graph.edata['a'] = edge_softmax(graph, e)\n",
    "#             graph.srcdata['h_out'] = self.fc_node_src(nfeats_src).view(-1, self._num_heads,\n",
    "#                                                              self._out_node_feats)\n",
    "#             # calc weighted sum\n",
    "#             graph.update_all(fn.u_mul_e('h_out', 'a', 'm'),\n",
    "#                              fn.sum('m', 'h_out'))\n",
    "\n",
    "#             h_out = graph.dstdata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "#             if get_attention:\n",
    "#                 return h_out, f_out, graph.edata.pop('a')\n",
    "#             else:\n",
    "#                 return h_out, f_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dgl\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# # from dgl.nn.pytorch import EGATConv\n",
    "\n",
    "# class GNNModel(nn.Module):\n",
    "#     def __init__(self, node_feat_dim, edge_feat_dim, num_classes, node_feats, edge_feats, training):\n",
    "#         super(GNNModel, self).__init__()\n",
    "#         self.node_feat_dim = node_feat_dim\n",
    "#         self.edge_feat_dim = edge_feat_dim\n",
    "#         self.num_classes = num_classes\n",
    "#         self.training = training\n",
    "        \n",
    "#         # Define node and edge feature embeddings\n",
    "#         self.node_feat_emb = nn.Embedding.from_pretrained(torch.from_numpy(node_feats).double(), freeze=True)\n",
    "#         self.edge_feat_emb = nn.Embedding.from_pretrained(torch.from_numpy(edge_feats).double(), freeze=True)\n",
    "        \n",
    "#         # Define GNN layers with attention mechanism\n",
    "#         self.gnn_layers = nn.ModuleList()\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim, \n",
    "#                                         in_edge_feats=edge_feat_dim, \n",
    "#                                         out_node_feats=node_feat_dim, \n",
    "#                                         out_edge_feats=edge_feat_dim, \n",
    "#                                         num_heads=3\n",
    "#                                         ))\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim, \n",
    "#                                         in_edge_feats=edge_feat_dim, \n",
    "#                                         out_node_feats=node_feat_dim, \n",
    "#                                         out_edge_feats=edge_feat_dim, \n",
    "#                                         num_heads=3, \n",
    "#                                         ))\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim + edge_feat_dim + node_feat_dim,\n",
    "#                                         in_edge_feats=edge_feat_dim,\n",
    "#                                         out_node_feats=node_feat_dim,\n",
    "#                                         out_edge_feats=edge_feat_dim,\n",
    "#                                         num_heads=3,\n",
    "#                                         ))\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim, \n",
    "#                                         in_edge_feats=edge_feat_dim, \n",
    "#                                         out_node_feats=node_feat_dim, \n",
    "#                                         out_edge_feats=edge_feat_dim, \n",
    "#                                         num_heads=3, \n",
    "#                                         ))\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim, \n",
    "#                                         in_edge_feats=edge_feat_dim, \n",
    "#                                         out_node_feats=node_feat_dim, \n",
    "#                                         out_edge_feats=edge_feat_dim, \n",
    "#                                         num_heads=3, \n",
    "#                                         ))\n",
    "        \n",
    "#         # Define attention mechanism\n",
    "#         self.attn_node = nn.Linear(node_feat_dim, 1, bias=False)\n",
    "#         self.attn_edge = nn.Linear(edge_feat_dim, 1, bias=False)\n",
    "        \n",
    "#         # Define final output layer\n",
    "#         self.output_layer = nn.Linear(node_feat_dim, num_classes)\n",
    "    \n",
    "#     def forward(self, g, get_attention=False):\n",
    "#         # Get node and edge features\n",
    "#         node_feats = self.node_feat_emb(g.ndata['node_ids']).double()\n",
    "#         edge_feats = self.edge_feat_emb(g.edata['edge_ids']).double()\n",
    "#         print(node_feats.shape, edge_feats.shape)\n",
    "        \n",
    "#         # Pass node and edge features through GNN layers with attention\n",
    "#         attentions = []\n",
    "#         for i in range(len(self.gnn_layers)):\n",
    "#             if i == 2:\n",
    "#                 # Add edge features to node features and perform attention\n",
    "#                 node_feats = torch.cat([node_feats, edge_feats, node_feats], dim=-1)\n",
    "#                 node_feats = F.dropout(node_feats, p=0.5, training=self.training)\n",
    "#                 attn_scores = self.attn_edge(edge_feats) + self.attn_node(node_feats)\n",
    "#                 attn_scores = torch.softmax(attn_scores, dim=1)\n",
    "#                 attn_scores = F.dropout(attn_scores, p=0.2, training=self.training)\n",
    "#                 edge_feats = edge_feats * attn_scores\n",
    "#                 node_feats, edge_feats = self.gnn_layers[i](g, node_feats, edge_feats)\n",
    "#                 node_feats = node_feats + edge_feats\n",
    "#                 attentions.append(attn_scores)\n",
    "#             else:\n",
    "#                 # Perform GNN layer and attention\n",
    "#                 node_feats, edge_feats = self.gnn_layers[i](g, node_feats, edge_feats)\n",
    "#                 node_feats = F.dropout(node_feats, p=0.5, training=self.training)\n",
    "#                 attn_scores = self.attn_node(node_feats)\n",
    "#                 attn_scores = torch.softmax(attn_scores, dim=1)\n",
    "#                 attn_scores = F.dropout(attn_scores, p=0.2, training=self.training)\n",
    "#                 node_feats = node_feats * attn_scores\n",
    "#                 attentions.append(attn_scores)\n",
    "        \n",
    "#         # Compute final output probabilities with sigmoid activation\n",
    "#         output_feats = self.output_layer(node_feats)\n",
    "#         output_probs = torch.sigmoid(output_feats)\n",
    "        \n",
    "#         if get_attention:\n",
    "#             return output_probs, attentions\n",
    "#         else:\n",
    "#             return output_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19347, 1536), (5042, 1536))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open('./graphs/cond_proc/CCSCM_CCSPROC/entity_embedding.pkl', 'rb') as f:\n",
    "#     ent_emb = pickle.load(f)\n",
    "# with open('./graphs/cond_proc/CCSCM_CCSPROC/relation_embedding.pkl', 'rb') as f:\n",
    "#     rel_emb = pickle.load(f)\n",
    "\n",
    "# ent_emb.shape, rel_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_feat_dim = ent_emb.shape[-1]\n",
    "# edge_feat_dim = rel_emb.shape[-1]\n",
    "# num_classes = len(sample_dataset.get_all_tokens('drugs'))\n",
    "# node_feats = ent_emb\n",
    "# edge_feats = rel_emb\n",
    "\n",
    "# gnn = GNNModel(node_feat_dim, edge_feat_dim, num_classes, node_feats, edge_feats, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(dgl)\n",
    "\n",
    "# g = dgl.add_self_loop(g)\n",
    "# gnn.double()\n",
    "# gnn(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
