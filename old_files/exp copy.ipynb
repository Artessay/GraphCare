{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling direct ehr nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9717/9717 [00:00<00:00, 18457.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset...\n",
      "Getting embedding...\n"
     ]
    }
   ],
   "source": [
    "from graphcare import *\n",
    "\n",
    "kg = \"GPT-KG\"\n",
    "dataset = \"mimic3\"\n",
    "task = \"mortality\"\n",
    "\n",
    "# load dataset\n",
    "sample_dataset, G, ent2id, rel2id, ent_emb, rel_emb, \\\n",
    "            map_cluster, map_cluster_inv, map_cluster_rel, map_cluster_rel_inv, \\\n",
    "                ccscm_id2clus, ccsproc_id2clus, atc3_id2clus = load_everything(dataset, task, kg)\n",
    "\n",
    "# label direct ehr node\n",
    "print(\"Labeling direct ehr nodes...\")\n",
    "sample_dataset = label_ehr_nodes(task, sample_dataset, len(map_cluster), ccscm_id2clus, ccsproc_id2clus, atc3_id2clus)\n",
    "print(\"Splitting dataset...\")\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)\n",
    "G_tg = from_networkx(G)\n",
    "\n",
    "# get embedding\n",
    "print(\"Getting embedding...\")\n",
    "rel_emb = get_rel_emb(map_cluster_rel)\n",
    "node_emb = G_tg.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 4598, 4598, 4598],\n",
       "        [ 275, 1997,    0,  ..., 3388, 3924, 4283]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_tg.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(G=G_tg, dataset=train_dataset, task=task)\n",
    "val_set = Dataset(G=G_tg, dataset=val_dataset, task=task)\n",
    "test_set = Dataset(G=G_tg, dataset=test_dataset, task=task)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4599, 1536])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "node_embedding = nn.Embedding.from_pretrained(node_emb, freeze=False)\n",
    "relation_embedding = nn.Embedding.from_pretrained(rel_emb, freeze=False)\n",
    "alpha_attn = nn.Linear(node_emb.shape[0], node_emb.shape[0])\n",
    "beta_attn = nn.Linear(node_emb.shape[0], 1)\n",
    "leakyrelu = nn.LeakyReLU(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(4599, 1536), Embedding(1077, 1536))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embedding, relation_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[89412, 1536], edge_index=[2, 54496], y=[89412], relation=[54496], label=[64], visit_padded_node=[1792, 4599], ehr_nodes=[294336], batch=[89412], ptr=[65])\n",
      "torch.Size([54496])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Softmax\n",
    "\n",
    "for batch in train_loader:\n",
    "    # print(batch.ehr_nodes)\n",
    "    # print(node_embedding(torch.tensor(batch.ehr_nodes[0])))\n",
    "    node_ids = batch.y\n",
    "    visit_node = batch.visit_padded_node.reshape(int(train_loader.batch_size), int(len(batch.visit_padded_node)/train_loader.batch_size), batch.visit_padded_node.shape[1]).double()\n",
    "    x = node_embedding(node_ids)\n",
    "    alpha = torch.softmax((leakyrelu(alpha_attn(visit_node.float()))), dim=1)\n",
    "    beta = torch.softmax((leakyrelu(beta_attn(visit_node.float()))), dim=0)\n",
    "    j = torch.arange(visit_node.shape[1], device=x.device).float()\n",
    "    lambda_j = torch.exp(0.03 * (visit_node.shape[1] - j)).unsqueeze(0).reshape(1, visit_node.shape[1], 1)\n",
    "    attn = alpha*beta*lambda_j\n",
    "    attn = torch.sum(attn, dim=1)\n",
    "    ehr_nodes = batch.ehr_nodes.reshape(int(train_loader.batch_size), int(len(batch.ehr_nodes)/train_loader.batch_size)).float()\n",
    "    xj_batch = batch.batch[batch.edge_index[0]]\n",
    "    xj_node_ids = batch.y[batch.edge_index[0]]\n",
    "    print(batch)\n",
    "    print(attn[xj_batch, xj_node_ids].shape)\n",
    "    # print(batch.batch.shape)\n",
    "    # print(attn.shape)\n",
    "    # print(batch.batch[batch.edge_index[0]].shape)\n",
    "    # print(batch.y[batch.edge_index[0]].shape)\n",
    "    # print(ehr_nodes.shape)\n",
    "    # print(ehr_nodes[1].view(1, -1) @ node_embedding.weight / torch.sum(ehr_nodes[1]))\n",
    "    # attn = attn[batch.edge_index[0]]\n",
    "    # print(attn.shape)\n",
    "    \n",
    "    # print(x.shape)\n",
    "    # print(visit_node.shape)\n",
    "    # visit_emb = x.view(visit_node.shape).sum(dim=2) / visit_node.sum(dim=2).clamp(min=1).view(visit_node.shape[:2] + (1,))\n",
    "    # print(visit_emb.shape)\n",
    "    # print(node_emb[:4599].shape)\n",
    "    # print(visit_node.shape)\n",
    "    # print(x.shape)\n",
    "    # print((visit_node @ node_emb[:4599]).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphCare import GAT, GIN, GraphCare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GINEConv\n",
    "# from pyhealth.models import RETAINLayer\n",
    "# from torch_geometric.nn.inits import reset\n",
    "\n",
    "# from typing import Callable, Optional, Union\n",
    "\n",
    "# import torch\n",
    "# from torch import Tensor\n",
    "\n",
    "# from torch_geometric.nn.conv import MessagePassing\n",
    "# from torch_geometric.nn.dense.linear import Linear\n",
    "# from torch_geometric.typing import (\n",
    "#     Adj,\n",
    "#     OptPairTensor,\n",
    "#     OptTensor,\n",
    "#     Size,\n",
    "#     SparseTensor,\n",
    "# )\n",
    "# from torch_geometric.utils import spmm\n",
    "# from torch_geometric.nn import global_mean_pool\n",
    "# from torch_geometric.utils import softmax\n",
    "# from torch.nn import LeakyReLU\n",
    "\n",
    "\n",
    "# class BiAttentionGNNConv(MessagePassing):\n",
    "#     def __init__(self, nn: torch.nn.Module, eps: float = 0.,\n",
    "#                  train_eps: bool = False, edge_dim: Optional[int] = None,\n",
    "#                  **kwargs):\n",
    "#         kwargs.setdefault('aggr', 'add')\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.nn = nn\n",
    "#         self.initial_eps = eps\n",
    "#         self.W_R = torch.nn.Linear(edge_dim, edge_dim)\n",
    "\n",
    "#         if train_eps:\n",
    "#             self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "#         else:\n",
    "#             self.register_buffer('eps', torch.Tensor([eps]))\n",
    "\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         self.nn.reset_parameters()\n",
    "#         self.eps.data.fill_(self.initial_eps)\n",
    "#         if self.W_R is not None:\n",
    "#             self.W_R.reset_parameters()\n",
    "\n",
    "#     def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "#                 edge_attr: OptTensor = None, size: Size = None, attn: Tensor = None) -> Tensor:\n",
    "\n",
    "#         if isinstance(x, Tensor):\n",
    "#             x: OptPairTensor = (x, x)\n",
    "\n",
    "#         out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size, attn=attn)\n",
    "\n",
    "#         x_r = x[1]\n",
    "#         if x_r is not None:\n",
    "#             out = out + (1 + self.eps) * x_r\n",
    "\n",
    "#         return self.nn(out)\n",
    "\n",
    "#     def message(self, x_j: Tensor, edge_attr: Tensor, attn: Tensor) -> Tensor:\n",
    "\n",
    "#         h_R = self.W_R(edge_attr)\n",
    "#         out = (x_j * attn + h_R).relu()\n",
    "#         return out\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return f'{self.__class__.__name__}(nn={self.nn})'\n",
    "\n",
    "\n",
    "# def masked_softmax(src: Tensor, mask: Tensor, dim: int = -1) -> Tensor:\n",
    "#     out = src.masked_fill(~mask, float('-inf'))\n",
    "#     out = torch.softmax(out, dim=dim)\n",
    "#     out = out.masked_fill(~mask, 0)\n",
    "#     return out\n",
    "\n",
    "# class GraphCare(nn.Module):\n",
    "#     def __init__(self, num_nodes, num_rels, max_visit, embedding_dim, hidden_dim, out_channels, layers=3, dropout=0.5, decay_rate=0.03, node_emb=None, rel_emb=None):\n",
    "#         super(GraphCare, self).__init__()\n",
    "\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.decay_rate = decay_rate\n",
    "\n",
    "#         j = torch.arange(max_visit).float()\n",
    "#         self.lambda_j = torch.exp(self.decay_rate * (max_visit - j)).unsqueeze(0).reshape(1, max_visit, 1).float()\n",
    "\n",
    "#         if node_emb is None:\n",
    "#             self.node_emb = nn.Embedding(num_nodes, embedding_dim)\n",
    "#         else:\n",
    "#             self.node_emb = nn.Embedding.from_pretrained(node_emb, freeze=False)\n",
    "\n",
    "#         if rel_emb is None:\n",
    "#             self.rel_emb = nn.Embedding(num_rels, embedding_dim)\n",
    "#         else:\n",
    "#             self.rel_emb = nn.Embedding.from_pretrained(rel_emb, freeze=False)\n",
    "\n",
    "#         self.lin = nn.Linear(embedding_dim, hidden_dim)\n",
    "#         self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "#         self.layers = layers\n",
    "#         self.dropout = dropout\n",
    "\n",
    "#         self.alpha_attn = nn.ModuleDict()\n",
    "#         self.beta_attn = nn.ModuleDict()\n",
    "#         self.conv = nn.ModuleDict()\n",
    "\n",
    "#         self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "#         for layer in range(1, layers+1):\n",
    "#             self.alpha_attn[str(layer)] = nn.Linear(num_nodes, num_nodes)\n",
    "#             self.beta_attn[str(layer)] = nn.Linear(num_nodes, 1)\n",
    "#             self.conv[str(layer)] = BiAttentionGNNConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=hidden_dim)\n",
    "\n",
    "#         self.MLP = nn.Linear(hidden_dim * 2, out_channels)\n",
    "        \n",
    "\n",
    "#     def to(self, device):\n",
    "#         super().to(device)\n",
    "#         self.lambda_j = self.lambda_j.float().to(device)\n",
    "\n",
    "\n",
    "#     def forward(self, node_ids, rel_ids, edge_index, batch, visit_node, ehr_nodes):\n",
    "#         x = self.node_emb(node_ids).float()\n",
    "#         edge_attr = self.rel_emb(rel_ids).float()\n",
    "\n",
    "#         x = self.bn1(self.lin(x))\n",
    "#         edge_attr = self.bn1(self.lin(edge_attr))\n",
    "\n",
    "\n",
    "#         for layer in range(1, self.layers+1):\n",
    "#             alpha = masked_softmax((self.leakyrelu(self.alpha_attn[str(layer)](visit_node.float()))), mask=visit_node>1, dim=1)\n",
    "#             beta = masked_softmax((self.leakyrelu(self.beta_attn[str(layer)](visit_node.float()))), mask=visit_node>1, dim=0) * self.lambda_j\n",
    "\n",
    "#             attn = alpha * beta\n",
    "#             attn = torch.sum(attn, dim=1)\n",
    "#             xj_node_ids = node_ids[edge_index[0]]\n",
    "#             xj_batch = batch[edge_index[0]]\n",
    "#             attn = attn[xj_batch, xj_node_ids].reshape(-1, 1)\n",
    "\n",
    "#             x = F.relu(self.conv[str(layer)](x, edge_index, edge_attr, attn=attn))\n",
    "#             x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "#         # patient graph embedding through global mean pooling\n",
    "#         x_graph = global_mean_pool(x, batch)\n",
    "#         x_graph = F.dropout(x_graph, p=self.dropout, training=self.training)\n",
    "\n",
    "#         # patient node embedding through local (direct EHR) mean pooling\n",
    "#         x_node = torch.stack([ehr_nodes[i].view(1, -1) @ self.node_emb.weight / torch.sum(ehr_nodes[i]) for i in range(batch.max().item() + 1)])\n",
    "#         x_node = self.lin(x_node).squeeze(1)\n",
    "#         x_node = F.dropout(x_node, p=self.dropout, training=self.training)\n",
    "\n",
    "#         # concatenate patient graph embedding and patient node embedding\n",
    "#         x_concat = torch.cat((x_graph, x_node), dim=1)\n",
    "#         x_concat = F.dropout(x_concat, p=self.dropout, training=self.training)\n",
    "\n",
    "#         # MLP for prediction\n",
    "#         logits = self.MLP(x_concat)\n",
    "\n",
    "#         return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, jaccard_score\n",
    "    \n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    tot_loss = 0\n",
    "    pbar= tqdm(enumerate(train_loader))\n",
    "    for i, data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        node_ids = data.y\n",
    "        rel_ids = data.relation\n",
    "\n",
    "        out = model(\n",
    "                node_ids = node_ids, \n",
    "                rel_ids = rel_ids,\n",
    "                edge_index = data.edge_index,\n",
    "                batch = data.batch,\n",
    "                visit_node = data.visit_padded_node.reshape(int(train_loader.batch_size), int(len(data.visit_padded_node)/train_loader.batch_size), data.visit_padded_node.shape[1]).float(), \n",
    "                ehr_nodes = data.ehr_nodes.reshape(int(train_loader.batch_size), int(len(data.ehr_nodes)/train_loader.batch_size)).float()\n",
    "                \n",
    "            )\n",
    "        try:\n",
    "            label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "        except:\n",
    "            continue\n",
    "        # print(out.shape, label.shape)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.float())\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return tot_loss\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():    \n",
    "            \n",
    "            node_ids = data.y\n",
    "            rel_ids = data.relation\n",
    "\n",
    "            logits = model(\n",
    "                    node_ids = node_ids, \n",
    "                    rel_ids = rel_ids,\n",
    "                    edge_index = data.edge_index,\n",
    "                    batch = data.batch,\n",
    "                    visit_node = data.visit_padded_node.reshape(int(loader.batch_size), int(len(data.visit_padded_node)/loader.batch_size), data.visit_padded_node.shape[1]).float(), \n",
    "                    ehr_nodes = data.ehr_nodes.reshape(int(loader.batch_size), int(len(data.ehr_nodes)/loader.batch_size)).float()               \n",
    "                )\n",
    "\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "\n",
    "            y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "\n",
    "    return y_true_all, y_prob_all\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs):\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer)\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader)\n",
    "\n",
    "        y_pred_all = (y_prob_all >= 0.5).astype(int)\n",
    "        \n",
    "        val_pr_auc = average_precision_score(y_true_all, y_prob_all)\n",
    "        val_roc_auc = roc_auc_score(y_true_all, y_prob_all)\n",
    "        val_jaccard = jaccard_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_acc = accuracy_score(y_true_all, y_pred_all)\n",
    "        val_f1 = f1_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_precision = precision_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_recall = recall_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "\n",
    "        if val_acc >= best_acc and val_f1 >= best_f1:\n",
    "            # torch.save(model.state_dict(), '../../../data/pj20/exp_data/saved_weights_gin_mimic3_readmission_dynamic.pkl')\n",
    "            # print(\"best model saved\")\n",
    "            best_acc = val_acc\n",
    "            best_f1 = val_f1\n",
    "\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}, Val ROC_AUC: {val_roc_auc:.4f}, Val acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val precision: {val_precision:.4f}, Val recall: {val_recall:.4f}, Val jaccard: {val_jaccard:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0]['visit_padded_node'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphCare(\n",
       "  (node_emb): Embedding(4599, 1536)\n",
       "  (rel_emb): Embedding(1077, 1536)\n",
       "  (lin): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (alpha_attn): ModuleDict(\n",
       "    (1): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "    (2): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "    (3): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "  )\n",
       "  (beta_attn): ModuleDict(\n",
       "    (1): Linear(in_features=4599, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=4599, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=4599, out_features=1, bias=True)\n",
       "  )\n",
       "  (conv): ModuleDict(\n",
       "    (1): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "    (2): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "    (3): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  )\n",
       "  (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
       "  (MLP): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GraphCare(\n",
    "    num_nodes=node_emb.shape[0],\n",
    "    num_rels=rel_emb.shape[0],\n",
    "    max_visit=sample_dataset[0]['visit_padded_node'].shape[0],\n",
    "    embedding_dim=node_emb.shape[1],\n",
    "    hidden_dim=512,\n",
    "    out_channels=1,\n",
    "    layers=3,\n",
    "    dropout=0.5,\n",
    "    decay_rate=0.01,\n",
    "    node_emb=node_emb,\n",
    "    rel_emb=rel_emb\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphCare(\n",
       "  (node_emb): Embedding(4599, 1536)\n",
       "  (rel_emb): Embedding(1077, 1536)\n",
       "  (lin): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (alpha_attn): ModuleDict(\n",
       "    (1): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "    (2): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "    (3): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "  )\n",
       "  (beta_attn): ModuleDict(\n",
       "    (1): Linear(in_features=4599, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=4599, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=4599, out_features=1, bias=True)\n",
       "  )\n",
       "  (conv): ModuleDict(\n",
       "    (1): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "    (2): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "    (3): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  )\n",
       "  (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
       "  (MLP): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2519272267818451: : 120it [00:50,  2.40it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 31.448814392089844, Val PRAUC: 0.1051, Val ROC_AUC: 0.6238, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.29536470770835876: : 120it [00:49,  2.44it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training loss: 29.591405868530273, Val PRAUC: 0.1188, Val ROC_AUC: 0.6551, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21529531478881836: : 120it [00:48,  2.47it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training loss: 29.21923065185547, Val PRAUC: 0.1232, Val ROC_AUC: 0.6488, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24924544990062714: : 120it [00:49,  2.44it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training loss: 28.605941772460938, Val PRAUC: 0.1231, Val ROC_AUC: 0.6415, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21240578591823578: : 120it [00:49,  2.41it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training loss: 28.39818000793457, Val PRAUC: 0.1294, Val ROC_AUC: 0.6412, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.47607263922691345: : 120it [00:49,  2.44it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training loss: 28.671480178833008, Val PRAUC: 0.1188, Val ROC_AUC: 0.6379, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21065305173397064: : 120it [00:49,  2.44it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training loss: 28.149593353271484, Val PRAUC: 0.1230, Val ROC_AUC: 0.6470, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21881476044654846: : 120it [00:49,  2.44it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training loss: 27.96839141845703, Val PRAUC: 0.1228, Val ROC_AUC: 0.6403, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.16195553541183472: : 120it [00:48,  2.48it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training loss: 27.971755981445312, Val PRAUC: 0.1227, Val ROC_AUC: 0.6346, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24497215449810028: : 120it [00:48,  2.47it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training loss: 28.16085433959961, Val PRAUC: 0.1254, Val ROC_AUC: 0.6531, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3121532201766968: : 120it [00:48,  2.49it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training loss: 28.187362670898438, Val PRAUC: 0.1312, Val ROC_AUC: 0.6510, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.27527880668640137: : 120it [00:50,  2.40it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training loss: 27.94673728942871, Val PRAUC: 0.1262, Val ROC_AUC: 0.6383, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2870514690876007: : 120it [00:49,  2.43it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training loss: 28.049955368041992, Val PRAUC: 0.1267, Val ROC_AUC: 0.6375, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1105605736374855: : 120it [00:50,  2.36it/s] \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Training loss: 27.855012893676758, Val PRAUC: 0.1241, Val ROC_AUC: 0.6453, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.479207307100296: : 120it [00:48,  2.45it/s]  \n",
      "100%|██████████| 15/15 [00:04<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Training loss: 27.622638702392578, Val PRAUC: 0.1221, Val ROC_AUC: 0.6362, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23939597606658936: : 120it [00:51,  2.33it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Training loss: 27.559215545654297, Val PRAUC: 0.1248, Val ROC_AUC: 0.6409, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20428146421909332: : 120it [00:50,  2.40it/s]\n",
      "100%|██████████| 15/15 [00:04<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Training loss: 27.391435623168945, Val PRAUC: 0.1263, Val ROC_AUC: 0.6380, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.17970827221870422: : 120it [00:50,  2.38it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Training loss: 27.598669052124023, Val PRAUC: 0.1230, Val ROC_AUC: 0.6267, Val acc: 0.9229, Val F1: 0.4800, Val precision: 0.4619, Val recall: 0.4994, Val jaccard: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2720431983470917: : 120it [00:51,  2.35it/s] \n",
      "100%|██████████| 15/15 [00:05<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training loss: 27.585289001464844, Val PRAUC: 0.1264, Val ROC_AUC: 0.6359, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.13986524939537048: : 120it [00:53,  2.25it/s]\n",
      "100%|██████████| 15/15 [00:05<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Training loss: 27.50395393371582, Val PRAUC: 0.1206, Val ROC_AUC: 0.6175, Val acc: 0.9240, Val F1: 0.4802, Val precision: 0.9620, Val recall: 0.5000, Val jaccard: 0.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2579880952835083: : 42it [00:19,  2.13it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pj20/experiment/exp copy.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m train_loop(train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, device\u001b[39m=\u001b[39;49mdevice, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/home/pj20/experiment/exp copy.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, val_loader, model, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m best_f1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     y_true_all, y_prob_all \u001b[39m=\u001b[39m evaluate(model, device, val_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     y_pred_all \u001b[39m=\u001b[39m (y_prob_all \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m)\n",
      "\u001b[1;32m/home/pj20/experiment/exp copy.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# print(out.shape, label.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(out, label\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m training_loss \u001b[39m=\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/exp%20copy.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m tot_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
