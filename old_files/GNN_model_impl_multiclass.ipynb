{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pyhealth.datasets import SampleDataset\n",
    "from pyhealth.datasets import split_by_patient\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "# with open('../../../data/pj20/exp_data/ccscm_ccsproc_atc3/sample_dataset_multiclass_lenofstay_th015.pkl', 'rb') as f:\n",
    "#     sample_dataset = pickle.load(f)\n",
    "\n",
    "# with open('../../../data/pj20/exp_data/ccscm_ccsproc_atc3/graph_multiclass_lenofstay_th015.pkl', 'rb') as f:\n",
    "#     G = pickle.load(f)\n",
    "\n",
    "with open('../../../data/pj20/exp_data/ccscm_ccsproc_atc3/sample_dataset_multiclass_lenofstay_mimic4_th015.pkl', 'rb') as f:\n",
    "    sample_dataset = pickle.load(f)\n",
    "\n",
    "with open('../../../data/pj20/exp_data/ccscm_ccsproc_atc3/graph_multiclass_lenofstay_mimic4_th015.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "G_tg = from_networkx(G) \n",
    "\n",
    "# filt_dataset = []\n",
    "\n",
    "# for patient in sample_dataset:\n",
    "#     if len(patient['node_set']) != 0:\n",
    "#         filt_dataset.append(patient)\n",
    "# filt_dataset = SampleDataset(samples=filt_dataset)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "c_v, p_v, d_v = [], [], []\n",
    "\n",
    "for patient in sample_dataset:\n",
    "    c_v.append(len(patient['conditions']))\n",
    "    p_v.append(len(patient['procedures']))\n",
    "print(max(c_v), max(p_v))\n",
    "max_visits = max(c_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "\n",
    "def get_subgraph(G, dataset, idx):\n",
    "    patient = dataset[idx]\n",
    "    while len(patient['node_set']) == 0:\n",
    "        idx -= 1\n",
    "        patient = dataset[idx]\n",
    "    # L = G.edge_subgraph(torch.tensor([*patient['node_set']]))\n",
    "    P = G.subgraph(torch.tensor([*patient['node_set']]))\n",
    "    P.label = patient['drugs_ind']\n",
    "    P.visits_cond = patient['visit_node_set_condition']\n",
    "    P.visits_proc = patient['visit_node_set_procedure']\n",
    "    \n",
    "    return P\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, G, dataset):\n",
    "        self.G = G\n",
    "        self.dataset=dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        return get_subgraph(G=self.G, dataset=self.dataset, idx=idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0.,  ..., 0., 0., 1.])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*train_dataset[1]['visit_node_set_condition']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GINConv, HGTConv\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.loader import DataListLoader\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1)\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # print(x.shape)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # print(x.shape)\n",
    "        logits = self.fc(x)\n",
    "        # print(logits.shape)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Linear(in_channels, hidden_channels))\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GINX(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, hidden_channels, out_channels, word_emb=None):\n",
    "        super(GINX, self).__init__()\n",
    "        \n",
    "        if word_emb == None:\n",
    "            self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "            self.conv1 = GINConv(Linear(embedding_dim, hidden_channels))\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(word_emb, freeze=False)\n",
    "            self.conv1 = GINConv(Linear(word_emb.shape[1], hidden_channels))\n",
    "\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, node_ids, edge_index, batch):\n",
    "        x = self.embedding(node_ids)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from pyhealth.models import RETAINLayer\n",
    "\n",
    "class GraphCare(nn.Module):\n",
    "    def __init__(self, num_nodes, feature_keys, embedding_dim, hidden_dim, out_channels, dropout=0.5, max_visits=None, word_emb=None, use_attn=True):\n",
    "        super(GraphCare, self).__init__()\n",
    "        self.max_visits = max_visits\n",
    "        self.max_nodes = len(word_emb)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_attn = use_attn\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "        if word_emb == None:\n",
    "            self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(word_emb, freeze=True)\n",
    "        \n",
    "        self.retain = nn.ModuleDict()\n",
    "        for feature_key in feature_keys:\n",
    "            self.retain[feature_key] = RETAINLayer(feature_size=self.max_nodes, dropout=dropout)\n",
    "        \n",
    "        self.conv1 = GINEConv(nn.Linear(embedding_dim, hidden_dim), edge_dim=1)\n",
    "        self.conv2 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=1)\n",
    "        self.conv3 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, node_ids, edge_index, batch, visits_cond, visits_proc):\n",
    "        x = self.embedding(node_ids)\n",
    "\n",
    "        if self.use_attn == True:\n",
    "\n",
    "            cond_attn = self.retain['cond'](visits_cond)\n",
    "            proc_attn = self.retain['proc'](visits_proc)\n",
    "            cross_attn = self.retain['cross'](visits_cond + visits_proc)\n",
    "\n",
    "            attn = cond_attn.add_(proc_attn).add_(cross_attn)    # (batch_size, max_nodes)\n",
    "\n",
    "            # Create a batch index tensor to map the batch index to the corresponding attention weight\n",
    "            batch_index = torch.arange(attn.size(0), device=node_ids.device).repeat_interleave(torch.bincount(batch))   \n",
    "            # print(\"batch index shape: \", batch_index.shape)\n",
    "            # print(\"edge index shape: \", edge_index.shape)\n",
    "            # Fill the attn_weights matrix with the correct weights using batch_index and node_ids\n",
    "            attn_weights = attn[batch_index, node_ids]\n",
    "            # Multiply the embeddings with the corresponding attention weights\n",
    "            # x = x * attn_weights\n",
    "            # x = (1 - self.alpha) * x + self.alpha * x\n",
    "            row, col = edge_index\n",
    "            # Define a small constant value epsilon\n",
    "            epsilon = 1e-6\n",
    "\n",
    "            # Calculate the geometric mean with added epsilon\n",
    "            # Normalize the attn_weights and replace NaNs with 0s\n",
    "            attn_weights = attn_weights / torch.max(attn_weights)\n",
    "            attn_weights = torch.where(torch.isnan(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n",
    "\n",
    "            # Calculate the geometric mean with added epsilon\n",
    "            edge_attr = ((attn_weights[row] + epsilon) + (attn_weights[col] + epsilon)).unsqueeze(-1)\n",
    "\n",
    "            \n",
    "            # print(\"row shape: \", row.shape) \n",
    "            # print(\"col shape: \", col.shape)\n",
    "            # print(\"attn shape: \", attn.shape)\n",
    "            # print(\"attn_weights shape: \", attn_weights.shape)\n",
    "            # print(\"edge_attr shape: \", edge_attr.shape)\n",
    "            # print(\"x shape: \", x.shape)\n",
    "\n",
    "\n",
    "        # Apply the first GIN layer\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        # Apply the second GIN layer\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, jaccard_score\n",
    "\n",
    "def train(model, device, train_loader, optimizer, model_):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    tot_loss = 0\n",
    "    pbar = tqdm(train_loader)\n",
    "    for data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if model_ == \"GIN\":\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "        elif model_ == \"GINX\":\n",
    "            out = model(data.y, data.edge_index, data.batch)\n",
    "        else:\n",
    "            out = model(\n",
    "                    data.y, \n",
    "                    data.edge_index, \n",
    "                    data.batch, \n",
    "                    data.visits_cond.reshape(int(train_loader.batch_size), int(len(data.visits_cond)/train_loader.batch_size), data.visits_cond.shape[1]).double(), \n",
    "                    data.visits_proc.reshape(int(train_loader.batch_size), int(len(data.visits_proc)/train_loader.batch_size), data.visits_proc.shape[1]).double(), \n",
    "                )\n",
    "        try:\n",
    "            label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        loss = F.cross_entropy(out, label)\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return tot_loss\n",
    "\n",
    "def evaluate(model, device, loader, model_):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if model_ == \"GIN\":\n",
    "                logits = model(data.x, data.edge_index, data.batch)\n",
    "            elif model_ == \"GINX\":\n",
    "                logits = model(data.y, data.edge_index, data.batch)\n",
    "            else:\n",
    "                logits = model(\n",
    "                    data.y, \n",
    "                    data.edge_index, \n",
    "                    data.batch, \n",
    "                    data.visits_cond.reshape(int(loader.batch_size), int(len(data.visits_cond)/loader.batch_size), data.visits_cond.shape[1]).double(), \n",
    "                    data.visits_proc.reshape(int(loader.batch_size), int(len(data.visits_proc)/loader.batch_size), data.visits_proc.shape[1]).double(), \n",
    "                )\n",
    "\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "            try:\n",
    "                y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "            except:\n",
    "                continue\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "    # pr_auc = multilabel_metrics_fn(y_true=y_true_all, y_prob=y_true_all, metrics=\"pr_auc_macro\")\n",
    "\n",
    "    return y_true_all, y_prob_all\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs, model_):\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer, model_)\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader, model_)\n",
    "\n",
    "        y_pred_all = (y_prob_all >= 0.5).astype(int)\n",
    "        \n",
    "        roc_auc_weighted_ovr = sklearn_metrics.roc_auc_score(\n",
    "            y_true, y_prob, average=\"weighted\", multi_class=\"ovr\"\n",
    "        )\n",
    "\n",
    "        if val_acc >= best_acc and val_f1 >= best_f1:\n",
    "            torch.save(model.state_dict(), '../../../data/pj20/exp_data/saved_weights_gin_mimic3_readmission_dynamic.pkl')\n",
    "            print(\"best model saved\")\n",
    "            best_acc = val_acc\n",
    "            best_f1 = val_f1\n",
    "\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}, Val ROC_AUC: {val_roc_auc:.4f}, Val acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val precision: {val_precision:.4f}, Val recall: {val_recall:.4f}, Val jaccard: {val_jaccard:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_tg.x = torch.randn(G_tg.num_nodes, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_set = Dataset(G=G_tg, dataset=train_dataset)\n",
    "val_set = Dataset(G=G_tg, dataset=val_dataset)\n",
    "test_set = Dataset(G=G_tg, dataset=test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphCare(\n",
       "  (embedding): Embedding(1000, 1024)\n",
       "  (retain): ModuleDict(\n",
       "    (cond): RETAINLayer(\n",
       "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "      (alpha_gru): GRU(1000, 1000, batch_first=True)\n",
       "      (beta_gru): GRU(1000, 1000, batch_first=True)\n",
       "      (alpha_li): Linear(in_features=1000, out_features=1, bias=True)\n",
       "      (beta_li): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    )\n",
       "    (proc): RETAINLayer(\n",
       "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "      (alpha_gru): GRU(1000, 1000, batch_first=True)\n",
       "      (beta_gru): GRU(1000, 1000, batch_first=True)\n",
       "      (alpha_li): Linear(in_features=1000, out_features=1, bias=True)\n",
       "      (beta_li): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    )\n",
       "    (cross): RETAINLayer(\n",
       "      (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "      (alpha_gru): GRU(1000, 1000, batch_first=True)\n",
       "      (beta_gru): GRU(1000, 1000, batch_first=True)\n",
       "      (alpha_li): Linear(in_features=1000, out_features=1, bias=True)\n",
       "      (beta_li): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (conv1): GINEConv(nn=Linear(in_features=1024, out_features=512, bias=True))\n",
       "  (conv2): GINEConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  (conv3): GINEConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  (fc): Linear(in_features=512, out_features=197, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = \"GINX\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if model_ == \"GIN\":\n",
    "    in_channels = train_set[0].x.shape[1]\n",
    "    model = GIN(in_channels=in_channels, out_channels=1, hidden_channels=512).to(device)\n",
    "    # model = GAT(in_channels=in_channels, out_channels=1, hidden_channels=256, heads=3).to(device)\n",
    "    # model = HGT(in_channels=in_channels, out_channels=out_channels, hidden_channels=512, heads=2).to(device)\n",
    "elif model_ == \"GINX\":\n",
    "    model = GINX(num_nodes=G_tg.num_nodes, embedding_dim=512, hidden_channels=512, out_channels=1, word_emb=G_tg.x).to(device)\n",
    "\n",
    "elif model_ == \"GraphCare\":\n",
    "    # model = GINX(num_nodes=G_tg.num_nodes, embedding_dim=512, hidden_channels=512, out_channels=out_channels, word_emb=G_tg.x).to(device)\n",
    "    model = GraphCare(\n",
    "        num_nodes=G_tg.num_nodes,\n",
    "        feature_keys=['cond', 'proc', 'cross'], \n",
    "        embedding_dim=len(G_tg.x[0]), \n",
    "        hidden_dim=512, \n",
    "        out_channels=1, \n",
    "        dropout=0.5, \n",
    "        max_visits=max_visits,\n",
    "        word_emb=G_tg.x,\n",
    "        use_attn=True\n",
    "    ).to(device)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.3190099504377803: 100%|██████████| 2223/2223 [17:14<00:00,  2.15it/s] \n",
      "100%|██████████| 271/271 [00:54<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 1, Training loss: 31558.401586260756, Val PRAUC: 0.6461, Val ROC_AUC: 0.9004, Val F1-score: 0.4687, Val Jaccard: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23548025798127212: 100%|██████████| 2223/2223 [17:08<00:00,  2.16it/s]\n",
      "100%|██████████| 271/271 [01:10<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 2, Training loss: 668.3661525146066, Val PRAUC: 0.6466, Val ROC_AUC: 0.9029, Val F1-score: 0.4687, Val Jaccard: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.28290939595346204: 100%|██████████| 2223/2223 [18:00<00:00,  2.06it/s]\n",
      "100%|██████████| 271/271 [00:56<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 3, Training loss: 607.9846786465552, Val PRAUC: 0.6466, Val ROC_AUC: 0.9031, Val F1-score: 0.4687, Val Jaccard: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2730543356437892: 100%|██████████| 2223/2223 [18:13<00:00,  2.03it/s] \n",
      "100%|██████████| 271/271 [01:43<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training loss: 592.9134928369855, Val PRAUC: 0.6467, Val ROC_AUC: 0.9030, Val F1-score: 0.4687, Val Jaccard: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.26231648551205394: 100%|██████████| 2223/2223 [17:35<00:00,  2.11it/s]\n",
      "100%|██████████| 271/271 [00:57<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training loss: 588.685593257485, Val PRAUC: 0.6466, Val ROC_AUC: 0.9031, Val F1-score: 0.4687, Val Jaccard: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2539298038136868: 100%|██████████| 2223/2223 [17:45<00:00,  2.09it/s] \n",
      "100%|██████████| 271/271 [00:57<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 6, Training loss: 587.4888132497164, Val PRAUC: 0.6467, Val ROC_AUC: 0.9031, Val F1-score: 0.4590, Val Jaccard: 0.3094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2947981430612893: 100%|██████████| 2223/2223 [17:55<00:00,  2.07it/s] \n",
      "100%|██████████| 271/271 [00:57<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training loss: 587.1197404865286, Val PRAUC: 0.6467, Val ROC_AUC: 0.9031, Val F1-score: 0.4687, Val Jaccard: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24527524525398567: 100%|██████████| 2223/2223 [17:42<00:00,  2.09it/s]\n",
      "100%|██████████| 271/271 [00:56<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training loss: 586.9825388997497, Val PRAUC: 0.6466, Val ROC_AUC: 0.9031, Val F1-score: 0.4687, Val Jaccard: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2842615063126055:  77%|███████▋  | 1709/2223 [13:48<04:09,  2.06it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m train_loop(train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, device\u001b[39m=\u001b[39;49mdevice, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, static\u001b[39m=\u001b[39;49mstatic)\n",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, val_loader, model, optimizer, device, epochs, static)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m best_roc_auc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m     loss \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer, static)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     y_true_all, y_prob_all \u001b[39m=\u001b[39m evaluate(model, device, val_loader, static)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     y_pred_all \u001b[39m=\u001b[39m y_prob_all\u001b[39m.\u001b[39mcopy()\n",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, static)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     out \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m             data\u001b[39m.\u001b[39;49my, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m             data\u001b[39m.\u001b[39;49medge_index, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m             data\u001b[39m.\u001b[39;49mbatch, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m             data\u001b[39m.\u001b[39;49mvisits_cond\u001b[39m.\u001b[39;49mreshape(\u001b[39mint\u001b[39;49m(train_loader\u001b[39m.\u001b[39;49mbatch_size), \u001b[39mint\u001b[39;49m(\u001b[39mlen\u001b[39;49m(data\u001b[39m.\u001b[39;49mvisits_cond)\u001b[39m/\u001b[39;49mtrain_loader\u001b[39m.\u001b[39;49mbatch_size), data\u001b[39m.\u001b[39;49mvisits_cond\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mdouble(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m             data\u001b[39m.\u001b[39;49mvisits_proc\u001b[39m.\u001b[39;49mreshape(\u001b[39mint\u001b[39;49m(train_loader\u001b[39m.\u001b[39;49mbatch_size), \u001b[39mint\u001b[39;49m(\u001b[39mlen\u001b[39;49m(data\u001b[39m.\u001b[39;49mvisits_proc)\u001b[39m/\u001b[39;49mtrain_loader\u001b[39m.\u001b[39;49mbatch_size), data\u001b[39m.\u001b[39;49mvisits_proc\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mdouble(), \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m label \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mreshape(\u001b[39mint\u001b[39m(train_loader\u001b[39m.\u001b[39mbatch_size), \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mlabel)\u001b[39m/\u001b[39mtrain_loader\u001b[39m.\u001b[39mbatch_size))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(out, label)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl.ipynb Cell 11\u001b[0m in \u001b[0;36mGraphCare.forward\u001b[0;34m(self, node_ids, edge_index, batch, visits_cond, visits_proc)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_attn \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     cond_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretain[\u001b[39m'\u001b[39m\u001b[39mcond\u001b[39m\u001b[39m'\u001b[39m](visits_cond)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     proc_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretain[\u001b[39m'\u001b[39;49m\u001b[39mproc\u001b[39;49m\u001b[39m'\u001b[39;49m](visits_proc)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     cross_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretain[\u001b[39m'\u001b[39m\u001b[39mcross\u001b[39m\u001b[39m'\u001b[39m](visits_cond \u001b[39m+\u001b[39m visits_proc)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     attn \u001b[39m=\u001b[39m cond_attn\u001b[39m.\u001b[39madd_(proc_attn)\u001b[39m.\u001b[39madd_(cross_attn)    \u001b[39m# (batch_size, max_nodes)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/pyhealth/models/retain.py:105\u001b[0m, in \u001b[0;36mRETAINLayer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    103\u001b[0m     lengths \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(mask\u001b[39m.\u001b[39mint(), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    104\u001b[0m rx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreverse_x(x, lengths)\n\u001b[0;32m--> 105\u001b[0m attn_alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_alpha(rx, lengths)\n\u001b[1;32m    106\u001b[0m attn_beta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_beta(rx, lengths)\n\u001b[1;32m    107\u001b[0m c \u001b[39m=\u001b[39m attn_alpha \u001b[39m*\u001b[39m attn_beta \u001b[39m*\u001b[39m x  \u001b[39m# (patient, sequence len, feature_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/pyhealth/models/retain.py:65\u001b[0m, in \u001b[0;36mRETAINLayer.compute_alpha\u001b[0;34m(self, rx, lengths)\u001b[0m\n\u001b[1;32m     61\u001b[0m rx \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mpack_padded_sequence(\n\u001b[1;32m     62\u001b[0m     rx, lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     63\u001b[0m )\n\u001b[1;32m     64\u001b[0m g, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha_gru(rx)\n\u001b[0;32m---> 65\u001b[0m g, _ \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39;49mpad_packed_sequence(g, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     66\u001b[0m attn_alpha \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha_li(g), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m attn_alpha\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch/nn/utils/rnn.py:334\u001b[0m, in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m unsorted_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m batch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m padded_output\u001b[39m.\u001b[39mindex_select(batch_dim, unsorted_indices), lengths[unsorted_indices]\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m padded_output, lengths\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100, static=static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in test_loader:\n",
    "    print(data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gat_mimic3_drugrec.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gin_mimic3_drugrec_random.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_hgt_mimic3_drugrec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('./exp_data/saved_weights_gat_mimic3_drugrec.pkl'))\n",
    "model.load_state_dict(torch.load('../../../data/pj20/exp_data/saved_weights_graph_mimic3_drugrec_th02.pkl'))\n",
    "model.double()\n",
    "\n",
    "y_true_all, y_prob_all = evaluate(model, device, val_loader, static)\n",
    "\n",
    "y_pred_all = y_prob_all.copy()\n",
    "y_pred_all[y_pred_all >= 0.5] = 1\n",
    "y_pred_all[y_pred_all < 0.5] = 0\n",
    "\n",
    "test_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "test_roc_auc = roc_auc_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "test_f1 = f1_score(y_true_all, y_pred_all, average='samples')\n",
    "test_jaccard = jaccard_score(y_true_all, y_pred_all, average='samples')\n",
    "\n",
    "print(f'test PRAUC: {test_pr_auc:.4f}, test ROC_AUC: {test_roc_auc:.4f}, test F1-score: {test_f1:.4f}, test Jaccard: {test_jaccard:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
