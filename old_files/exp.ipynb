{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"lenofstay\"\n",
    "\n",
    "voc_ratios = [\n",
    "    0.1, \n",
    "    0.2,\n",
    "    0.3,\n",
    "    # 0.4,\n",
    "    # 0.5,\n",
    "    # 0.6,\n",
    "    # 0.7,\n",
    "    # 0.8,\n",
    "    # 0.9,\n",
    "]\n",
    "\n",
    "   \n",
    "data_idxs = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def process(d):\n",
    "    new_d = defaultdict(list)\n",
    "    for key, value in d.items():\n",
    "        if key == \"conditions\" or key == \"procedures\" or key == \"drugs\" or key == \"label\" or key == \"patient_id\" or key == \"visit_id\":\n",
    "            new_d[key] = value\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for j in range(3):\n",
    "    for ratio in voc_ratios:\n",
    "        with open(f'/data/pj20/exp_data/ccscm_ccsproc/sample_dataset_mimic3_{task}_th015_mask_{ratio}_{j}.pkl', 'rb') as f:\n",
    "            sample_dataset = pickle.load(f)\n",
    "    \n",
    "        sample_dataset = list(sample_dataset)\n",
    "        for i in range(len(sample_dataset)):\n",
    "            sample_dataset[i] = process(sample_dataset[i])\n",
    "        \n",
    "        with open(f'/data/pj20/exp_data/ccscm_ccsproc/sample_dataset_mimic3_{task}_th015_mask_{ratio}_{j}_pyhealth.pkl', 'wb') as f:\n",
    "            pickle.dump(sample_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4600])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.append(sample_dataset[0]['visit_padded_node'].numpy(), 0)).reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_nodes_from([\n",
    "    (4599, {'y': int(4599), 'x': np.zeros((1, ent_emb.shape[1])).tolist()[0]})\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/pj20/exp_data/ccscm_ccsproc_atc3/attention_weights_readmission.pkl\", \"rb\") as f:\n",
    "    attention_weights = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08184401],\n",
       "       [0.10589472],\n",
       "       [0.20249375],\n",
       "       ...,\n",
       "       [0.0846555 ],\n",
       "       [0.06695282],\n",
       "       [0.21378001]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.sum(torch.randn(1, 2, 3, 4), dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4600, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(attention_weights, [0]).reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tg = from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4599, 1536], edge_index=[2, 62127], y=[4599], relation=[62127])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "zero_emb = np.zeros((1, ent_emb.shape[1])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/clusters_inv_th015.json', 'r') as f:\n",
    "    map_cluster_inv = json.load(f)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/clusters_th015.json', 'r') as f:\n",
    "    map_cluster = json.load(f)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/ccscm_id2clus.json', 'r') as f:\n",
    "    ccscm_id2clus = json.load(f)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/ccsproc_id2clus.json', 'r') as f:\n",
    "    ccsproc_id2clus = json.load(f)\n",
    "\n",
    "# with open('/data/pj20/exp_data/ccscm_ccsproc/atc3_id2clus.json', 'r') as f:\n",
    "#     atc3_id2clus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2755"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "map_cluster_inv['50000'] = '4599'\n",
    "\n",
    "new_d = {}\n",
    "new_d['nodes'] = [50000]\n",
    "new_d['embeddings'] = zero_emb\n",
    "new_d['attention_mortality'] = float(0)\n",
    "new_d['attention_readmission'] = float(0)\n",
    "\n",
    "map_cluster['4599'] = new_d\n",
    "\n",
    "ccscm_id2clus['0'] = \"4599\"\n",
    "ccsproc_id2clus['0'] = \"4599\"\n",
    "# atc3_id2clus['0'] = \"4599\"\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/clusters_inv_th015_ana.json', 'w') as f:\n",
    "    json.dump(map_cluster_inv, f, indent=6)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/clusters_th015_ana.json', 'w') as f:\n",
    "    json.dump(map_cluster, f, indent=6)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/ccscm_id2clus_ana.json', 'w') as f:\n",
    "    json.dump(ccscm_id2clus, f, indent=6)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/ccsproc_id2clus_ana.json', 'w') as f:\n",
    "    json.dump(ccsproc_id2clus, f, indent=6)\n",
    "\n",
    "# with open('/data/pj20/exp_data/ccscm_ccsproc/atc3_id2clus_ana.json', 'w') as f:\n",
    "#     json.dump(atc3_id2clus, f, indent=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "map_cluster_inv['50000'] = '2755'\n",
    "\n",
    "new_d = {}\n",
    "new_d['nodes'] = [50000]\n",
    "new_d['embeddings'] = zero_emb\n",
    "new_d['attention_mortality'] = float(0)\n",
    "new_d['attention_readmission'] = float(0)\n",
    "\n",
    "map_cluster['2755'] = new_d\n",
    "\n",
    "ccscm_id2clus['0'] = \"2755\"\n",
    "ccsproc_id2clus['0'] = \"2755\"\n",
    "# atc3_id2clus['0'] = \"2755\"\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/clusters_inv_th015_ana.json', 'w') as f:\n",
    "    json.dump(map_cluster_inv, f, indent=6)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/clusters_th015_ana.json', 'w') as f:\n",
    "    json.dump(map_cluster, f, indent=6)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/ccscm_id2clus_ana.json', 'w') as f:\n",
    "    json.dump(ccscm_id2clus, f, indent=6)\n",
    "\n",
    "with open('/data/pj20/exp_data/ccscm_ccsproc/ccsproc_id2clus_ana.json', 'w') as f:\n",
    "    json.dump(ccsproc_id2clus, f, indent=6)\n",
    "\n",
    "# with open('/data/pj20/exp_data/ccscm_ccsproc/atc3_id2clus_ana.json', 'w') as f:\n",
    "#     json.dump(atc3_id2clus, f, indent=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [1855, 12262, 12427, 12463, 13996, 15340],\n",
       " 'embedding': [[-0.024169489000000002,\n",
       "   0.005908007066666666,\n",
       "   0.009427462499999999,\n",
       "   -0.03270923383333333,\n",
       "   -0.016854609666666666,\n",
       "   0.02583410883333333,\n",
       "   -0.03445191733333333,\n",
       "   -0.007291627716666668,\n",
       "   -0.009447122566666667,\n",
       "   -0.011272293275,\n",
       "   0.009516451399999999,\n",
       "   0.016091675833333333,\n",
       "   -0.0011152709333333334,\n",
       "   0.008967516716666665,\n",
       "   0.005806045166666667,\n",
       "   0.0044882192333333334,\n",
       "   0.04350491833333334,\n",
       "   -0.008243818866666667,\n",
       "   0.006098547266666666,\n",
       "   -0.043683581833333325,\n",
       "   -0.017771537398333332,\n",
       "   -0.0098944165,\n",
       "   0.004230811316666667,\n",
       "   0.0084989289,\n",
       "   -0.010257851178333334,\n",
       "   -0.01127160431,\n",
       "   0.013979220083333334,\n",
       "   -0.024441926666666666,\n",
       "   -0.004481958311066667,\n",
       "   0.0028877146250000005,\n",
       "   0.026744374833333334,\n",
       "   0.004555557078333333,\n",
       "   -0.018772000333333334,\n",
       "   -0.033216114666666664,\n",
       "   -0.015670728833333335,\n",
       "   -0.012785657483333332,\n",
       "   -0.014909380833333333,\n",
       "   -0.012378851699999999,\n",
       "   0.019959114333333333,\n",
       "   -0.0037218914083333335,\n",
       "   0.0034949010833333336,\n",
       "   -0.009386672433333333,\n",
       "   -0.005157473433333334,\n",
       "   -0.00044699161666666667,\n",
       "   -0.019168975,\n",
       "   -0.002764298566666667,\n",
       "   -0.024741576166666668,\n",
       "   -0.0008931855166666667,\n",
       "   0.002479706266666666,\n",
       "   0.0011000765,\n",
       "   0.030122788666666667,\n",
       "   0.012581291583333334,\n",
       "   -0.033768269999999996,\n",
       "   -0.008396534283333335,\n",
       "   0.0016460033166666667,\n",
       "   0.014834224333333331,\n",
       "   -0.021810174,\n",
       "   0.008182048983333334,\n",
       "   0.002659083783333333,\n",
       "   -0.007606079443333334,\n",
       "   0.020092478333333334,\n",
       "   0.008141943016666665,\n",
       "   0.0008257944316666664,\n",
       "   0.0150438065,\n",
       "   -0.0161656775,\n",
       "   0.020761291766666665,\n",
       "   -0.023230372333333332,\n",
       "   0.006706488899999999,\n",
       "   -0.008415754578333332,\n",
       "   -0.009473761666666669,\n",
       "   0.0391293115,\n",
       "   0.017537581616666666,\n",
       "   0.010496313983333332,\n",
       "   0.009332652943333334,\n",
       "   0.012185989583333334,\n",
       "   -0.00787634535,\n",
       "   0.0007543610216666667,\n",
       "   -0.012080207116666667,\n",
       "   -0.013296086083333334,\n",
       "   0.025617766,\n",
       "   0.010261518033333332,\n",
       "   -0.0157950365,\n",
       "   -0.021600222666666665,\n",
       "   0.013506618633333333,\n",
       "   -0.0145122382,\n",
       "   0.022915806166666667,\n",
       "   -0.0013867335083333331,\n",
       "   0.02888536133333333,\n",
       "   -0.0001537899499999999,\n",
       "   -0.0017698301166666668,\n",
       "   0.01803692163333333,\n",
       "   0.007970551566666666,\n",
       "   0.011206679833333332,\n",
       "   0.00655115705,\n",
       "   -0.005607410741666667,\n",
       "   0.008651194085,\n",
       "   -0.009025088483333332,\n",
       "   0.02491707333333333,\n",
       "   0.016142356,\n",
       "   -0.0376466565,\n",
       "   -0.019770341916666666,\n",
       "   -7.458429333333335e-05,\n",
       "   -0.022864464,\n",
       "   -0.005393376629999999,\n",
       "   -0.031227830499999998,\n",
       "   0.0065385586,\n",
       "   0.006450438021666667,\n",
       "   0.0029949787666666665,\n",
       "   0.02509998416666667,\n",
       "   -0.020564991066666663,\n",
       "   -0.014018531333333334,\n",
       "   0.02408216116666667,\n",
       "   0.015483864883333332,\n",
       "   -0.025404547166666666,\n",
       "   0.013944094300000001,\n",
       "   -0.027914024833333332,\n",
       "   0.014820900899999998,\n",
       "   0.019828153166666668,\n",
       "   -0.016367811083333333,\n",
       "   -0.017640544583333334,\n",
       "   0.009082337933333332,\n",
       "   0.015395078500000001,\n",
       "   0.034913575,\n",
       "   -0.012033068833333334,\n",
       "   -0.0008461077500000001,\n",
       "   -0.013066928800000002,\n",
       "   -0.04028114766666666,\n",
       "   0.001099634016666667,\n",
       "   -0.025573892833333334,\n",
       "   -0.007473485266666666,\n",
       "   0.02630871,\n",
       "   0.01760436705,\n",
       "   0.024635840333333336,\n",
       "   -0.01177668666666667,\n",
       "   -0.005862717233333333,\n",
       "   0.019913869533333333,\n",
       "   -0.009562092583333334,\n",
       "   -0.009158955016666666,\n",
       "   -0.036211728500000005,\n",
       "   -0.021631789666666665,\n",
       "   -0.0019355520666666664,\n",
       "   0.02320153966666667,\n",
       "   -0.00688420025,\n",
       "   0.003842056733333333,\n",
       "   -0.009932188533333334,\n",
       "   0.009960372083333334,\n",
       "   0.0463368085,\n",
       "   0.004179004133333333,\n",
       "   -0.006714860938333333,\n",
       "   -0.006124557816666667,\n",
       "   0.004884688066666667,\n",
       "   -0.033599749000000005,\n",
       "   -0.0010081533999999998,\n",
       "   -0.022285424833333334,\n",
       "   -0.010384915733333331,\n",
       "   0.00926558215,\n",
       "   0.006426010970000001,\n",
       "   0.00110631175,\n",
       "   -0.013282784833333333,\n",
       "   0.0016240268733333334,\n",
       "   -0.024053579500000002,\n",
       "   -0.01261775205,\n",
       "   0.013952771916666667,\n",
       "   -0.0006217760666666666,\n",
       "   0.01837793316666667,\n",
       "   0.03333603016666667,\n",
       "   -0.014146242545000002,\n",
       "   0.028708434333333335,\n",
       "   -0.016334468166666668,\n",
       "   -0.01929371016666667,\n",
       "   0.01599304476666667,\n",
       "   0.0017423886266666668,\n",
       "   -0.03131763966666667,\n",
       "   0.0015014085,\n",
       "   0.0056568249166666675,\n",
       "   0.003244438633333333,\n",
       "   0.006368911816666667,\n",
       "   0.029824328166666667,\n",
       "   -0.026982178833333332,\n",
       "   -0.009001369050000001,\n",
       "   -0.019264028833333332,\n",
       "   0.013027260566666666,\n",
       "   0.038227508333333333,\n",
       "   0.012623976233333334,\n",
       "   -0.009592874923333333,\n",
       "   -0.0021550268000000003,\n",
       "   0.0059728930333333334,\n",
       "   -0.014171865233333336,\n",
       "   -0.007167105193333335,\n",
       "   -0.007187626083333333,\n",
       "   0.0105671772,\n",
       "   0.024476448666666664,\n",
       "   -0.0017056841666666664,\n",
       "   -0.018281266166666667,\n",
       "   -0.6814585316666667,\n",
       "   -0.03065779366666667,\n",
       "   0.015759184883333333,\n",
       "   -0.03058685833333333,\n",
       "   0.022575303333333335,\n",
       "   0.002948883015,\n",
       "   0.015967102333333334,\n",
       "   -0.005453243649999999,\n",
       "   4.704679999999972e-05,\n",
       "   0.0031365824000000003,\n",
       "   -0.01890782783333333,\n",
       "   0.0013305923166666673,\n",
       "   -0.007930313033333334,\n",
       "   0.012013369345000002,\n",
       "   -0.0027729849333333334,\n",
       "   -0.004362954783333333,\n",
       "   -0.004923014116666667,\n",
       "   -0.020707190666666663,\n",
       "   0.009736820766666666,\n",
       "   0.00462153995,\n",
       "   0.007166609833333333,\n",
       "   0.009874669683333333,\n",
       "   -0.015127827916666664,\n",
       "   0.019826520099999998,\n",
       "   0.0061107287474999996,\n",
       "   0.0029168620666666658,\n",
       "   0.009916572166666667,\n",
       "   -0.00865324284,\n",
       "   -0.01920511683333333,\n",
       "   -0.009507973483333335,\n",
       "   -0.006809880599999999,\n",
       "   0.024508374833333332,\n",
       "   0.0012626125766666667,\n",
       "   -0.015150189949999997,\n",
       "   0.047937496833333336,\n",
       "   0.006849905133333334,\n",
       "   -0.011573218233333333,\n",
       "   0.017283608000000002,\n",
       "   -0.0014055770933333331,\n",
       "   0.04102355783333333,\n",
       "   -0.0128061365,\n",
       "   0.002838431216666666,\n",
       "   0.009025222883333335,\n",
       "   -0.0013577710166666663,\n",
       "   0.014565185483333335,\n",
       "   0.0030863771166666667,\n",
       "   0.0184183524,\n",
       "   -0.0056766098833333336,\n",
       "   -0.011005014666666667,\n",
       "   -0.03400401066666666,\n",
       "   0.0094711963,\n",
       "   0.027207935666666672,\n",
       "   0.014827416666666668,\n",
       "   0.007803231649999999,\n",
       "   0.016738822000000004,\n",
       "   0.005342958166666666,\n",
       "   0.02306599866666667,\n",
       "   -0.017728472666666665,\n",
       "   0.010832484866666667,\n",
       "   0.0059396065699999995,\n",
       "   -0.0013413650833333333,\n",
       "   0.018157172166666666,\n",
       "   -0.006437179516666666,\n",
       "   -0.02444840283333333,\n",
       "   -0.009704803741666667,\n",
       "   0.026572149833333333,\n",
       "   0.006943798183333333,\n",
       "   0.0018035349216666667,\n",
       "   0.020323508,\n",
       "   -0.026961500666666666,\n",
       "   0.022691737416666663,\n",
       "   0.002335764896666667,\n",
       "   -0.0038058118999999995,\n",
       "   -0.011585706833333332,\n",
       "   0.00931863699,\n",
       "   0.024562050166666665,\n",
       "   -0.00023030964333333338,\n",
       "   -0.008166598500000002,\n",
       "   0.0024759057,\n",
       "   -0.005579657333333334,\n",
       "   -0.0001426832666666667,\n",
       "   0.006393527216666667,\n",
       "   -0.007393394066666666,\n",
       "   -0.004887659100000001,\n",
       "   0.03752798533333333,\n",
       "   -0.014496365966666667,\n",
       "   -0.009546506433333333,\n",
       "   -0.0031696017599999998,\n",
       "   -0.0059956672,\n",
       "   -0.009239425416666667,\n",
       "   0.0015422763233333334,\n",
       "   0.023877771500000002,\n",
       "   -0.017774154166666667,\n",
       "   -0.0323474405,\n",
       "   -0.0034514558333333333,\n",
       "   0.0031318858333333334,\n",
       "   -0.022281275333333333,\n",
       "   -0.011138056333333332,\n",
       "   0.02955969633333334,\n",
       "   -0.01291309325,\n",
       "   -0.0074644030499999995,\n",
       "   0.0027891627499999996,\n",
       "   -0.010217420683333332,\n",
       "   0.005221645616666666,\n",
       "   0.025868788,\n",
       "   0.0026740549166666665,\n",
       "   -0.025133663,\n",
       "   0.01693109,\n",
       "   0.023061275333333336,\n",
       "   -0.04082895883333334,\n",
       "   -0.004277488966666667,\n",
       "   -0.013497548793333333,\n",
       "   -0.005761012116666666,\n",
       "   -0.020511672666666664,\n",
       "   0.011504710666666668,\n",
       "   -0.027991893833333333,\n",
       "   0.0029334114833333325,\n",
       "   -0.005202776313333333,\n",
       "   0.006933995333333334,\n",
       "   -0.019331829000000002,\n",
       "   0.014720588178333332,\n",
       "   -0.006408168933333334,\n",
       "   0.0155764965,\n",
       "   0.009383387900000002,\n",
       "   -0.018905621333333334,\n",
       "   0.015623111583333333,\n",
       "   0.0040637447333333335,\n",
       "   -0.03516154866666667,\n",
       "   -0.0087740411,\n",
       "   0.006171906566666666,\n",
       "   -0.00194659075,\n",
       "   -0.007018088466666667,\n",
       "   0.009705625666666667,\n",
       "   -0.005945451166666667,\n",
       "   0.02376202245,\n",
       "   -0.014604496566666667,\n",
       "   -0.015761539666666668,\n",
       "   -0.014484001733333333,\n",
       "   0.022160291,\n",
       "   -0.021957398000000003,\n",
       "   -0.030855682,\n",
       "   -0.00201975315,\n",
       "   0.013558193528333333,\n",
       "   0.010753242966666667,\n",
       "   0.006724093,\n",
       "   -0.009618602116666668,\n",
       "   -0.0076851859,\n",
       "   -0.010047567200000001,\n",
       "   -0.0010805867050000001,\n",
       "   0.0012582259666666666,\n",
       "   -0.013569410483333334,\n",
       "   -0.003891164166666666,\n",
       "   -0.01104660345,\n",
       "   0.012748846313333333,\n",
       "   0.012039914816666665,\n",
       "   -0.022252679333333334,\n",
       "   -0.006180535583333334,\n",
       "   -0.017567067666666665,\n",
       "   -0.0227465245,\n",
       "   -0.014728922166666665,\n",
       "   -0.018302048166666668,\n",
       "   0.01629483225,\n",
       "   -0.017541258666666667,\n",
       "   -0.0025485528,\n",
       "   0.008115786816666666,\n",
       "   -0.028764288833333335,\n",
       "   0.0010803581000000003,\n",
       "   0.014420615166666666,\n",
       "   -0.014842476166666665,\n",
       "   -0.02007571983333333,\n",
       "   0.021866909000000004,\n",
       "   -0.019698377,\n",
       "   0.015886668333333336,\n",
       "   0.019251272833333333,\n",
       "   -0.0021642474666666665,\n",
       "   0.00011191495666666689,\n",
       "   -0.019844970066666665,\n",
       "   0.005115206166666666,\n",
       "   0.029732965666666666,\n",
       "   0.0012921668583333334,\n",
       "   -0.0036446017333333336,\n",
       "   -0.0019424225666666666,\n",
       "   0.006047963766666667,\n",
       "   0.004350849206666667,\n",
       "   0.0187044975,\n",
       "   0.002369957816666667,\n",
       "   0.013107165833333335,\n",
       "   0.016687034666666666,\n",
       "   -0.009214586499999998,\n",
       "   0.0286145205,\n",
       "   -0.02958576766666667,\n",
       "   0.0034016600333333334,\n",
       "   -0.020613589666666668,\n",
       "   -0.0070931775499999995,\n",
       "   0.0084196303,\n",
       "   -0.0005658584333333334,\n",
       "   0.0102650236,\n",
       "   0.018797362083333335,\n",
       "   0.0047523905833333335,\n",
       "   0.012108878833333331,\n",
       "   0.029403364166666668,\n",
       "   -0.018305545933333334,\n",
       "   0.015386975933333333,\n",
       "   0.007817855333333333,\n",
       "   0.016902877916666666,\n",
       "   -0.014736419166666667,\n",
       "   -0.00559986715,\n",
       "   -0.005202989066666667,\n",
       "   0.004912973716666667,\n",
       "   0.003013861978333334,\n",
       "   -0.020244156,\n",
       "   -0.029827521333333332,\n",
       "   0.005838361683333334,\n",
       "   -0.016491004516666668,\n",
       "   0.007220214011666667,\n",
       "   0.012558351508333334,\n",
       "   0.029467758666666666,\n",
       "   0.006437524883333334,\n",
       "   -0.02101138745,\n",
       "   -0.0024544956000000003,\n",
       "   -0.019308624333333333,\n",
       "   -0.012502892,\n",
       "   0.007400518011666667,\n",
       "   -0.005259746333333333,\n",
       "   -0.00231761715,\n",
       "   0.0205400855,\n",
       "   0.023547423583333334,\n",
       "   0.012624475916666664,\n",
       "   0.007723541895000001,\n",
       "   -0.018326694333333334,\n",
       "   -0.013270254300000001,\n",
       "   0.025626887,\n",
       "   0.015273479750000001,\n",
       "   0.00749433925,\n",
       "   0.002175462883333333,\n",
       "   -0.0004167370499999999,\n",
       "   0.021824828333333334,\n",
       "   -0.006312090813333333,\n",
       "   0.018809016916666668,\n",
       "   -0.005854238883333334,\n",
       "   0.006481491366666667,\n",
       "   0.007847050950000002,\n",
       "   0.0068950819216666666,\n",
       "   0.004226083566666667,\n",
       "   0.0095975635,\n",
       "   0.005832752383333334,\n",
       "   0.043030830166666666,\n",
       "   0.011435943683333333,\n",
       "   0.0009592302600000001,\n",
       "   0.014782143633333335,\n",
       "   0.00110148075,\n",
       "   -0.011031772691666665,\n",
       "   -0.008211349683333332,\n",
       "   0.022032589833333335,\n",
       "   0.006877143133333333,\n",
       "   -0.02981357383333334,\n",
       "   -0.020658435,\n",
       "   0.0076602795966666665,\n",
       "   0.0159603757,\n",
       "   0.026859693166666667,\n",
       "   0.00018169236666666644,\n",
       "   -0.005728753183333334,\n",
       "   0.007778345933333332,\n",
       "   0.00841431855,\n",
       "   0.011094387016666667,\n",
       "   -0.013207987650000001,\n",
       "   -0.0005466273333333338,\n",
       "   0.0008697536199999996,\n",
       "   -0.018269042833333332,\n",
       "   -0.007849926966666665,\n",
       "   -0.015766230333333336,\n",
       "   -0.019180867166666667,\n",
       "   0.006822870490000001,\n",
       "   -0.00926311008,\n",
       "   0.010465433233333333,\n",
       "   0.0018455625474999998,\n",
       "   -0.010896390725000002,\n",
       "   0.008502678416666668,\n",
       "   -0.0025263026166666668,\n",
       "   -0.016060083183333334,\n",
       "   -0.007490647494999999,\n",
       "   -0.04693644216666667,\n",
       "   0.0061833887833333346,\n",
       "   0.0171530745,\n",
       "   0.0007161313000000002,\n",
       "   -0.015913542833333332,\n",
       "   0.0036340863433333335,\n",
       "   0.006356579756666668,\n",
       "   -0.0083976683,\n",
       "   0.015464807166666669,\n",
       "   0.007983134738333333,\n",
       "   -0.004780249166666667,\n",
       "   0.016093655833333335,\n",
       "   -0.013103028646666667,\n",
       "   -0.004107466783333333,\n",
       "   -0.011607747966666665,\n",
       "   0.007370028973333334,\n",
       "   -0.01617260483333333,\n",
       "   0.010643352783333332,\n",
       "   0.0002271360998333334,\n",
       "   -0.003475219069999999,\n",
       "   -0.018655510500000003,\n",
       "   -0.0179614405,\n",
       "   -0.004197797483333334,\n",
       "   0.040282719833333334,\n",
       "   0.012996764016666667,\n",
       "   -0.019598840666666666,\n",
       "   -0.028487582166666667,\n",
       "   0.006062261716666667,\n",
       "   -0.010174218466666666,\n",
       "   -0.009052655000000001,\n",
       "   -0.010451322716666668,\n",
       "   -0.0134914031,\n",
       "   0.0016548234833333331,\n",
       "   -0.0041280374666666664,\n",
       "   -0.0049517515,\n",
       "   -0.028409422333333333,\n",
       "   0.009473780533333333,\n",
       "   0.04527020666666667,\n",
       "   -0.006266911583333333,\n",
       "   -0.008801011916666665,\n",
       "   -0.01794315175,\n",
       "   -0.0063076488833333335,\n",
       "   0.02676544933333333,\n",
       "   0.04825648216666666,\n",
       "   0.015225701083333333,\n",
       "   -0.011635063383333333,\n",
       "   0.0074551618666666665,\n",
       "   -0.00948956325,\n",
       "   0.005126878418333334,\n",
       "   -0.014702005999999998,\n",
       "   -0.026548353166666667,\n",
       "   0.015848107833333333,\n",
       "   0.016395956716666667,\n",
       "   0.010657934633333333,\n",
       "   -0.00643187015,\n",
       "   0.03147190083333334,\n",
       "   -0.009649805366666667,\n",
       "   0.011334414233333335,\n",
       "   0.03512180583333333,\n",
       "   -0.018473606,\n",
       "   -0.01401009155,\n",
       "   -0.016320062943333335,\n",
       "   -0.021475294333333336,\n",
       "   -0.005319676,\n",
       "   -0.007293005733333332,\n",
       "   0.007503767666666667,\n",
       "   0.024546248666666663,\n",
       "   0.012158868066666667,\n",
       "   0.007718338483333333,\n",
       "   0.012138195566666668,\n",
       "   -0.004168078266666667,\n",
       "   0.005568129333333334,\n",
       "   -0.014165681050000002,\n",
       "   -0.023699468333333334,\n",
       "   0.002741141141,\n",
       "   -0.004288352999999999,\n",
       "   0.014250407166666668,\n",
       "   0.0047500707,\n",
       "   5.593837036666683e-05,\n",
       "   0.012244417616666664,\n",
       "   -0.0023299485166666674,\n",
       "   0.010209493716666667,\n",
       "   -0.011373712078333334,\n",
       "   0.0038348604499999998,\n",
       "   0.03110642033333333,\n",
       "   0.010500776766666666,\n",
       "   -0.010460743,\n",
       "   0.0012917803333333331,\n",
       "   -0.010098542666666667,\n",
       "   -0.024090052216666668,\n",
       "   0.014815270283333334,\n",
       "   0.003458111033333333,\n",
       "   -0.013608471116666666,\n",
       "   0.03590854433333333,\n",
       "   0.006018509516666667,\n",
       "   -0.009310918999999999,\n",
       "   -0.01892925566666667,\n",
       "   0.013172543221666666,\n",
       "   0.017750385916666667,\n",
       "   0.0016946595166666665,\n",
       "   -0.006813795095000001,\n",
       "   0.023378918833333335,\n",
       "   -0.016157592783333334,\n",
       "   -0.018946064666666665,\n",
       "   -0.029049616833333333,\n",
       "   0.0017215870166666664,\n",
       "   -0.0003896159266666667,\n",
       "   -0.0014938110333333331,\n",
       "   -0.0418015595,\n",
       "   -0.029469029000000004,\n",
       "   -0.008711560083333333,\n",
       "   -0.01427276075,\n",
       "   -0.00017785634333333344,\n",
       "   -0.016446954249999996,\n",
       "   0.005169609055000001,\n",
       "   -0.023751565500000002,\n",
       "   -0.013246210066666667,\n",
       "   0.012812640266666668,\n",
       "   0.0023023479778333336,\n",
       "   -0.00626096855,\n",
       "   -0.009695810266666667,\n",
       "   -0.006509269510333333,\n",
       "   0.024350524833333335,\n",
       "   -0.008664645966666667,\n",
       "   0.00717204425,\n",
       "   -0.011766053866666666,\n",
       "   -0.018167164916666666,\n",
       "   -0.018944934383333334,\n",
       "   0.006232509033333333,\n",
       "   -0.005059774088333334,\n",
       "   0.0017235704099999999,\n",
       "   -0.00315438255,\n",
       "   0.0004752937500000001,\n",
       "   0.02604429966666667,\n",
       "   0.012532998583333335,\n",
       "   0.007338996899999999,\n",
       "   -0.017932386833333334,\n",
       "   0.0034321379100000007,\n",
       "   -0.007133728033333332,\n",
       "   0.020190170833333333,\n",
       "   -0.0037308898666666666,\n",
       "   -0.007613171883333333,\n",
       "   0.0064253412166666675,\n",
       "   0.010073017433333333,\n",
       "   -0.0351647805,\n",
       "   -0.010086282116666666,\n",
       "   -0.015887193916666667,\n",
       "   0.014570462166666666,\n",
       "   0.007517452066666667,\n",
       "   0.005071792183333334,\n",
       "   0.019464374083333333,\n",
       "   -0.019409247333333334,\n",
       "   -0.0060841312,\n",
       "   0.017080702166666666,\n",
       "   -0.017902169666666665,\n",
       "   0.012608101000000002,\n",
       "   -0.012523852833333333,\n",
       "   0.0085618783,\n",
       "   0.01231371216666667,\n",
       "   -0.02307587033333333,\n",
       "   0.023325676,\n",
       "   -0.004767490631666667,\n",
       "   0.00808801595,\n",
       "   -0.009881674416666666,\n",
       "   -0.005630478233333333,\n",
       "   0.023208838,\n",
       "   0.02142951083333333,\n",
       "   8.879742166666698e-05,\n",
       "   0.007467455124666666,\n",
       "   0.014323497999999999,\n",
       "   -0.011313400500000001,\n",
       "   -0.022567842833333334,\n",
       "   0.016859482166666665,\n",
       "   -0.0076367521333333315,\n",
       "   0.02442989,\n",
       "   0.013355275550000002,\n",
       "   -0.0031772607333333345,\n",
       "   -0.0279044265,\n",
       "   -0.014082331466666665,\n",
       "   -0.003171847183333333,\n",
       "   0.003556598916666667,\n",
       "   0.004064424133333334,\n",
       "   -0.018789462500000003,\n",
       "   -0.005133803741,\n",
       "   0.0022639555333333334,\n",
       "   -0.011772036223333332,\n",
       "   -0.0006900561000000002,\n",
       "   0.019241786083333334,\n",
       "   -0.02397086866666667,\n",
       "   -0.0035136448666666666,\n",
       "   0.01809002766666667,\n",
       "   -0.028647667666666668,\n",
       "   0.011752023,\n",
       "   -0.00474423465,\n",
       "   0.006177648663333334,\n",
       "   -0.029557065999999996,\n",
       "   -0.007260589158333333,\n",
       "   -0.0037292280666666663,\n",
       "   -0.005490933104333334,\n",
       "   -0.021294089000000002,\n",
       "   -0.0270447875,\n",
       "   0.020859911,\n",
       "   0.0027686475,\n",
       "   0.018563224583333333,\n",
       "   0.006695403449999999,\n",
       "   0.017426101333333333,\n",
       "   -0.009898131166666668,\n",
       "   0.005683485566666666,\n",
       "   0.008551231099999999,\n",
       "   0.0011987180802833335,\n",
       "   -0.0020043833666666664,\n",
       "   -0.007776430341666668,\n",
       "   0.033209412,\n",
       "   0.01790904466666667,\n",
       "   -0.008106330883333334,\n",
       "   0.008462196916666666,\n",
       "   0.0010474347333333334,\n",
       "   0.020948484916666666,\n",
       "   0.01774848141666667,\n",
       "   -0.003754747516666667,\n",
       "   -0.012087776549999998,\n",
       "   -0.011937690383333331,\n",
       "   -0.025635222,\n",
       "   -0.016332295,\n",
       "   -0.003026380066666666,\n",
       "   0.010777867749999998,\n",
       "   0.009275713833333333,\n",
       "   -0.0094967677,\n",
       "   0.0006170725149999999,\n",
       "   0.026680619000000003,\n",
       "   0.0067583979,\n",
       "   -0.013794925083333331,\n",
       "   -0.012746345883333335,\n",
       "   0.01057094765,\n",
       "   -0.007176097383333334,\n",
       "   0.015631748666666664,\n",
       "   0.003950387176666666,\n",
       "   0.011426076282666666,\n",
       "   -0.003625312583333334,\n",
       "   -0.014507076361666664,\n",
       "   0.009853154599999999,\n",
       "   -0.009239878783333334,\n",
       "   0.0036815735000000002,\n",
       "   -0.005684265933333334,\n",
       "   0.0012886323833333335,\n",
       "   -0.005283101568833333,\n",
       "   0.0009637479499999994,\n",
       "   0.0011507779833333336,\n",
       "   0.015290205166666666,\n",
       "   -0.016593478833333335,\n",
       "   0.0019168881773333334,\n",
       "   0.004133251300000001,\n",
       "   -0.018843887333333333,\n",
       "   0.0043501815000000004,\n",
       "   -0.02338234733333333,\n",
       "   -0.021612653083333332,\n",
       "   -0.041052570833333336,\n",
       "   0.0019692290343333333,\n",
       "   0.005117763016666666,\n",
       "   0.005649640933333334,\n",
       "   0.013897930083333334,\n",
       "   -0.024985219599999997,\n",
       "   -0.015932895000000002,\n",
       "   -0.004988687416666666,\n",
       "   -0.018876390333333336,\n",
       "   0.025885693166666664,\n",
       "   0.004155939049999999,\n",
       "   0.018300043666666668,\n",
       "   0.008244987971666668,\n",
       "   -0.006896655066666666,\n",
       "   -0.025627117166666668,\n",
       "   -0.001518772501666667,\n",
       "   -0.0031646005166666665,\n",
       "   -0.0052039106166666665,\n",
       "   0.0015303014000000003,\n",
       "   0.024181392166666666,\n",
       "   -2.6071166666666774e-05,\n",
       "   -0.00235726138,\n",
       "   0.0015949266,\n",
       "   0.015270953666666668,\n",
       "   -0.013998680833333334,\n",
       "   -0.01708921505,\n",
       "   0.009102838683333335,\n",
       "   -0.012646078333333333,\n",
       "   0.019157617000000002,\n",
       "   0.019923503500000002,\n",
       "   -0.007950070383333334,\n",
       "   -0.011216190333333334,\n",
       "   0.033448960666666666,\n",
       "   -0.01320300175,\n",
       "   -0.0250600315,\n",
       "   -0.017369647000000002,\n",
       "   -0.009197053933333333,\n",
       "   -0.004529019116666667,\n",
       "   0.012047421518000001,\n",
       "   -0.009892881666666667,\n",
       "   0.014528433802333332,\n",
       "   -0.0032288331666666665,\n",
       "   -0.003782635716666667,\n",
       "   -0.012903055116666666,\n",
       "   -0.013227343433333335,\n",
       "   -0.020093565833333334,\n",
       "   0.011082602041666667,\n",
       "   0.0010799179999999998,\n",
       "   0.02403262216666667,\n",
       "   0.0032982794333333335,\n",
       "   0.007165897666666666,\n",
       "   8.700081666666686e-05,\n",
       "   -0.0015445762666666668,\n",
       "   -0.03175878183333333,\n",
       "   0.010279493166666666,\n",
       "   -0.0007236056000000003,\n",
       "   0.029539710999999996,\n",
       "   -0.005694258333333333,\n",
       "   -0.013651127999999998,\n",
       "   -0.015518139483333337,\n",
       "   0.003490163483333333,\n",
       "   0.014266986916666667,\n",
       "   -0.010871976583333333,\n",
       "   -0.012327245666666667,\n",
       "   -0.0030117236166666665,\n",
       "   -0.0024722693166666673,\n",
       "   -0.013174976833333333,\n",
       "   0.022304169333333332,\n",
       "   0.005019634283333334,\n",
       "   -0.009902429150000001,\n",
       "   -0.0052490452,\n",
       "   -0.008737292383333333,\n",
       "   -0.004633519966666667,\n",
       "   0.012304816833333334,\n",
       "   -0.0112411534,\n",
       "   0.007540612833333334,\n",
       "   0.0043870783666666675,\n",
       "   -0.012111547083333334,\n",
       "   -0.006123666966666667,\n",
       "   -0.0051041918,\n",
       "   0.020883473316666665,\n",
       "   -0.0083259456,\n",
       "   -0.012743674966666667,\n",
       "   -0.002837852283333333,\n",
       "   0.025937152166666668,\n",
       "   -0.01329337725,\n",
       "   0.011921434066666667,\n",
       "   -0.023963899999999996,\n",
       "   0.0048952498833333335,\n",
       "   -0.004594077416666666,\n",
       "   0.042045426500000004,\n",
       "   -0.005088513741666666,\n",
       "   -0.020352510666666667,\n",
       "   0.025113132,\n",
       "   -0.0027127412883333336,\n",
       "   0.0039195842606666665,\n",
       "   -0.007197532173333334,\n",
       "   0.005734141505000001,\n",
       "   0.012519515266666668,\n",
       "   0.024076234000000002,\n",
       "   0.005006790216666667,\n",
       "   0.0168891655,\n",
       "   -0.0012563620333333333,\n",
       "   0.012777520763333333,\n",
       "   -0.0012056160833333331,\n",
       "   -0.018643992733333333,\n",
       "   0.01596780156666667,\n",
       "   -0.014392873476666667,\n",
       "   -0.0028796689833333333,\n",
       "   0.015860820933333332,\n",
       "   -0.007643757538333333,\n",
       "   0.0007401208949999995,\n",
       "   0.018548242133333333,\n",
       "   0.0177082065,\n",
       "   0.006954545933333333,\n",
       "   -0.048870061,\n",
       "   -0.013045307616666668,\n",
       "   -0.010680991299999998,\n",
       "   0.015361968699999999,\n",
       "   -0.013187576,\n",
       "   -0.0126675869,\n",
       "   0.008753727033333332,\n",
       "   0.0028108964833333337,\n",
       "   0.0002360493233333334,\n",
       "   0.024346038,\n",
       "   -0.0009216239833333336,\n",
       "   -0.004665037883333333,\n",
       "   0.005026105083333334,\n",
       "   -0.0013361391799999996,\n",
       "   -0.0009536448800000003,\n",
       "   -0.013768400283333334,\n",
       "   -0.006649795566666668,\n",
       "   0.003369180003333333,\n",
       "   0.004538762983333333,\n",
       "   -0.017947496333333333,\n",
       "   -0.0108883807,\n",
       "   0.004274023983333332,\n",
       "   -0.003511074783333333,\n",
       "   0.017282386,\n",
       "   0.013358522783333333,\n",
       "   -0.020467341666666663,\n",
       "   -0.013527986233333332,\n",
       "   0.006944596383333334,\n",
       "   -0.033124482499999996,\n",
       "   -0.011036756783333332,\n",
       "   -0.02302876466666667,\n",
       "   0.0153231499,\n",
       "   0.008377519133333332,\n",
       "   0.004449123928333333,\n",
       "   -0.007861628924166668,\n",
       "   0.0002770335833333333,\n",
       "   -0.005267774149999999,\n",
       "   -0.007000615016666667,\n",
       "   -0.017399661666666667,\n",
       "   -3.0218426666666558e-05,\n",
       "   0.019056102000000002,\n",
       "   0.0068292041,\n",
       "   -0.014969388916666666,\n",
       "   0.007729320866666667,\n",
       "   -0.030187304333333335,\n",
       "   -0.008013559316666667,\n",
       "   0.009768227838333334,\n",
       "   0.010334404141666668,\n",
       "   0.0012843859166666667,\n",
       "   0.012890268166666668,\n",
       "   0.024716484666666667,\n",
       "   -0.009190216450000002,\n",
       "   -0.00673127375,\n",
       "   -0.004147162916666666,\n",
       "   -0.0010690147666666666,\n",
       "   0.01490014775,\n",
       "   -0.0030902519833333336,\n",
       "   -0.0183984105,\n",
       "   0.0015141825000000002,\n",
       "   0.007324154950000001,\n",
       "   0.006988670233333333,\n",
       "   -0.008420250316666667,\n",
       "   -0.0104319327,\n",
       "   -0.0004175000166666664,\n",
       "   0.00758031305,\n",
       "   -0.0006408052500000001,\n",
       "   -0.024342449000000002,\n",
       "   -0.005904816533333333,\n",
       "   -0.0100702155,\n",
       "   0.0016123936000000001,\n",
       "   0.02424781083333333,\n",
       "   0.003460466298333334,\n",
       "   0.005531427161666667,\n",
       "   0.021499534333333334,\n",
       "   0.0018119848599999998,\n",
       "   -0.004937437449999999,\n",
       "   -0.011769395083333333,\n",
       "   -0.0017574126,\n",
       "   -0.028696897666666665,\n",
       "   -0.007200943550000001,\n",
       "   0.002295132371666667,\n",
       "   -0.038468814166666664,\n",
       "   0.003977453266666667,\n",
       "   0.009537303533333335,\n",
       "   0.0036934725,\n",
       "   -0.02077559333333333,\n",
       "   0.016193337449999998,\n",
       "   0.004712744233333333,\n",
       "   0.0036979467666666665,\n",
       "   -0.0017643842833333333,\n",
       "   0.016077663333333336,\n",
       "   0.029806137833333333,\n",
       "   -0.015951503833333335,\n",
       "   0.017157833666666667,\n",
       "   -0.00539303885,\n",
       "   -0.028467125833333332,\n",
       "   -0.018717560666666664,\n",
       "   0.00029726705,\n",
       "   -0.010057846383333334,\n",
       "   -0.039267870833333336,\n",
       "   -0.003005399116666667,\n",
       "   0.006644663606666666,\n",
       "   -0.0007508401499999998,\n",
       "   -0.013344239483333334,\n",
       "   -0.027560817166666668,\n",
       "   0.006046261033333333,\n",
       "   -0.007734884116666667,\n",
       "   0.017852986666666664,\n",
       "   0.17972087333333334,\n",
       "   -0.013050760583333333,\n",
       "   -0.01981989266666667,\n",
       "   0.00983989525,\n",
       "   -0.010298218716666667,\n",
       "   0.00047346464000000015,\n",
       "   0.01963722725,\n",
       "   0.009277306583333334,\n",
       "   -0.008205952466666666,\n",
       "   0.022470132,\n",
       "   0.00874256709,\n",
       "   -0.0077655853333333325,\n",
       "   -0.013763483633333332,\n",
       "   -0.00210510703895,\n",
       "   0.007746926283333333,\n",
       "   -0.0015494296999999998,\n",
       "   -0.025810863000000003,\n",
       "   -0.018146080499999998,\n",
       "   -0.023817321949999998,\n",
       "   -0.026561421166666665,\n",
       "   -0.013646162250000001,\n",
       "   -0.010532328733333334,\n",
       "   0.0218732635,\n",
       "   -0.005082492465,\n",
       "   0.03946901433333333,\n",
       "   -0.00420702165,\n",
       "   0.0072400668533333335,\n",
       "   -0.00025482296666666667,\n",
       "   0.023960259833333334,\n",
       "   0.0015741902833333336,\n",
       "   -0.017640002333333335,\n",
       "   0.009494698250000001,\n",
       "   0.01408619805,\n",
       "   0.004304202991666667,\n",
       "   -0.026006257333333328,\n",
       "   -0.005635691333333334,\n",
       "   0.0022883126500000002,\n",
       "   0.0048699867666666665,\n",
       "   0.012057267500000001,\n",
       "   0.01253300725,\n",
       "   0.007113868500000001,\n",
       "   0.013331600383333331,\n",
       "   0.003912358899999999,\n",
       "   -0.009659561066666668,\n",
       "   0.00034349591666666654,\n",
       "   0.018416793333333334,\n",
       "   ...]],\n",
       " 'attention_mortality': 0.15261274074226605,\n",
       " 'attention_readmission': 0.21378001345903985}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_cluster['5000']\n",
    "\n",
    "new_d = {}\n",
    "new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccscm_id2clus['0'] = \"5000\"\n",
    "ccsproc_id2clus['0'] = \"5000\"\n",
    "atc3_id2clus['0'] = \"5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_cluster['4598']['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling direct ehr nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9717/9717 [00:00<00:00, 14781.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset...\n",
      "Getting embedding...\n"
     ]
    }
   ],
   "source": [
    "from graphcare import *\n",
    "\n",
    "kg = \"GPT-KG\"\n",
    "dataset = \"mimic3\"\n",
    "task = \"mortality\"\n",
    "\n",
    "# load dataset\n",
    "sample_dataset, G, ent2id, rel2id, ent_emb, rel_emb, \\\n",
    "            map_cluster, map_cluster_inv, map_cluster_rel, map_cluster_rel_inv, \\\n",
    "                ccscm_id2clus, ccsproc_id2clus, atc3_id2clus = load_everything(dataset, task, kg)\n",
    "\n",
    "# label direct ehr node\n",
    "print(\"Labeling direct ehr nodes...\")\n",
    "sample_dataset = label_ehr_nodes(task, sample_dataset, len(map_cluster), ccscm_id2clus, ccsproc_id2clus, atc3_id2clus)\n",
    "print(\"Splitting dataset...\")\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)\n",
    "G_tg = from_networkx(G)\n",
    "\n",
    "# get embedding\n",
    "print(\"Getting embedding...\")\n",
    "rel_emb = get_rel_emb(map_cluster_rel)\n",
    "node_emb = G_tg.x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 4598, 4598, 4598],\n",
       "        [ 275, 1997,    0,  ..., 3388, 3924, 4283]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_tg.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Dataset(G=G_tg, dataset=train_dataset, task=task)\n",
    "val_set = Dataset(G=G_tg, dataset=val_dataset, task=task)\n",
    "test_set = Dataset(G=G_tg, dataset=test_dataset, task=task)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4599, 1536])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "node_embedding = nn.Embedding.from_pretrained(node_emb, freeze=False)\n",
    "relation_embedding = nn.Embedding.from_pretrained(rel_emb, freeze=False)\n",
    "alpha_attn = nn.Linear(node_emb.shape[0], node_emb.shape[0])\n",
    "beta_attn = nn.Linear(node_emb.shape[0], 1)\n",
    "leakyrelu = nn.LeakyReLU(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Embedding(4599, 1536), Embedding(1077, 1536))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embedding, relation_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[90729, 1536], edge_index=[2, 54985], y=[90729], relation=[54985], label=[64], visit_padded_node=[1792, 4599], ehr_nodes=[294336], batch=[90729], ptr=[65])\n",
      "torch.Size([54985])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Softmax\n",
    "\n",
    "for batch in train_loader:\n",
    "    # print(batch.ehr_nodes)\n",
    "    # print(node_embedding(torch.tensor(batch.ehr_nodes[0])))\n",
    "    node_ids = batch.y\n",
    "    visit_node = batch.visit_padded_node.reshape(int(train_loader.batch_size), int(len(batch.visit_padded_node)/train_loader.batch_size), batch.visit_padded_node.shape[1]).double()\n",
    "    x = node_embedding(node_ids)\n",
    "    alpha = torch.softmax((leakyrelu(alpha_attn(visit_node.float()))), dim=1)\n",
    "    beta = torch.softmax((leakyrelu(beta_attn(visit_node.float()))), dim=0)\n",
    "    j = torch.arange(visit_node.shape[1], device=x.device).float()\n",
    "    lambda_j = torch.exp(0.03 * (visit_node.shape[1] - j)).unsqueeze(0).reshape(1, visit_node.shape[1], 1)\n",
    "    attn = alpha*beta*lambda_j\n",
    "    attn = torch.sum(attn, dim=1)\n",
    "    ehr_nodes = batch.ehr_nodes.reshape(int(train_loader.batch_size), int(len(batch.ehr_nodes)/train_loader.batch_size)).float()\n",
    "    xj_batch = batch.batch[batch.edge_index[0]]\n",
    "    xj_node_ids = batch.y[batch.edge_index[0]]\n",
    "    print(batch)\n",
    "    print(attn[xj_batch, xj_node_ids].shape)\n",
    "    # print(batch.batch.shape)\n",
    "    # print(attn.shape)\n",
    "    # print(batch.batch[batch.edge_index[0]].shape)\n",
    "    # print(batch.y[batch.edge_index[0]].shape)\n",
    "    # print(ehr_nodes.shape)\n",
    "    # print(ehr_nodes[1].view(1, -1) @ node_embedding.weight / torch.sum(ehr_nodes[1]))\n",
    "    # attn = attn[batch.edge_index[0]]\n",
    "    # print(attn.shape)\n",
    "    \n",
    "    # print(x.shape)\n",
    "    # print(visit_node.shape)\n",
    "    # visit_emb = x.view(visit_node.shape).sum(dim=2) / visit_node.sum(dim=2).clamp(min=1).view(visit_node.shape[:2] + (1,))\n",
    "    # print(visit_emb.shape)\n",
    "    # print(node_emb[:4599].shape)\n",
    "    # print(visit_node.shape)\n",
    "    # print(x.shape)\n",
    "    # print((visit_node @ node_emb[:4599]).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from pyhealth.models import RETAINLayer\n",
    "from torch_geometric.nn.inits import reset\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.typing import (\n",
    "    Adj,\n",
    "    OptPairTensor,\n",
    "    OptTensor,\n",
    "    Size,\n",
    "    SparseTensor,\n",
    ")\n",
    "from torch_geometric.utils import spmm\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import softmax\n",
    "from torch.nn import LeakyReLU\n",
    "\n",
    "\n",
    "class BiAttentionGNNConv(MessagePassing):\n",
    "    def __init__(self, nn: torch.nn.Module, eps: float = 0.,\n",
    "                 train_eps: bool = False, edge_dim: Optional[int] = None,\n",
    "                 **kwargs):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "        self.nn = nn\n",
    "        self.initial_eps = eps\n",
    "        self.W_R = torch.nn.Linear(edge_dim, edge_dim)\n",
    "\n",
    "        if train_eps:\n",
    "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.Tensor([eps]))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.nn.reset_parameters()\n",
    "        self.eps.data.fill_(self.initial_eps)\n",
    "        if self.W_R is not None:\n",
    "            self.W_R.reset_parameters()\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None, attn: Tensor = None) -> Tensor:\n",
    "\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size, attn=attn)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if x_r is not None:\n",
    "            out = out + (1 + self.eps) * x_r\n",
    "\n",
    "        return self.nn(out)\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor, attn: Tensor) -> Tensor:\n",
    "\n",
    "        h_R = self.W_R(edge_attr)\n",
    "        out = (x_j * attn + h_R).relu()\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(nn={self.nn})'\n",
    "\n",
    "\n",
    "def masked_softmax(src: Tensor, mask: Tensor, dim: int = -1) -> Tensor:\n",
    "    out = src.masked_fill(~mask, float('-inf'))\n",
    "    out = torch.softmax(out, dim=dim)\n",
    "    out = out.masked_fill(~mask, 0)\n",
    "    return out\n",
    "\n",
    "class GraphCare(nn.Module):\n",
    "    def __init__(self, num_nodes, num_rels, max_visit, embedding_dim, hidden_dim, out_channels, layers=3, dropout=0.5, decay_rate=0.03, node_emb=None, rel_emb=None):\n",
    "        super(GraphCare, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "        j = torch.arange(max_visit).float()\n",
    "        self.lambda_j = torch.exp(self.decay_rate * (max_visit - j)).unsqueeze(0).reshape(1, max_visit, 1).float()\n",
    "\n",
    "        if node_emb is None:\n",
    "            self.node_emb = nn.Embedding(num_nodes, embedding_dim)\n",
    "        else:\n",
    "            self.node_emb = nn.Embedding.from_pretrained(node_emb, freeze=False)\n",
    "\n",
    "        if rel_emb is None:\n",
    "            self.rel_emb = nn.Embedding(num_rels, embedding_dim)\n",
    "        else:\n",
    "            self.rel_emb = nn.Embedding.from_pretrained(rel_emb, freeze=False)\n",
    "\n",
    "        self.lin = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.alpha_attn = nn.ModuleDict()\n",
    "        self.beta_attn = nn.ModuleDict()\n",
    "        self.conv = nn.ModuleDict()\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "        for layer in range(1, layers+1):\n",
    "            self.alpha_attn[str(layer)] = nn.Linear(num_nodes, num_nodes)\n",
    "            self.beta_attn[str(layer)] = nn.Linear(num_nodes, 1)\n",
    "            self.conv[str(layer)] = BiAttentionGNNConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=hidden_dim)\n",
    "\n",
    "        self.MLP = nn.Linear(hidden_dim * 2, out_channels)\n",
    "        \n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.lambda_j = self.lambda_j.float().to(device)\n",
    "\n",
    "\n",
    "    def forward(self, node_ids, rel_ids, edge_index, batch, visit_node, ehr_nodes):\n",
    "        x = self.node_emb(node_ids).float()\n",
    "        edge_attr = self.rel_emb(rel_ids).float()\n",
    "\n",
    "        x = self.bn1(self.lin(x))\n",
    "        edge_attr = self.bn1(self.lin(edge_attr))\n",
    "\n",
    "\n",
    "        for layer in range(1, self.layers+1):\n",
    "            alpha = masked_softmax((self.leakyrelu(self.alpha_attn[str(layer)](visit_node.float()))), mask=visit_node>1, dim=1)\n",
    "            beta = masked_softmax((self.leakyrelu(self.beta_attn[str(layer)](visit_node.float()))), mask=visit_node>1, dim=0) * self.lambda_j\n",
    "\n",
    "            attn = alpha * beta\n",
    "            attn = torch.sum(attn, dim=1)\n",
    "            xj_node_ids = node_ids[edge_index[0]]\n",
    "            xj_batch = batch[edge_index[0]]\n",
    "            attn = attn[xj_batch, xj_node_ids].reshape(-1, 1)\n",
    "\n",
    "            x = F.relu(self.conv[str(layer)](x, edge_index, edge_attr, attn=attn))\n",
    "            x = F.dropout(x, p=0.3, training=self.training)\n",
    "\n",
    "        # patient graph embedding through global mean pooling\n",
    "        x_graph = global_mean_pool(x, batch)\n",
    "        x_graph = F.dropout(x_graph, p=self.dropout, training=self.training)\n",
    "\n",
    "        # patient node embedding through local (direct EHR) mean pooling\n",
    "        x_node = torch.stack([ehr_nodes[i].view(1, -1) @ self.node_emb.weight / torch.sum(ehr_nodes[i]) for i in range(batch.max().item() + 1)])\n",
    "        x_node = self.lin(x_node).squeeze(1)\n",
    "        x_node = F.dropout(x_node, p=self.dropout, training=self.training)\n",
    "\n",
    "        # concatenate patient graph embedding and patient node embedding\n",
    "        x_concat = torch.cat((x_graph, x_node), dim=1)\n",
    "        x_concat = F.dropout(x_concat, p=self.dropout, training=self.training)\n",
    "\n",
    "        # MLP for prediction\n",
    "        logits = self.MLP(x_concat)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, jaccard_score\n",
    "    \n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    tot_loss = 0\n",
    "    pbar= tqdm(enumerate(train_loader))\n",
    "    for i, data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        node_ids = data.y\n",
    "        rel_ids = data.relation\n",
    "\n",
    "        out = model(\n",
    "                node_ids = node_ids, \n",
    "                rel_ids = rel_ids,\n",
    "                edge_index = data.edge_index,\n",
    "                batch = data.batch,\n",
    "                visit_node = data.visit_padded_node.reshape(int(train_loader.batch_size), int(len(data.visit_padded_node)/train_loader.batch_size), data.visit_padded_node.shape[1]).float(), \n",
    "                ehr_nodes = data.ehr_nodes.reshape(int(train_loader.batch_size), int(len(data.ehr_nodes)/train_loader.batch_size)).float()\n",
    "                \n",
    "            )\n",
    "        try:\n",
    "            label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "        except:\n",
    "            continue\n",
    "        # print(out.shape, label.shape)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.float())\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return tot_loss\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():    \n",
    "            \n",
    "            node_ids = data.y\n",
    "            rel_ids = data.relation\n",
    "\n",
    "            logits = model(\n",
    "                    node_ids = node_ids, \n",
    "                    rel_ids = rel_ids,\n",
    "                    edge_index = data.edge_index,\n",
    "                    batch = data.batch,\n",
    "                    visit_node = data.visit_padded_node.reshape(int(loader.batch_size), int(len(data.visit_padded_node)/loader.batch_size), data.visit_padded_node.shape[1]).float(), \n",
    "                    ehr_nodes = data.ehr_nodes.reshape(int(loader.batch_size), int(len(data.ehr_nodes)/loader.batch_size)).float()               \n",
    "                )\n",
    "\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "            try:\n",
    "                y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "            except:\n",
    "                continue\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "\n",
    "    return y_true_all, y_prob_all\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs):\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer)\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader)\n",
    "\n",
    "        y_pred_all = (y_prob_all >= 0.5).astype(int)\n",
    "        \n",
    "        val_pr_auc = average_precision_score(y_true_all, y_prob_all)\n",
    "        val_roc_auc = roc_auc_score(y_true_all, y_prob_all)\n",
    "        val_jaccard = jaccard_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_acc = accuracy_score(y_true_all, y_pred_all)\n",
    "        val_f1 = f1_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_precision = precision_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_recall = recall_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "\n",
    "        if val_acc >= best_acc and val_f1 >= best_f1:\n",
    "            # torch.save(model.state_dict(), '../../../data/pj20/exp_data/saved_weights_gin_mimic3_readmission_dynamic.pkl')\n",
    "            # print(\"best model saved\")\n",
    "            best_acc = val_acc\n",
    "            best_f1 = val_f1\n",
    "\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}, Val ROC_AUC: {val_roc_auc:.4f}, Val acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val precision: {val_precision:.4f}, Val recall: {val_recall:.4f}, Val jaccard: {val_jaccard:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0]['visit_padded_node'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GraphCare(\n",
    "    num_nodes=node_emb.shape[0],\n",
    "    num_rels=rel_emb.shape[0],\n",
    "    max_visit=sample_dataset[0]['visit_padded_node'].shape[0],\n",
    "    embedding_dim=node_emb.shape[1],\n",
    "    hidden_dim=512,\n",
    "    out_channels=1,\n",
    "    layers=3,\n",
    "    dropout=0.5,\n",
    "    decay_rate=0.01,\n",
    "    node_emb=node_emb,\n",
    "    rel_emb=rel_emb\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphCare(\n",
       "  (node_emb): Embedding(4599, 1536)\n",
       "  (rel_emb): Embedding(1077, 1536)\n",
       "  (lin): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (alpha_attn): ModuleDict(\n",
       "    (1): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "    (2): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "    (3): Linear(in_features=4599, out_features=4599, bias=True)\n",
       "  )\n",
       "  (beta_attn): ModuleDict(\n",
       "    (1): Linear(in_features=4599, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=4599, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=4599, out_features=1, bias=True)\n",
       "  )\n",
       "  (conv): ModuleDict(\n",
       "    (1): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "    (2): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "    (3): BiAttentionGNNConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  )\n",
       "  (leakyrelu): LeakyReLU(negative_slope=0.1)\n",
       "  (MLP): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
