{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pyhealth.datasets import split_by_patient\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "with open('../../../data/pj20/exp_data/ccscm_ccsproc_atc3/sample_dataset_mimic4_mortality_th015.pkl', 'rb') as f:\n",
    "    sample_dataset = pickle.load(f)\n",
    "\n",
    "with open('../../../data/pj20/exp_data/ccscm_ccsproc_atc3/graph_mimic4_mortality_th015.pkl', 'rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "G_tg = from_networkx(G) \n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n"
     ]
    }
   ],
   "source": [
    "c_v, p_v, d_v = [], [], []\n",
    "\n",
    "for patient in sample_dataset:\n",
    "    c_v.append(len(patient['conditions']))\n",
    "    p_v.append(len(patient['procedures']))\n",
    "    d_v.append(len(patient['drugs']))\n",
    "\n",
    "print(max(c_v), max(p_v), max(d_v))\n",
    "max_visits = max(c_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# import torch\n",
    "\n",
    "# # Define the target dimensionality\n",
    "# target_dim = 512\n",
    "\n",
    "# # Fit PCA to the word embeddings\n",
    "# pca = PCA(n_components=target_dim)\n",
    "# G_tg.x = torch.tensor(pca.fit_transform(G_tg.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'visit_id': '22595853',\n",
       " 'patient_id': '10000032',\n",
       " 'conditions': [['151', '6', '127', '657', '651', '663']],\n",
       " 'procedures': [['88']],\n",
       " 'drugs': [['B01A',\n",
       "   'J07B',\n",
       "   'A12B',\n",
       "   'C03D',\n",
       "   'C03C',\n",
       "   'N02B',\n",
       "   'J05A',\n",
       "   'R03A',\n",
       "   'N07B',\n",
       "   'R03B']],\n",
       " 'label': 0,\n",
       " 'node_set': [3,\n",
       "  5,\n",
       "  2057,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  18,\n",
       "  21,\n",
       "  2073,\n",
       "  2074,\n",
       "  2077,\n",
       "  30,\n",
       "  36,\n",
       "  2085,\n",
       "  4134,\n",
       "  2086,\n",
       "  4136,\n",
       "  4137,\n",
       "  4138,\n",
       "  2090,\n",
       "  2092,\n",
       "  2093,\n",
       "  48,\n",
       "  2097,\n",
       "  4144,\n",
       "  4148,\n",
       "  55,\n",
       "  57,\n",
       "  58,\n",
       "  63,\n",
       "  2112,\n",
       "  66,\n",
       "  69,\n",
       "  4168,\n",
       "  73,\n",
       "  75,\n",
       "  77,\n",
       "  79,\n",
       "  80,\n",
       "  2129,\n",
       "  82,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  2138,\n",
       "  91,\n",
       "  2145,\n",
       "  2147,\n",
       "  100,\n",
       "  99,\n",
       "  2151,\n",
       "  105,\n",
       "  2162,\n",
       "  115,\n",
       "  116,\n",
       "  118,\n",
       "  2167,\n",
       "  121,\n",
       "  2170,\n",
       "  122,\n",
       "  4220,\n",
       "  127,\n",
       "  129,\n",
       "  130,\n",
       "  133,\n",
       "  4230,\n",
       "  135,\n",
       "  2184,\n",
       "  2187,\n",
       "  139,\n",
       "  2189,\n",
       "  144,\n",
       "  2193,\n",
       "  146,\n",
       "  2198,\n",
       "  2201,\n",
       "  154,\n",
       "  153,\n",
       "  2205,\n",
       "  160,\n",
       "  163,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  171,\n",
       "  172,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  181,\n",
       "  185,\n",
       "  188,\n",
       "  189,\n",
       "  2236,\n",
       "  2241,\n",
       "  194,\n",
       "  195,\n",
       "  2249,\n",
       "  2251,\n",
       "  2253,\n",
       "  209,\n",
       "  214,\n",
       "  2262,\n",
       "  216,\n",
       "  218,\n",
       "  2268,\n",
       "  220,\n",
       "  2270,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  4323,\n",
       "  2279,\n",
       "  232,\n",
       "  231,\n",
       "  235,\n",
       "  236,\n",
       "  2285,\n",
       "  238,\n",
       "  2286,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  2292,\n",
       "  247,\n",
       "  249,\n",
       "  2300,\n",
       "  261,\n",
       "  264,\n",
       "  4363,\n",
       "  268,\n",
       "  4364,\n",
       "  2318,\n",
       "  271,\n",
       "  270,\n",
       "  4369,\n",
       "  273,\n",
       "  275,\n",
       "  2321,\n",
       "  277,\n",
       "  4374,\n",
       "  4375,\n",
       "  4380,\n",
       "  288,\n",
       "  297,\n",
       "  299,\n",
       "  300,\n",
       "  4399,\n",
       "  304,\n",
       "  310,\n",
       "  311,\n",
       "  4412,\n",
       "  319,\n",
       "  324,\n",
       "  329,\n",
       "  332,\n",
       "  337,\n",
       "  2388,\n",
       "  341,\n",
       "  342,\n",
       "  345,\n",
       "  4444,\n",
       "  350,\n",
       "  356,\n",
       "  2405,\n",
       "  359,\n",
       "  4456,\n",
       "  2411,\n",
       "  364,\n",
       "  4469,\n",
       "  375,\n",
       "  380,\n",
       "  2434,\n",
       "  387,\n",
       "  388,\n",
       "  2436,\n",
       "  2438,\n",
       "  391,\n",
       "  394,\n",
       "  4492,\n",
       "  397,\n",
       "  399,\n",
       "  2451,\n",
       "  406,\n",
       "  4504,\n",
       "  410,\n",
       "  415,\n",
       "  416,\n",
       "  419,\n",
       "  427,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  435,\n",
       "  2486,\n",
       "  4538,\n",
       "  451,\n",
       "  455,\n",
       "  458,\n",
       "  459,\n",
       "  465,\n",
       "  2513,\n",
       "  4563,\n",
       "  467,\n",
       "  471,\n",
       "  472,\n",
       "  2522,\n",
       "  476,\n",
       "  477,\n",
       "  480,\n",
       "  482,\n",
       "  485,\n",
       "  486,\n",
       "  488,\n",
       "  489,\n",
       "  493,\n",
       "  494,\n",
       "  496,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  516,\n",
       "  517,\n",
       "  2569,\n",
       "  525,\n",
       "  538,\n",
       "  539,\n",
       "  541,\n",
       "  553,\n",
       "  2603,\n",
       "  558,\n",
       "  559,\n",
       "  567,\n",
       "  568,\n",
       "  573,\n",
       "  575,\n",
       "  2628,\n",
       "  583,\n",
       "  585,\n",
       "  2634,\n",
       "  586,\n",
       "  597,\n",
       "  600,\n",
       "  602,\n",
       "  614,\n",
       "  620,\n",
       "  2669,\n",
       "  622,\n",
       "  623,\n",
       "  626,\n",
       "  2676,\n",
       "  628,\n",
       "  636,\n",
       "  641,\n",
       "  2692,\n",
       "  2694,\n",
       "  2695,\n",
       "  2696,\n",
       "  650,\n",
       "  656,\n",
       "  658,\n",
       "  661,\n",
       "  675,\n",
       "  676,\n",
       "  679,\n",
       "  2728,\n",
       "  681,\n",
       "  684,\n",
       "  686,\n",
       "  2750,\n",
       "  703,\n",
       "  2758,\n",
       "  713,\n",
       "  2767,\n",
       "  721,\n",
       "  2771,\n",
       "  725,\n",
       "  733,\n",
       "  734,\n",
       "  737,\n",
       "  2787,\n",
       "  745,\n",
       "  2805,\n",
       "  2806,\n",
       "  759,\n",
       "  2813,\n",
       "  766,\n",
       "  772,\n",
       "  774,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  784,\n",
       "  785,\n",
       "  796,\n",
       "  2849,\n",
       "  804,\n",
       "  807,\n",
       "  808,\n",
       "  2855,\n",
       "  816,\n",
       "  825,\n",
       "  826,\n",
       "  830,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  843,\n",
       "  845,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  856,\n",
       "  857,\n",
       "  2904,\n",
       "  860,\n",
       "  862,\n",
       "  864,\n",
       "  2914,\n",
       "  870,\n",
       "  2920,\n",
       "  872,\n",
       "  882,\n",
       "  886,\n",
       "  890,\n",
       "  892,\n",
       "  896,\n",
       "  897,\n",
       "  907,\n",
       "  908,\n",
       "  910,\n",
       "  912,\n",
       "  916,\n",
       "  2966,\n",
       "  919,\n",
       "  2967,\n",
       "  920,\n",
       "  2970,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  934,\n",
       "  935,\n",
       "  940,\n",
       "  941,\n",
       "  954,\n",
       "  3008,\n",
       "  3010,\n",
       "  970,\n",
       "  973,\n",
       "  974,\n",
       "  982,\n",
       "  985,\n",
       "  986,\n",
       "  988,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  994,\n",
       "  995,\n",
       "  997,\n",
       "  1001,\n",
       "  1003,\n",
       "  1004,\n",
       "  3056,\n",
       "  3063,\n",
       "  1015,\n",
       "  1018,\n",
       "  1020,\n",
       "  1022,\n",
       "  1025,\n",
       "  1028,\n",
       "  3082,\n",
       "  1037,\n",
       "  1041,\n",
       "  1046,\n",
       "  1047,\n",
       "  1048,\n",
       "  3099,\n",
       "  1052,\n",
       "  3101,\n",
       "  1069,\n",
       "  3124,\n",
       "  1078,\n",
       "  1082,\n",
       "  1083,\n",
       "  1089,\n",
       "  1090,\n",
       "  1091,\n",
       "  1095,\n",
       "  1096,\n",
       "  3145,\n",
       "  1099,\n",
       "  3148,\n",
       "  1103,\n",
       "  1104,\n",
       "  1105,\n",
       "  1109,\n",
       "  3159,\n",
       "  1116,\n",
       "  3166,\n",
       "  1120,\n",
       "  1125,\n",
       "  3179,\n",
       "  1133,\n",
       "  3188,\n",
       "  1141,\n",
       "  1142,\n",
       "  1145,\n",
       "  1147,\n",
       "  1152,\n",
       "  1157,\n",
       "  3206,\n",
       "  3208,\n",
       "  3219,\n",
       "  1174,\n",
       "  1178,\n",
       "  1180,\n",
       "  3228,\n",
       "  1182,\n",
       "  3232,\n",
       "  1187,\n",
       "  3236,\n",
       "  3242,\n",
       "  1196,\n",
       "  3252,\n",
       "  1211,\n",
       "  1213,\n",
       "  3261,\n",
       "  1218,\n",
       "  1220,\n",
       "  1238,\n",
       "  1240,\n",
       "  3289,\n",
       "  1242,\n",
       "  1250,\n",
       "  3300,\n",
       "  1252,\n",
       "  3302,\n",
       "  1254,\n",
       "  1258,\n",
       "  1260,\n",
       "  3311,\n",
       "  3315,\n",
       "  1267,\n",
       "  3317,\n",
       "  1287,\n",
       "  3341,\n",
       "  1294,\n",
       "  3345,\n",
       "  3346,\n",
       "  1299,\n",
       "  1301,\n",
       "  1304,\n",
       "  3357,\n",
       "  1310,\n",
       "  3366,\n",
       "  3370,\n",
       "  1324,\n",
       "  1333,\n",
       "  1336,\n",
       "  1338,\n",
       "  1344,\n",
       "  3393,\n",
       "  1345,\n",
       "  3395,\n",
       "  1350,\n",
       "  269,\n",
       "  1360,\n",
       "  3411,\n",
       "  1374,\n",
       "  3429,\n",
       "  1385,\n",
       "  1393,\n",
       "  3447,\n",
       "  3448,\n",
       "  1401,\n",
       "  3450,\n",
       "  3458,\n",
       "  1410,\n",
       "  1415,\n",
       "  1417,\n",
       "  1425,\n",
       "  1431,\n",
       "  1433,\n",
       "  3487,\n",
       "  3490,\n",
       "  1443,\n",
       "  3496,\n",
       "  1449,\n",
       "  1454,\n",
       "  1457,\n",
       "  3506,\n",
       "  1461,\n",
       "  3517,\n",
       "  3523,\n",
       "  1477,\n",
       "  1480,\n",
       "  1483,\n",
       "  3537,\n",
       "  3538,\n",
       "  3545,\n",
       "  1506,\n",
       "  3564,\n",
       "  1527,\n",
       "  3577,\n",
       "  1530,\n",
       "  1546,\n",
       "  1548,\n",
       "  3597,\n",
       "  1551,\n",
       "  1556,\n",
       "  1557,\n",
       "  1566,\n",
       "  3614,\n",
       "  1568,\n",
       "  1572,\n",
       "  1579,\n",
       "  1582,\n",
       "  1584,\n",
       "  1589,\n",
       "  1611,\n",
       "  3661,\n",
       "  1615,\n",
       "  1621,\n",
       "  1625,\n",
       "  1626,\n",
       "  3676,\n",
       "  1637,\n",
       "  3693,\n",
       "  3699,\n",
       "  3705,\n",
       "  3709,\n",
       "  1661,\n",
       "  3724,\n",
       "  1680,\n",
       "  3728,\n",
       "  1683,\n",
       "  3735,\n",
       "  1698,\n",
       "  3753,\n",
       "  1713,\n",
       "  3764,\n",
       "  1718,\n",
       "  1719,\n",
       "  3766,\n",
       "  1724,\n",
       "  3777,\n",
       "  3780,\n",
       "  1738,\n",
       "  1739,\n",
       "  1742,\n",
       "  1743,\n",
       "  1747,\n",
       "  1755,\n",
       "  1757,\n",
       "  1758,\n",
       "  3805,\n",
       "  1769,\n",
       "  3823,\n",
       "  1787,\n",
       "  1790,\n",
       "  1794,\n",
       "  1813,\n",
       "  1814,\n",
       "  1816,\n",
       "  1826,\n",
       "  3881,\n",
       "  3887,\n",
       "  3888,\n",
       "  1841,\n",
       "  1861,\n",
       "  1875,\n",
       "  1881,\n",
       "  1882,\n",
       "  3933,\n",
       "  3934,\n",
       "  1890,\n",
       "  1894,\n",
       "  1900,\n",
       "  1901,\n",
       "  1902,\n",
       "  1905,\n",
       "  3960,\n",
       "  1917,\n",
       "  1924,\n",
       "  3974,\n",
       "  1927,\n",
       "  1939,\n",
       "  3990,\n",
       "  3996,\n",
       "  1950,\n",
       "  1954,\n",
       "  4003,\n",
       "  1961,\n",
       "  1963,\n",
       "  1967,\n",
       "  1968,\n",
       "  1969,\n",
       "  4016,\n",
       "  1971,\n",
       "  4027,\n",
       "  1980,\n",
       "  1987,\n",
       "  1988,\n",
       "  1994,\n",
       "  2014,\n",
       "  4069,\n",
       "  2025,\n",
       "  2028,\n",
       "  2032,\n",
       "  2033,\n",
       "  4082,\n",
       "  4087,\n",
       "  2040,\n",
       "  4093],\n",
       " 'visit_padded_node': tensor([[0., 0., 0.,  ..., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def get_subgraph(G, dataset, idx):\n",
    "    patient = dataset[idx]\n",
    "    L = G.edge_subgraph(torch.tensor(patient['node_set']))\n",
    "    P = L.subgraph(torch.tensor(patient['node_set']))\n",
    "    P.label = patient['label']\n",
    "    P.visits_cond = patient['visit_node_set_condition']\n",
    "    P.visits_proc = patient['visit_node_set_procedure']\n",
    "    P.visits_drug = patient['visit_node_set_drug']\n",
    "    \n",
    "    return P\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, G, dataset):\n",
    "        self.G = G\n",
    "        self.dataset=dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        return get_subgraph(G=self.G, dataset=self.dataset, idx=idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'visit_node_set_condition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl_bin.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_bin.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m P \u001b[39m=\u001b[39m get_subgraph(G_tg, train_dataset, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/pj20/experiment/GNN_model_impl_bin.ipynb Cell 5\u001b[0m in \u001b[0;36mget_subgraph\u001b[0;34m(G, dataset, idx)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_bin.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m P \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39msubgraph(torch\u001b[39m.\u001b[39mtensor(patient[\u001b[39m'\u001b[39m\u001b[39mnode_set\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_bin.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m P\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m patient[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_bin.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m P\u001b[39m.\u001b[39mvisits_cond \u001b[39m=\u001b[39m patient[\u001b[39m'\u001b[39;49m\u001b[39mvisit_node_set_condition\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_bin.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m P\u001b[39m.\u001b[39mvisits_proc \u001b[39m=\u001b[39m patient[\u001b[39m'\u001b[39m\u001b[39mvisit_node_set_procedure\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/GNN_model_impl_bin.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m P\u001b[39m.\u001b[39mvisits_drug \u001b[39m=\u001b[39m patient[\u001b[39m'\u001b[39m\u001b[39mvisit_node_set_drug\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'visit_node_set_condition'"
     ]
    }
   ],
   "source": [
    "P = get_subgraph(G_tg, train_dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(P.visits_cond[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GINConv, HGTConv\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.loader import DataListLoader\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1)\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # print(x.shape)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # print(x.shape)\n",
    "        logits = self.fc(x)\n",
    "        # print(logits.shape)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Linear(in_channels, hidden_channels))\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GINX(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, hidden_channels, out_channels, word_emb=None):\n",
    "        super(GINX, self).__init__()\n",
    "        \n",
    "        if word_emb == None:\n",
    "            self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "            self.conv1 = GINConv(Linear(embedding_dim, hidden_channels))\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(word_emb, freeze=False)\n",
    "            self.conv1 = GINConv(Linear(word_emb.shape[1], hidden_channels))\n",
    "\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, node_ids, edge_index, batch):\n",
    "        x = self.embedding(node_ids)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from pyhealth.models import RETAINLayer\n",
    "\n",
    "class GraphCare(nn.Module):\n",
    "    def __init__(self, num_nodes, feature_keys, embedding_dim, hidden_dim, out_channels, dropout=0.5, max_visits=None, word_emb=None, use_attn=True):\n",
    "        super(GraphCare, self).__init__()\n",
    "        self.max_visits = max_visits\n",
    "        self.max_nodes = len(word_emb)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_attn = use_attn\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "        if word_emb == None:\n",
    "            self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(word_emb, freeze=False)\n",
    "        \n",
    "        self.retain = nn.ModuleDict()\n",
    "        for feature_key in feature_keys:\n",
    "            self.retain[feature_key] = RETAINLayer(feature_size=self.max_nodes, dropout=dropout)\n",
    "        \n",
    "        self.conv1 = GINEConv(nn.Linear(embedding_dim, hidden_dim), edge_dim=1)\n",
    "        self.conv2 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=1)\n",
    "        self.conv3 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=1)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "    def forward(self, node_ids, edge_index, batch, visits_cond, visits_proc, visits_drug):\n",
    "        x = self.embedding(node_ids)  # (num_nodes, embedding_dim)\n",
    "        if self.use_attn == True:\n",
    "            cond_attn = self.retain['cond'](visits_cond)\n",
    "            proc_attn = self.retain['proc'](visits_proc)\n",
    "            drug_attn = self.retain['drug'](visits_drug)\n",
    "            cross_attn = self.retain['cross'](visits_cond + visits_proc + visits_drug)\n",
    "\n",
    "            attn = (cond_attn.add_(proc_attn).add_(drug_attn).add_(cross_attn))   # (batch_size, max_nodes)\n",
    "            \n",
    "            # Create a batch index tensor to map the batch index to the corresponding attention weight\n",
    "            batch_index = torch.arange(attn.size(0), device=node_ids.device).repeat_interleave(torch.bincount(batch))   \n",
    "            # print(\"batch index shape: \", batch_index.shape)\n",
    "            # print(\"edge index shape: \", edge_index.shape)\n",
    "            # Fill the attn_weights matrix with the correct weights using batch_index and node_ids\n",
    "            attn_weights = attn[batch_index, node_ids]\n",
    "            # Multiply the embeddings with the corresponding attention weights\n",
    "            # x = x * attn_weights\n",
    "            # x = (1 - self.alpha) * x + self.alpha * x\n",
    "            row, col = edge_index\n",
    "            # Define a small constant value epsilon\n",
    "            epsilon = 1e-6\n",
    "\n",
    "            # Calculate the geometric mean with added epsilon\n",
    "            # Normalize the attn_weights and replace NaNs with 0s\n",
    "            attn_weights = attn_weights / torch.max(attn_weights)\n",
    "            attn_weights = torch.where(torch.isnan(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n",
    "\n",
    "            # Calculate the geometric mean with added epsilon\n",
    "            edge_attr = ((attn_weights[row] + epsilon) + (attn_weights[col] + epsilon)).unsqueeze(-1)\n",
    "\n",
    "        # Apply the first GIN layer\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        # Apply the second GIN layer\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, jaccard_score\n",
    "    \n",
    "def train(model, device, train_loader, optimizer, model_):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    tot_loss = 0\n",
    "    pbar= tqdm(enumerate(train_loader))\n",
    "    for i, data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if model_ == \"GIN\":\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "        elif model_ == \"GINX\":\n",
    "            out = model(data.y, data.edge_index, data.batch)\n",
    "        else:\n",
    "            out = model(\n",
    "                    data.y, \n",
    "                    data.edge_index, \n",
    "                    data.batch, \n",
    "                    data.visits_cond.reshape(int(train_loader.batch_size), int(len(data.visits_cond)/train_loader.batch_size), data.visits_cond.shape[1]).double(), \n",
    "                    data.visits_proc.reshape(int(train_loader.batch_size), int(len(data.visits_proc)/train_loader.batch_size), data.visits_proc.shape[1]).double(), \n",
    "                    data.visits_drug.reshape(int(train_loader.batch_size), int(len(data.visits_drug)/train_loader.batch_size), data.visits_drug.shape[1]).double(),\n",
    "                )\n",
    "        try:\n",
    "            label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.float())\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return tot_loss\n",
    "\n",
    "def evaluate(model, device, loader, model_):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():    \n",
    "            \n",
    "            if model_ == \"GIN\":\n",
    "                logits = model(data.x, data.edge_index, data.batch)\n",
    "            elif model_ == \"GINX\":\n",
    "                logits = model(data.y, data.edge_index, data.batch)\n",
    "            else:\n",
    "                logits = model(\n",
    "                    data.y, \n",
    "                    data.edge_index, \n",
    "                    data.batch, \n",
    "                    data.visits_cond.reshape(int(loader.batch_size), int(len(data.visits_cond)/loader.batch_size), data.visits_cond.shape[1]).double(), \n",
    "                    data.visits_proc.reshape(int(loader.batch_size), int(len(data.visits_proc)/loader.batch_size), data.visits_proc.shape[1]).double(), \n",
    "                    data.visits_drug.reshape(int(loader.batch_size), int(len(data.visits_drug)/loader.batch_size), data.visits_drug.shape[1]).double(),\n",
    "                )\n",
    "\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "            try:\n",
    "                y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "            except:\n",
    "                continue\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "\n",
    "    return y_true_all, y_prob_all\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs, model_):\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer, model_)\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader, model_)\n",
    "\n",
    "        y_pred_all = (y_prob_all >= 0.5).astype(int)\n",
    "        \n",
    "        val_pr_auc = average_precision_score(y_true_all, y_prob_all)\n",
    "        val_roc_auc = roc_auc_score(y_true_all, y_prob_all)\n",
    "        val_jaccard = jaccard_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_acc = accuracy_score(y_true_all, y_pred_all)\n",
    "        val_f1 = f1_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_precision = precision_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "        val_recall = recall_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "\n",
    "        if val_acc >= best_acc and val_f1 >= best_f1:\n",
    "            torch.save(model.state_dict(), '../../../data/pj20/exp_data/saved_weights_gin_mimic3_readmission_dynamic.pkl')\n",
    "            print(\"best model saved\")\n",
    "            best_acc = val_acc\n",
    "            best_f1 = val_f1\n",
    "\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}, Val ROC_AUC: {val_roc_auc:.4f}, Val acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val precision: {val_precision:.4f}, Val recall: {val_recall:.4f}, Val jaccard: {val_jaccard:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_set = Dataset(G=G_tg, dataset=train_dataset)\n",
    "val_set = Dataset(G=G_tg, dataset=val_dataset)\n",
    "test_set = Dataset(G=G_tg, dataset=test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4599])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i, data in enumerate(train_loader):\n",
    "    # batch_size = torch.max(data.batch).item() + 1  # number of batches\n",
    "    # batch_x = []\n",
    "    # for i in range(batch_size):\n",
    "    #     batch_mask = (data.batch == i)\n",
    "    #     batch_nodes = torch.sum(batch_mask).item()  # number of nodes in this batch\n",
    "    #     batch_x.append(torch.masked_select(data.y, batch_mask).view(batch_nodes, -1))\n",
    "    #     print(len(batch_x[i]))\n",
    "    print(data.visits_cond.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GINX(\n",
       "  (embedding): Embedding(4599, 1536)\n",
       "  (conv1): GINConv(nn=Linear(in_features=1536, out_features=512, bias=True))\n",
       "  (conv2): GINConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  (conv3): GINConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = \"GINX\"\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if model_ == \"GIN\":\n",
    "    in_channels = train_set[0].x.shape[1]\n",
    "    model = GIN(in_channels=in_channels, out_channels=1, hidden_channels=512).to(device)\n",
    "    # model = GAT(in_channels=in_channels, out_channels=1, hidden_channels=256, heads=3).to(device)\n",
    "    # model = HGT(in_channels=in_channels, out_channels=out_channels, hidden_channels=512, heads=2).to(device)\n",
    "elif model_ == \"GINX\":\n",
    "    model = GINX(num_nodes=G_tg.num_nodes, embedding_dim=512, hidden_channels=512, out_channels=1, word_emb=G_tg.x).to(device)\n",
    "\n",
    "elif model_ == \"GraphCare\":\n",
    "    # model = GINX(num_nodes=G_tg.num_nodes, embedding_dim=512, hidden_channels=512, out_channels=out_channels, word_emb=G_tg.x).to(device)\n",
    "    model = GraphCare(\n",
    "        num_nodes=G_tg.num_nodes,\n",
    "        feature_keys=['cond', 'proc', 'cross'], \n",
    "        embedding_dim=len(G_tg.x[0]), \n",
    "        hidden_dim=512, \n",
    "        out_channels=1, \n",
    "        dropout=0.5, \n",
    "        max_visits=max_visits,\n",
    "        word_emb=G_tg.x,\n",
    "        use_attn=True\n",
    "    ).to(device)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100, model_=model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gat_mimic3_drugrec.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gin_mimic3_readmission.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_hgt_mimic3_drugrec.pkl')\n",
    "torch.save(model.state_dict(), '../../../data/pj20/exp_data/saved_weights_graphcare_mimic3_mortality_th02.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [00:29<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test PRAUC: 0.0854, test ROC_AUC: 0.5622, test acc: 0.9232, test F1: 0.4800, test precision: 0.9616, test recall: 0.5000, test jaccard: 0.4616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('./exp_data/saved_weights_gat_mimic3_drugrec.pkl'))\n",
    "# model.load_state_dict(torch.load('./exp_data/saved_weights_gin_mimic3_readmission_dynamic.pkl'))\n",
    "# model.double()\n",
    "\n",
    "y_true_all, y_prob_all = evaluate(model, device, val_loader, static)\n",
    "\n",
    "y_pred_all = (y_prob_all >= 0.5).astype(int)\n",
    "\n",
    "test_pr_auc = average_precision_score(y_true_all, y_prob_all)\n",
    "test_roc_auc = roc_auc_score(y_true_all, y_prob_all)\n",
    "test_jaccard = jaccard_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "test_acc = accuracy_score(y_true_all, y_pred_all)\n",
    "test_f1 = f1_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "test_precision = precision_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "test_recall = recall_score(y_true_all, y_pred_all, average='macro', zero_division=1)\n",
    "\n",
    "print(f'test PRAUC: {test_pr_auc:.4f}, test ROC_AUC: {test_roc_auc:.4f}, test acc: {test_acc:.4f}, test F1: {test_f1:.4f}, test precision: {test_precision:.4f}, test recall: {test_recall:.4f}, test jaccard: {test_jaccard:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.pretrained import PretrainedPipeline \n",
    "pipeline = PretrainedPipeline( \"icd10cm_umls_mapping\",\"en\",\"clinical/models\")\n",
    "pipeline.annotate([\"M8950\", \"R822\", \"R0901\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
