{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kg = \"UMLS-KG\"\n",
    "kg = \"GPT-KG\"\n",
    "# mode = \"node\"\n",
    "# mode = \"graph\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pyhealth.datasets import SampleDataset\n",
    "from pyhealth.datasets import split_by_patient\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "if kg == \"UMLS-KG\":\n",
    "    with open('../../../data/pj20/exp_data/icd9cm_icd9proc/drugrec_dataset_umls_1000.pkl', 'rb') as f:\n",
    "        sample_dataset = pickle.load(f)\n",
    "\n",
    "    with open('../../../data/pj20/exp_data/icd9cm_icd9proc/graph_umls_1000_cp.pkl', 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    with open('../../../data/pj20/exp_data/ccscm_ccsproc/sample_dataset_drugrec_th015.pkl', 'rb') as f:\n",
    "        sample_dataset = pickle.load(f)\n",
    "\n",
    "    with open('../../../data/pj20/exp_data/ccscm_ccsproc/graph_pd_th015.pkl', 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "\n",
    "with open('../../../data/pj20/exp_data/ccscm_ccsproc/clusters_inv_th015.json', 'r', encoding='utf-8') as f:\n",
    "    map_cluster_inv = json.load(f)\n",
    "\n",
    "with open('../../../data/pj20/exp_data/ccscm_ccsproc/clusters_th015.json', 'r', encoding='utf-8') as f:\n",
    "    map_cluster = json.load(f)\n",
    "\n",
    "with open('./graphs/cond_proc/CCSCM_CCSPROC/ent2id.json', 'r') as file:\n",
    "    ent2id = json.load(file)\n",
    "\n",
    "with open('./graphs/cond_proc/CCSCM_CCSPROC/entity_embedding.pkl', 'rb') as file:\n",
    "    ent_emb = pickle.load(file)\n",
    "\n",
    "ccscm_id2name = {}\n",
    "with open('./resources/CCSCM.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split(',')\n",
    "        ccscm_id2name[line[0]] = line[1]\n",
    "\n",
    "ccsproc_id2name = {}\n",
    "with open('./resources/CCSPROC.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        line = line.strip().split(',')\n",
    "        ccsproc_id2name[line[0]] = line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(map_cluster['0']['embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 29\n"
     ]
    }
   ],
   "source": [
    "c_v, p_v, d_v = [], [], []\n",
    "\n",
    "patient_id_set = set()\n",
    "\n",
    "for patient in sample_dataset:\n",
    "    c_v.append(len(patient['conditions']))\n",
    "    p_v.append(len(patient['procedures']))\n",
    "    patient_id_set.add(patient['patient_id'])\n",
    "\n",
    "i = 0\n",
    "pid_map = {}\n",
    "for patient_id in patient_id_set:\n",
    "    pid_map[patient_id] = i\n",
    "    i += 1\n",
    "\n",
    "for patient in sample_dataset:\n",
    "    patient['patient_id'] = pid_map[patient['patient_id']]\n",
    "\n",
    "print(max(c_v), max(p_v))\n",
    "max_visits = max(c_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44399/44399 [00:02<00:00, 16755.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def flatten(lst):\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(flatten(item))\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "G_new = G\n",
    "ex_pat_node_num = len(G_new.nodes)\n",
    "\n",
    "patient_set = set()\n",
    "i = 0\n",
    "\n",
    "for patient in tqdm(sample_dataset):\n",
    "    pat_node_id = int(ex_pat_node_num + i)\n",
    "    triples = []\n",
    "    nodes = []\n",
    "    patient['pat_node_id'] = pat_node_id\n",
    "    patient['node_set'].append(pat_node_id)\n",
    "    i += 1\n",
    "\n",
    "    # if patient['patient_id'] in patient_set:\n",
    "    #     continue\n",
    "    patient_set.add(patient['patient_id'])\n",
    "    \n",
    "    for condition in flatten(patient['conditions']):\n",
    "        try:\n",
    "            ehr_node = map_cluster_inv[ent2id[ccscm_id2name[condition].lower()]]\n",
    "            triples.append((pat_node_id, int(ehr_node)))\n",
    "            nodes.append(int(ehr_node))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    for procedure in flatten(patient['procedures']):\n",
    "        try:\n",
    "            ehr_node = map_cluster_inv[ent2id[ccsproc_id2name[procedure].lower()]]\n",
    "            triples.append((pat_node_id, int(ehr_node)))\n",
    "            nodes.append(int(ehr_node))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    nodes = np.array(nodes)\n",
    "\n",
    "    try:\n",
    "        G_new.add_nodes_from([\n",
    "            (pat_node_id, {'y': pat_node_id, 'x': np.mean(ent_emb[nodes], axis=0)})\n",
    "            ])\n",
    "    except:\n",
    "        #randomly initialize\n",
    "        G_new.add_nodes_from([\n",
    "            (pat_node_id, {'y': pat_node_id, 'x': np.random.rand(1536)})\n",
    "            ])\n",
    "    \n",
    "    \n",
    "    G_new.add_edges_from(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = G_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 2755])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset[1]['visit_node_set_condition'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "\n",
    "def get_subgraph(G, dataset, idx):\n",
    "    patient = dataset[idx]\n",
    "    while len(patient['node_set']) == 0:\n",
    "        idx -= 1\n",
    "        patient = dataset[idx]\n",
    "    # L = G.edge_subgraph(torch.tensor([*patient['node_set']]))\n",
    "    P = G.subgraph(torch.tensor([*patient['node_set']]))\n",
    "    P.label = patient['drugs_ind']\n",
    "    P.visits_cond = patient['visit_node_set_condition']\n",
    "    P.visits_proc = patient['visit_node_set_procedure']\n",
    "    P.patient_id = patient['pat_node_id']\n",
    "    \n",
    "    return P\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, G, dataset):\n",
    "        self.G = G\n",
    "        self.dataset=dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        return get_subgraph(G=self.G, dataset=self.dataset, idx=idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/utils/convert.py:250: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "G_tg = from_networkx(G) \n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv, GINConv, HGTConv\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.loader import DataListLoader\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1)\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # print(x.shape)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # print(x.shape)\n",
    "        logits = self.fc(x)\n",
    "        # print(logits.shape)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Linear(in_channels, hidden_channels))\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GINX(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim, hidden_channels, out_channels, word_emb=None):\n",
    "        super(GINX, self).__init__()\n",
    "        \n",
    "        if word_emb == None:\n",
    "            self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "            self.conv1 = GINConv(Linear(embedding_dim, hidden_channels))\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(word_emb, freeze=False)\n",
    "            self.conv1 = GINConv(Linear(word_emb.shape[1], hidden_channels))\n",
    "\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, node_ids, edge_index, batch):\n",
    "        x = self.embedding(node_ids)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from pyhealth.models import RETAINLayer\n",
    "\n",
    "class GraphCare(nn.Module):\n",
    "    def __init__(self, num_nodes, feature_keys, embedding_dim, hidden_dim, out_channels, dropout=0.5, max_visits=None, word_emb=None, use_attn=True, mode='graph', num_patient=None):\n",
    "        super(GraphCare, self).__init__()\n",
    "        self.max_visits = max_visits\n",
    "        self.max_nodes = len(word_emb)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.use_attn = use_attn\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "        self.mode = mode\n",
    "        self.pat_node_emb = None\n",
    "\n",
    "        if word_emb == None:\n",
    "            self.embedding = torch.nn.Embedding(num_nodes, embedding_dim)\n",
    "        else:\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(word_emb, freeze=True)\n",
    "        \n",
    "        if self.use_attn:\n",
    "            self.conv1 = GINEConv(nn.Linear(embedding_dim, hidden_dim), edge_dim=1)\n",
    "            self.conv2 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=1)\n",
    "            self.conv3 = GINEConv(nn.Linear(hidden_dim, hidden_dim), edge_dim=1)\n",
    "\n",
    "            self.retain = nn.ModuleDict()\n",
    "            for feature_key in feature_keys:\n",
    "                self.retain[feature_key] = RETAINLayer(feature_size=self.max_nodes, dropout=dropout)\n",
    "        else:\n",
    "            self.conv1 = GINConv(Linear(embedding_dim, hidden_dim))\n",
    "            self.conv2 = GINConv(Linear(hidden_dim, hidden_dim))\n",
    "            self.conv3 = GINConv(Linear(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, node_ids, edge_index, batch, visits_cond, visits_proc, patient_id=None):\n",
    "        x = self.embedding(node_ids)\n",
    "        patient_indices = torch.tensor([(node_ids == patient_id[i]).nonzero(as_tuple=True)[0].item() for i in range(len(patient_id))])\n",
    "\n",
    "        if self.use_attn:\n",
    "\n",
    "            cond_attn = self.retain['cond'](visits_cond)\n",
    "            proc_attn = self.retain['proc'](visits_proc)\n",
    "            # cross_attn = self.retain['cross'](visits_cond + visits_proc)\n",
    "\n",
    "            attn = cond_attn.add_(proc_attn)   # (batch_size, max_nodes)\n",
    "\n",
    "            # Create a batch index tensor to map the batch index to the corresponding attention weight\n",
    "            batch_index = torch.arange(attn.size(0), device=node_ids.device).repeat_interleave(torch.bincount(batch))   \n",
    "\n",
    "            # Fill the attn_weights matrix with the correct weights using batch_index and node_ids\n",
    "            attn_weights = attn[batch_index, node_ids]\n",
    "\n",
    "            row, col = edge_index\n",
    "            # Define a small constant value epsilon\n",
    "            epsilon = 1e-6\n",
    "\n",
    "            attn_weights = attn_weights / torch.max(attn_weights)\n",
    "            attn_weights = torch.where(torch.isnan(attn_weights), torch.zeros_like(attn_weights), attn_weights)\n",
    "\n",
    "            edge_attr = ((attn_weights[row] + epsilon) + (attn_weights[col] + epsilon)).unsqueeze(-1)\n",
    "\n",
    "\n",
    "            x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "            x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "            x = F.relu(self.conv3(x, edge_index, edge_attr))\n",
    "\n",
    "        else:\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = F.relu(self.conv2(x, edge_index))\n",
    "            x = F.relu(self.conv3(x, edge_index))\n",
    "\n",
    "        if self.mode == \"graph\":\n",
    "            x = global_mean_pool(x, batch)\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        elif self.mode == \"node\":\n",
    "            patient_node_embeddings = x[patient_indices]\n",
    "            x = F.dropout(patient_node_embeddings, p=0.5, training=self.training)\n",
    "\n",
    "        logits = self.fc(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     print(data)\n",
    "#     patient_id = data.patient_id\n",
    "#     print(patient_id)\n",
    "#     print(data.y)\n",
    "#     print(patient_id[0])\n",
    "#     print(patient_id[0] in data.y)\n",
    "#     print(torch.tensor([(data.y == patient_id[i]).nonzero(as_tuple=True)[0].item() for i in range(len(patient_id))]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, jaccard_score\n",
    "    \n",
    "def train(model, device, train_loader, optimizer, model_):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    tot_loss = 0\n",
    "    pbar= tqdm(enumerate(train_loader))\n",
    "    for i, data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if model_ == \"GIN\":\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "        elif model_ == \"GINX\":\n",
    "            out = model(data.y, data.edge_index, data.batch)\n",
    "        else:\n",
    "            out = model(\n",
    "                    data.y, \n",
    "                    data.edge_index, \n",
    "                    data.batch, \n",
    "                    data.visits_cond.reshape(int(train_loader.batch_size), int(len(data.visits_cond)/train_loader.batch_size), data.visits_cond.shape[1]).double(), \n",
    "                    data.visits_proc.reshape(int(train_loader.batch_size), int(len(data.visits_proc)/train_loader.batch_size), data.visits_proc.shape[1]).double(), \n",
    "                    # data.visits_drug.reshape(int(train_loader.batch_size), int(len(data.visits_drug)/train_loader.batch_size), data.visits_drug.shape[1]).double(),\n",
    "                    patient_id = data.patient_id.reshape(int(train_loader.batch_size), int(len(data.patient_id)/train_loader.batch_size)).long()\n",
    "                    \n",
    "                )\n",
    "        try:\n",
    "            label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "        except:\n",
    "            continue\n",
    "        # print(out.shape, label.shape)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label.float())\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        tot_loss += loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return tot_loss\n",
    "\n",
    "def evaluate(model, device, loader, model_):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():    \n",
    "            \n",
    "            if model_ == \"GIN\":\n",
    "                logits = model(data.x, data.edge_index, data.batch)\n",
    "            elif model_ == \"GINX\":\n",
    "                logits = model(data.y, data.edge_index, data.batch)\n",
    "            else:\n",
    "                logits = model(\n",
    "                    data.y, \n",
    "                    data.edge_index, \n",
    "                    data.batch, \n",
    "                    data.visits_cond.reshape(int(loader.batch_size), int(len(data.visits_cond)/loader.batch_size), data.visits_cond.shape[1]).double(), \n",
    "                    data.visits_proc.reshape(int(loader.batch_size), int(len(data.visits_proc)/loader.batch_size), data.visits_proc.shape[1]).double(), \n",
    "                    # data.visits_drug.reshape(int(loader.batch_size), int(len(data.visits_drug)/loader.batch_size), data.visits_drug.shape[1]).double(),\n",
    "                    patient_id = data.patient_id.reshape(int(loader.batch_size), int(len(data.patient_id)/loader.batch_size)).long()\n",
    "                )\n",
    "\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "            try:\n",
    "                y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "            except:\n",
    "                continue\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "\n",
    "    return y_true_all, y_prob_all\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs, model_, task_name):\n",
    "    best_acc = 0\n",
    "    best_f1 = 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer, model_)\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader, model_)\n",
    "\n",
    "        y_pred_all = (y_prob_all >= 0.5).astype(int)\n",
    "        \n",
    "        val_pr_auc = average_precision_score(y_true_all, y_prob_all, average='samples')\n",
    "        val_roc_auc = roc_auc_score(y_true_all, y_prob_all, average='samples')\n",
    "        val_jaccard = jaccard_score(y_true_all, y_pred_all, average='samples', zero_division=1)\n",
    "        val_acc = accuracy_score(y_true_all, y_pred_all)\n",
    "        val_f1 = f1_score(y_true_all, y_pred_all, average='samples', zero_division=1)\n",
    "        val_precision = precision_score(y_true_all, y_pred_all, average='samples', zero_division=1)\n",
    "        val_recall = recall_score(y_true_all, y_pred_all, average='samples', zero_division=1)\n",
    "\n",
    "        if val_acc >= best_acc and val_f1 >= best_f1:\n",
    "            torch.save(model.state_dict(), f'../../../data/pj20/exp_data/saved_weights_{model_}_{task_name}.pkl')\n",
    "            print(\"best model saved\")\n",
    "            best_acc = val_acc\n",
    "            best_f1 = val_f1\n",
    "\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}, Val ROC_AUC: {val_roc_auc:.4f}, Val acc: {val_acc:.4f}, Val F1: {val_f1:.4f}, Val precision: {val_precision:.4f}, Val recall: {val_recall:.4f}, Val jaccard: {val_jaccard:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_tg.x = torch.randn(G_tg.num_nodes, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_set = Dataset(G=G_tg, dataset=train_dataset)\n",
    "val_set = Dataset(G=G_tg, dataset=val_dataset)\n",
    "test_set = Dataset(G=G_tg, dataset=test_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=16, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11447"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_tg.num_nodes - len(patient_id_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphCare(\n",
       "  (embedding): Embedding(47154, 1536)\n",
       "  (conv1): GINConv(nn=Linear(in_features=1536, out_features=512, bias=True))\n",
       "  (conv2): GINConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  (conv3): GINConv(nn=Linear(in_features=512, out_features=512, bias=True))\n",
       "  (fc): Linear(in_features=512, out_features=197, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = \"GraphCare\"\n",
    "out_channels = len(train_set[0].label)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if model_ == \"GIN\":\n",
    "    in_channels = train_set[0].x.shape[1]\n",
    "    model = GIN(in_channels=in_channels, out_channels=out_channels, hidden_channels=512).to(device)\n",
    "    # model = GAT(in_channels=in_channels, out_channels=1, hidden_channels=256, heads=3).to(device)\n",
    "    # model = HGT(in_channels=in_channels, out_channels=out_channels, hidden_channels=512, heads=2).to(device)\n",
    "elif model_ == \"GINX\":\n",
    "    model = GINX(num_nodes=G_tg.num_nodes, embedding_dim=512, hidden_channels=512, out_channels=out_channels, word_emb=G_tg.x).to(device)\n",
    "\n",
    "elif model_ == \"GraphCare\":\n",
    "    # model = GINX(num_nodes=G_tg.num_nodes, embedding_dim=512, hidden_channels=512, out_channels=out_channels, word_emb=G_tg.x).to(device)\n",
    "    model = GraphCare(\n",
    "        num_nodes=G_tg.num_nodes - len(patient_id_set),\n",
    "        feature_keys=['cond', 'proc'], \n",
    "        embedding_dim=len(G_tg.x[0]), \n",
    "        hidden_dim=512, \n",
    "        out_channels=out_channels, \n",
    "        dropout=0.5, \n",
    "        max_visits=max_visits,\n",
    "        word_emb=G_tg.x,\n",
    "        use_attn=False,\n",
    "        mode=\"node\",\n",
    "        num_patient=len(patient_id_set),\n",
    "    ).to(device)\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21572403609752655: : 2223it [04:33,  8.11it/s]\n",
      "100%|██████████| 271/271 [00:33<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model saved\n",
      "Epoch: 1, Training loss: 525.5340576171875, Val PRAUC: 0.7118, Val ROC_AUC: 0.9251, Val acc: 0.0212, Val F1: 0.5578, Val precision: 0.7232, Val recall: 0.4962, Val jaccard: 0.4048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22138352692127228: : 2112it [04:10,  8.01it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "task_name = \"drugrec_th015\"\n",
    "\n",
    "state_dict = torch.load(f'../../../data/pj20/exp_data/saved_weights_{model_}_{task_name}.pkl')\n",
    "model.load_state_dict(state_dict)\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100, model_=model_, task_name=task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gat_mimic3_drugrec.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gin_mimic3_drugrec_random.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_hgt_mimic3_drugrec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'../../../data/pj20/exp_data/saved_weights_{model_}_{task_name}.pkl'))\n",
    "model.double()\n",
    "\n",
    "y_true_all, y_prob_all = evaluate(model, device, val_loader, static)\n",
    "\n",
    "y_pred_all = y_prob_all.copy()\n",
    "y_pred_all[y_pred_all >= 0.5] = 1\n",
    "y_pred_all[y_pred_all < 0.5] = 0\n",
    "\n",
    "test_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "test_roc_auc = roc_auc_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "test_f1 = f1_score(y_true_all, y_pred_all, average='samples')\n",
    "test_jaccard = jaccard_score(y_true_all, y_pred_all, average='samples')\n",
    "\n",
    "print(f'test PRAUC: {test_pr_auc:.4f}, test ROC_AUC: {test_roc_auc:.4f}, test F1-score: {test_f1:.4f}, test Jaccard: {test_jaccard:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
