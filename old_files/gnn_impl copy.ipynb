{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_dir = \"./graphs/condition/CCSCM\"\n",
    "\n",
    "file_id2ent = f\"{file_dir}/id2ent.json\"\n",
    "file_ent2id = f\"{file_dir}/ent2id.json\"\n",
    "file_id2rel = f\"{file_dir}/id2rel.json\"\n",
    "file_rel2id = f\"{file_dir}/rel2id.json\"\n",
    "\n",
    "with open(file_id2ent, 'r') as file:\n",
    "    cond_id2ent = json.load(file)\n",
    "with open(file_ent2id, 'r') as file:\n",
    "    cond_ent2id = json.load(file)\n",
    "with open(file_id2rel, 'r') as file:\n",
    "    cond_id2rel = json.load(file)\n",
    "with open(file_rel2id, 'r') as file:\n",
    "    cond_rel2id = json.load(file)\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "condition_mapping_file = \"./resources/CCSCM.csv\"\n",
    "procedure_mapping_file = \"./resources/CCSPROC.csv\"\n",
    "drug_file = \"./resources/ATC.csv\"\n",
    "\n",
    "condition_dict = {}\n",
    "with open(condition_mapping_file, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        condition_dict[row['code']] = row['name'].lower()\n",
    "\n",
    "procedure_dict = {}\n",
    "with open(procedure_mapping_file, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        procedure_dict[row['code']] = row['name'].lower()\n",
    "\n",
    "drug_dict = {}\n",
    "with open(drug_file, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if row['level'] == '5.0':\n",
    "            drug_dict[row['code']] = row['name'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyhealth.datasets import MIMIC3Dataset\n",
    "# from GraphCare.task_fn import drug_recommendation_fn\n",
    "\n",
    "# mimic3_ds = MIMIC3Dataset(\n",
    "#     root=\"../../../data/physionet.org/files/mimiciii/1.4/\", \n",
    "#     tables=[\"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"],      \n",
    "#     code_mapping={\n",
    "#         \"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}}),\n",
    "#         \"ICD9CM\": \"CCSCM\",\n",
    "#         \"ICD9PROC\": \"CCSPROC\"\n",
    "#         },\n",
    "# )\n",
    "\n",
    "# sample_dataset = mimic3_ds.set_task(drug_recommendation_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    result = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            result.extend(flatten(item))\n",
    "        else:\n",
    "            result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.tokenizer import Tokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def multihot(label, num_labels):\n",
    "    multihot = np.zeros(num_labels)\n",
    "    for l in label:\n",
    "        multihot[l] = 1\n",
    "    return multihot\n",
    "\n",
    "def prepare_label(drugs):\n",
    "    label_tokenizer = Tokenizer(\n",
    "        sample_dataset.get_all_tokens(key='drugs')\n",
    "    )\n",
    "\n",
    "    labels_index = label_tokenizer.convert_tokens_to_indices(drugs)\n",
    "    # print(labels_index)\n",
    "    # convert to multihot\n",
    "    num_labels = label_tokenizer.get_vocabulary_size()\n",
    "    # print(num_labels)\n",
    "    labels = multihot(labels_index, num_labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# for patient in tqdm(sample_dataset):\n",
    "#     # patient['drugs_all'] = flatten(patient['drugs'])\n",
    "#     # print(patient['drugs_all'])\n",
    "#     patient['drugs_ind'] = torch.tensor(prepare_label(patient['drugs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./exp_data/ccscm_ccsproc/sample_dataset.pkl', 'wb') as f:\n",
    "#     pickle.dump(sample_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./exp_data/ccscm_ccsproc/sample_dataset.pkl', 'rb') as f:\n",
    "    sample_dataset= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import split_by_patient\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(sample_dataset, [0.8, 0.1, 0.1], seed=528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['101', '106', '98', '138']], [['44', '47', '50']])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]['conditions'], train_dataset[0]['procedures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "with open('./graphs/cond_proc/CCSCM_CCSPROC/ent2id.json', 'r') as file:\n",
    "    ent2id = json.load(file)\n",
    "with open('./graphs/cond_proc/CCSCM_CCSPROC/rel2id.json', 'r') as file:\n",
    "    rel2id = json.load(file)\n",
    "with open('./graphs/cond_proc/CCSCM_CCSPROC/entity_embedding.pkl', 'rb') as file:\n",
    "    ent_emb = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02357265,  0.002313  ,  0.02204529, ..., -0.01157682,\n",
       "        0.01255   ,  0.00188047])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44399/44399 [02:26<00:00, 303.22it/s]\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "for i in range(len(ent_emb)):\n",
    "    G.add_nodes_from([\n",
    "        (i, {'y': i, 'x': ent_emb[i]})\n",
    "    ])\n",
    "\n",
    "triples_all = []\n",
    "for patient in tqdm(sample_dataset):\n",
    "    triples = []\n",
    "    triple_set = set()\n",
    "    # node_set = set()\n",
    "    conditions = flatten(patient['conditions'])\n",
    "    for condition in conditions:\n",
    "        cond_file = f'./graphs/condition/CCSCM/{condition}.txt'\n",
    "        with open(cond_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items = line.split('\\t')\n",
    "            if len(items) == 3:\n",
    "                h, r, t = items\n",
    "                t = t[:-1]\n",
    "                h = int(ent2id[h])\n",
    "                r = int(rel2id[r])\n",
    "                t = int(ent2id[t])\n",
    "                triple = (h, r, t)\n",
    "                if triple not in triple_set:\n",
    "                    triples.append((h, t))\n",
    "                    triple_set.add(triple)\n",
    "                    # node_set.add(h)\n",
    "                    # node_set.add(r)\n",
    "    procedures = flatten(patient['procedures'])\n",
    "    for procedure in procedures:\n",
    "        proc_file = f'./graphs/procedure/CCSPROC/{procedure}.txt'\n",
    "        with open(proc_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items = line.split('\\t')\n",
    "            if len(items) == 3:\n",
    "                h, r, t = items\n",
    "                t = t[:-1]\n",
    "                h = int(ent2id[h])\n",
    "                r = int(rel2id[r])\n",
    "                t = int(ent2id[t])\n",
    "                triple = (h, r, t)\n",
    "                if triple not in triple_set:\n",
    "                    triples.append((h, t))\n",
    "                    triple_set.add(triple)\n",
    "\n",
    "    G.add_edges_from(\n",
    "        triples,\n",
    "        # label=prepare_label(patient['drugs'])\n",
    "    )\n",
    "    \n",
    "    # triples.append(prepare_label(patient['drugs']))\n",
    "    # triples_all.append(np.array(triples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/utils/convert.py:250: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "import pickle\n",
    "\n",
    "G_tg = from_networkx(G)\n",
    "\n",
    "with open('./exp_data/ccscm_ccsproc/graph_tg.pkl', 'wb') as f:\n",
    "    pickle.dump(G, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_subgraph(dataset, idx):\n",
    "    \n",
    "    subgraph_list = []\n",
    "    # for patient in tqdm(dataset):\n",
    "    patient = dataset[idx]\n",
    "    triple_set = set()\n",
    "    node_set = set()\n",
    "    conditions = flatten(patient['conditions'])\n",
    "    for condition in conditions:\n",
    "        cond_file = f'./graphs/condition/CCSCM/{condition}.txt'\n",
    "        with open(cond_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items = line.split('\\t')\n",
    "            if len(items) == 3:\n",
    "                h, r, t = items\n",
    "                t = t[:-1]\n",
    "                h = int(ent2id[h])\n",
    "                r = int(rel2id[r])\n",
    "                t = int(ent2id[t])\n",
    "                triple = (h, r, t)\n",
    "                if triple not in triple_set:\n",
    "                    triple_set.add(triple)\n",
    "                    node_set.add(h)\n",
    "                    node_set.add(r)\n",
    "\n",
    "    procedures = flatten(patient['procedures'])\n",
    "    for procedure in procedures:\n",
    "        proc_file = f'./graphs/procedure/CCSPROC/{procedure}.txt'\n",
    "        with open(proc_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            items = line.split('\\t')\n",
    "            if len(items) == 3:\n",
    "                h, r, t = items\n",
    "                t = t[:-1]\n",
    "                h = int(ent2id[h])\n",
    "                r = int(rel2id[r])\n",
    "                t = int(ent2id[t])\n",
    "                triple = (h, r, t)\n",
    "                if triple not in triple_set:\n",
    "                    triple_set.add(triple)\n",
    "                    node_set.add(h)\n",
    "                    node_set.add(r)\n",
    "\n",
    "    P = G_tg.subgraph(torch.tensor([*node_set]))\n",
    "    P.label = patient['drugs_ind']\n",
    "        # subgraph_list.append(P)\n",
    "\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_graph_list = get_subgraph(train_dataset)\n",
    "# val_graph_list = get_subgraph(val_dataset)\n",
    "# test_graph_list = get_subgraph(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset=dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        return get_subgraph(dataset=self.dataset, idx=idx)\n",
    "\n",
    "train_set = Dataset(train_dataset)\n",
    "val_set = Dataset(val_dataset)\n",
    "test_set = Dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.loader import DataListLoader\n",
    "import pickle\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels*heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = GATConv(hidden_channels*heads, hidden_channels, heads=1)\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv2(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = F.elu(self.conv3(x, edge_index))\n",
    "        # print(x.shape)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        # print(x.shape)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # print(x.shape)\n",
    "        logits = self.fc(x)\n",
    "        # print(logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINConv\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(Linear(in_channels, hidden_channels))\n",
    "        self.conv2 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "        self.conv3 = GINConv(Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HGTConv\n",
    "\n",
    "class HGT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=2):\n",
    "        super(HGT, self).__init__()\n",
    "        self.conv1 = HGTConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = HGTConv(hidden_channels*heads, hidden_channels, heads=heads)\n",
    "        self.conv3 = HGTConv(hidden_channels*heads, hidden_channels, heads=1)\n",
    "\n",
    "        self.fc = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    training_loss = 0\n",
    "    pbar = tqdm(train_loader)\n",
    "    for data in pbar:\n",
    "        pbar.set_description(f'loss: {training_loss}')\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        try:\n",
    "            label = data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size))\n",
    "        except:\n",
    "            continue\n",
    "        loss = F.binary_cross_entropy_with_logits(out, label)\n",
    "        loss.backward()\n",
    "        training_loss = loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    return training_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_loader:\n",
    "#     print(train_loader.batch_size)\n",
    "#     print(len(data.label))\n",
    "#     print(data.label.reshape(int(train_loader.batch_size), int(len(data.label)/train_loader.batch_size)).shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "\n",
    "def evaluate(model, device, loader):\n",
    "    model.eval()\n",
    "    y_prob_all = []\n",
    "    y_true_all = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(data.x, data.edge_index, data.batch)\n",
    "            y_prob = torch.sigmoid(logits)\n",
    "            try:\n",
    "                y_true = data.label.reshape(int(loader.batch_size), int(len(data.label)/loader.batch_size))\n",
    "            except:\n",
    "                continue\n",
    "            y_prob_all.append(y_prob.cpu())\n",
    "            y_true_all.append(y_true.cpu())\n",
    "            \n",
    "    y_true_all = np.concatenate(y_true_all, axis=0)\n",
    "    y_prob_all = np.concatenate(y_prob_all, axis=0)\n",
    "    # pr_auc = multilabel_metrics_fn(y_true=y_true_all, y_prob=y_true_all, metrics=\"pr_auc_macro\")\n",
    "\n",
    "    return y_true_all, y_prob_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def train_loop(train_loader, val_loader, model, optimizer, device, epochs):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train(model, device, train_loader, optimizer)\n",
    "        # y_true_all, y_prob_all = evaluate(model, device, train_loader)\n",
    "        # train_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"macro\")\n",
    "        y_true_all, y_prob_all = evaluate(model, device, val_loader)\n",
    "        val_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "        print(f'Epoch: {epoch}, Training loss: {loss}, Val PRAUC: {val_pr_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 197)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].x.shape[1], len(train_set[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pj20/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20845470383603468: 100%|██████████| 2224/2224 [08:51<00:00,  4.19it/s]\n",
      "100%|██████████| 544/544 [01:05<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 0.20845470383603468, Val PRAUC: 0.7174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23972805611222395: 100%|██████████| 2224/2224 [09:05<00:00,  4.08it/s]\n",
      "100%|██████████| 544/544 [01:04<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training loss: 0.23972805611222395, Val PRAUC: 0.7288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22434450699606728: 100%|██████████| 2224/2224 [08:41<00:00,  4.26it/s]\n",
      "100%|██████████| 544/544 [01:05<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training loss: 0.22434450699606728, Val PRAUC: 0.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2418892674426145: 100%|██████████| 2224/2224 [08:37<00:00,  4.30it/s] \n",
      "100%|██████████| 544/544 [00:58<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training loss: 0.2418892674426145, Val PRAUC: 0.7354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22962764692098747: 100%|██████████| 2224/2224 [09:08<00:00,  4.05it/s]\n",
      "100%|██████████| 544/544 [01:00<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training loss: 0.22962764692098747, Val PRAUC: 0.7387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24029206685211477: 100%|██████████| 2224/2224 [08:22<00:00,  4.42it/s]\n",
      "100%|██████████| 544/544 [00:53<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training loss: 0.24029206685211477, Val PRAUC: 0.7409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2112569540783083: 100%|██████████| 2224/2224 [08:26<00:00,  4.39it/s] \n",
      "100%|██████████| 544/544 [00:54<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training loss: 0.2112569540783083, Val PRAUC: 0.7395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22318523439524726: 100%|██████████| 2224/2224 [08:13<00:00,  4.51it/s]\n",
      "100%|██████████| 544/544 [00:54<00:00, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training loss: 0.22318523439524726, Val PRAUC: 0.7473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20956010375467465: 100%|██████████| 2224/2224 [08:22<00:00,  4.42it/s]\n",
      "100%|██████████| 544/544 [00:55<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training loss: 0.20956010375467465, Val PRAUC: 0.7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21258448169505043: 100%|██████████| 2224/2224 [08:13<00:00,  4.50it/s]\n",
      "100%|██████████| 544/544 [00:54<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training loss: 0.21258448169505043, Val PRAUC: 0.7522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20101141359798774: 100%|██████████| 2224/2224 [08:26<00:00,  4.39it/s]\n",
      "100%|██████████| 544/544 [00:56<00:00,  9.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training loss: 0.20101141359798774, Val PRAUC: 0.7545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.17546434132352914: 100%|██████████| 2224/2224 [08:22<00:00,  4.42it/s]\n",
      "100%|██████████| 544/544 [00:55<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training loss: 0.17546434132352914, Val PRAUC: 0.7536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21171690192051368: 100%|██████████| 2224/2224 [08:23<00:00,  4.42it/s]\n",
      "100%|██████████| 544/544 [00:52<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training loss: 0.21171690192051368, Val PRAUC: 0.7524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1776039270844646: 100%|██████████| 2224/2224 [08:25<00:00,  4.40it/s] \n",
      "100%|██████████| 544/544 [00:52<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Training loss: 0.1776039270844646, Val PRAUC: 0.7559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.26245010133760927: 100%|██████████| 2224/2224 [08:24<00:00,  4.41it/s]\n",
      "100%|██████████| 544/544 [00:56<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Training loss: 0.26245010133760927, Val PRAUC: 0.7546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22165916829809776: 100%|██████████| 2224/2224 [08:26<00:00,  4.39it/s]\n",
      "100%|██████████| 544/544 [00:56<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Training loss: 0.22165916829809776, Val PRAUC: 0.7584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19589370207235915: 100%|██████████| 2224/2224 [08:45<00:00,  4.23it/s]\n",
      "100%|██████████| 544/544 [00:58<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Training loss: 0.19589370207235915, Val PRAUC: 0.7595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.17310211766259195: 100%|██████████| 2224/2224 [08:28<00:00,  4.37it/s]\n",
      "100%|██████████| 544/544 [00:56<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Training loss: 0.17310211766259195, Val PRAUC: 0.7579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19706800228609456: 100%|██████████| 2224/2224 [08:28<00:00,  4.37it/s]\n",
      "100%|██████████| 544/544 [00:55<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training loss: 0.19706800228609456, Val PRAUC: 0.7610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23533733806053414: 100%|██████████| 2224/2224 [08:20<00:00,  4.44it/s]\n",
      "100%|██████████| 544/544 [00:55<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Training loss: 0.23533733806053414, Val PRAUC: 0.7604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.208458969008806: 100%|██████████| 2224/2224 [08:42<00:00,  4.25it/s]  \n",
      "100%|██████████| 544/544 [00:55<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Training loss: 0.208458969008806, Val PRAUC: 0.7613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2005399138125495: 100%|██████████| 2224/2224 [08:37<00:00,  4.29it/s] \n",
      "100%|██████████| 544/544 [00:56<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Training loss: 0.2005399138125495, Val PRAUC: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.16708124847935812: 100%|██████████| 2224/2224 [08:44<00:00,  4.24it/s]\n",
      "100%|██████████| 544/544 [00:57<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Training loss: 0.16708124847935812, Val PRAUC: 0.7635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19868043015748374: 100%|██████████| 2224/2224 [08:41<00:00,  4.26it/s]\n",
      "100%|██████████| 544/544 [01:06<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Training loss: 0.19868043015748374, Val PRAUC: 0.7626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.17750884154332322: 100%|██████████| 2224/2224 [08:42<00:00,  4.26it/s]\n",
      "100%|██████████| 544/544 [00:55<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Training loss: 0.17750884154332322, Val PRAUC: 0.7647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1761976968581584: 100%|██████████| 2224/2224 [08:35<00:00,  4.31it/s] \n",
      "100%|██████████| 544/544 [00:58<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Training loss: 0.1761976968581584, Val PRAUC: 0.7650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.18582236073673208: 100%|██████████| 2224/2224 [08:40<00:00,  4.28it/s]\n",
      "100%|██████████| 544/544 [00:58<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Training loss: 0.18582236073673208, Val PRAUC: 0.7656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2638587587362063: 100%|██████████| 2224/2224 [08:51<00:00,  4.18it/s] \n",
      "100%|██████████| 544/544 [00:58<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Training loss: 0.2638587587362063, Val PRAUC: 0.7649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20087744499222818: 100%|██████████| 2224/2224 [08:48<00:00,  4.21it/s]\n",
      "100%|██████████| 544/544 [00:58<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Training loss: 0.20087744499222818, Val PRAUC: 0.7642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1780377905530934: 100%|██████████| 2224/2224 [08:53<00:00,  4.17it/s] \n",
      "100%|██████████| 544/544 [00:58<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Training loss: 0.1780377905530934, Val PRAUC: 0.7662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22515538179057615: 100%|██████████| 2224/2224 [08:46<00:00,  4.22it/s]\n",
      "100%|██████████| 544/544 [00:57<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Training loss: 0.22515538179057615, Val PRAUC: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1847679121280134: 100%|██████████| 2224/2224 [08:45<00:00,  4.23it/s] \n",
      "100%|██████████| 544/544 [00:59<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Training loss: 0.1847679121280134, Val PRAUC: 0.7670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2540184548991319: 100%|██████████| 2224/2224 [08:48<00:00,  4.21it/s] \n",
      "100%|██████████| 544/544 [00:58<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Training loss: 0.2540184548991319, Val PRAUC: 0.7670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20348826640186155: 100%|██████████| 2224/2224 [08:52<00:00,  4.18it/s]\n",
      "100%|██████████| 544/544 [01:00<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Training loss: 0.20348826640186155, Val PRAUC: 0.7657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20162477042439256: 100%|██████████| 2224/2224 [08:44<00:00,  4.24it/s]\n",
      "100%|██████████| 544/544 [00:58<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Training loss: 0.20162477042439256, Val PRAUC: 0.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21079879979609947: 100%|██████████| 2224/2224 [08:39<00:00,  4.28it/s]\n",
      "100%|██████████| 544/544 [01:04<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Training loss: 0.21079879979609947, Val PRAUC: 0.7692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.226747016133661: 100%|██████████| 2224/2224 [08:36<00:00,  4.30it/s]  \n",
      "100%|██████████| 544/544 [00:55<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Training loss: 0.226747016133661, Val PRAUC: 0.7680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2285863094093403: 100%|██████████| 2224/2224 [08:33<00:00,  4.33it/s] \n",
      "100%|██████████| 544/544 [00:45<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Training loss: 0.2285863094093403, Val PRAUC: 0.7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.18187432528792763: 100%|██████████| 2224/2224 [07:23<00:00,  5.02it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Training loss: 0.18187432528792763, Val PRAUC: 0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1910438167349075: 100%|██████████| 2224/2224 [07:25<00:00,  4.99it/s] \n",
      "100%|██████████| 544/544 [00:47<00:00, 11.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Training loss: 0.1910438167349075, Val PRAUC: 0.7689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19246263441065442: 100%|██████████| 2224/2224 [07:24<00:00,  5.00it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Training loss: 0.19246263441065442, Val PRAUC: 0.7674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21248835976265035: 100%|██████████| 2224/2224 [07:18<00:00,  5.08it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Training loss: 0.21248835976265035, Val PRAUC: 0.7686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19745928648934732: 100%|██████████| 2224/2224 [08:01<00:00,  4.62it/s]\n",
      "100%|██████████| 544/544 [00:48<00:00, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Training loss: 0.19745928648934732, Val PRAUC: 0.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.28744406703851383: 100%|██████████| 2224/2224 [07:59<00:00,  4.64it/s]\n",
      "100%|██████████| 544/544 [00:55<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Training loss: 0.28744406703851383, Val PRAUC: 0.7692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.27873875669321696: 100%|██████████| 2224/2224 [08:03<00:00,  4.60it/s]\n",
      "100%|██████████| 544/544 [00:49<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Training loss: 0.27873875669321696, Val PRAUC: 0.7693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22060631948070736: 100%|██████████| 2224/2224 [07:23<00:00,  5.02it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Training loss: 0.22060631948070736, Val PRAUC: 0.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2211875746321175: 100%|██████████| 2224/2224 [07:21<00:00,  5.04it/s] \n",
      "100%|██████████| 544/544 [00:45<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Training loss: 0.2211875746321175, Val PRAUC: 0.7675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21826267332896776: 100%|██████████| 2224/2224 [07:24<00:00,  5.00it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Training loss: 0.21826267332896776, Val PRAUC: 0.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19526134620361288: 100%|██████████| 2224/2224 [07:26<00:00,  4.98it/s]\n",
      "100%|██████████| 544/544 [00:44<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Training loss: 0.19526134620361288, Val PRAUC: 0.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2026119061625142: 100%|██████████| 2224/2224 [07:26<00:00,  4.99it/s] \n",
      "100%|██████████| 544/544 [00:45<00:00, 12.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Training loss: 0.2026119061625142, Val PRAUC: 0.7690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24128056124161143: 100%|██████████| 2224/2224 [07:25<00:00,  4.99it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, Training loss: 0.24128056124161143, Val PRAUC: 0.7706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.221472125282259: 100%|██████████| 2224/2224 [07:25<00:00,  5.00it/s]  \n",
      "100%|██████████| 544/544 [00:45<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52, Training loss: 0.221472125282259, Val PRAUC: 0.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21651583959616824: 100%|██████████| 2224/2224 [07:28<00:00,  4.96it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53, Training loss: 0.21651583959616824, Val PRAUC: 0.7712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2398981508837271: 100%|██████████| 2224/2224 [07:27<00:00,  4.97it/s] \n",
      "100%|██████████| 544/544 [00:46<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, Training loss: 0.2398981508837271, Val PRAUC: 0.7711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2202709591540564: 100%|██████████| 2224/2224 [07:30<00:00,  4.93it/s] \n",
      "100%|██████████| 544/544 [00:46<00:00, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Training loss: 0.2202709591540564, Val PRAUC: 0.7725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23969705890707457: 100%|██████████| 2224/2224 [07:28<00:00,  4.96it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56, Training loss: 0.23969705890707457, Val PRAUC: 0.7713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2119968150811147: 100%|██████████| 2224/2224 [07:22<00:00,  5.02it/s] \n",
      "100%|██████████| 544/544 [00:46<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57, Training loss: 0.2119968150811147, Val PRAUC: 0.7720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20444621267863758: 100%|██████████| 2224/2224 [07:16<00:00,  5.10it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58, Training loss: 0.20444621267863758, Val PRAUC: 0.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20279250825933506: 100%|██████████| 2224/2224 [07:15<00:00,  5.10it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 12.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59, Training loss: 0.20279250825933506, Val PRAUC: 0.7726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23962509343566418: 100%|██████████| 2224/2224 [08:57<00:00,  4.14it/s]\n",
      "100%|██████████| 544/544 [01:23<00:00,  6.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60, Training loss: 0.23962509343566418, Val PRAUC: 0.7723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.20205192508226952: 100%|██████████| 2224/2224 [10:32<00:00,  3.52it/s]\n",
      "100%|██████████| 544/544 [00:47<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, Training loss: 0.20205192508226952, Val PRAUC: 0.7726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19791778946605693: 100%|██████████| 2224/2224 [07:29<00:00,  4.95it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Training loss: 0.19791778946605693, Val PRAUC: 0.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2656511314999034: 100%|██████████| 2224/2224 [07:29<00:00,  4.95it/s] \n",
      "100%|██████████| 544/544 [00:46<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63, Training loss: 0.2656511314999034, Val PRAUC: 0.7722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.17504192640642616: 100%|██████████| 2224/2224 [07:28<00:00,  4.96it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64, Training loss: 0.17504192640642616, Val PRAUC: 0.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.15994202639227145: 100%|██████████| 2224/2224 [07:32<00:00,  4.92it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Training loss: 0.15994202639227145, Val PRAUC: 0.7715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1648333914614242: 100%|██████████| 2224/2224 [07:27<00:00,  4.98it/s] \n",
      "100%|██████████| 544/544 [00:46<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66, Training loss: 0.1648333914614242, Val PRAUC: 0.7719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.13525692814589593: 100%|██████████| 2224/2224 [07:24<00:00,  5.00it/s]\n",
      "100%|██████████| 544/544 [00:47<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67, Training loss: 0.13525692814589593, Val PRAUC: 0.7711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22046469213048428: 100%|██████████| 2224/2224 [07:28<00:00,  4.96it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Training loss: 0.22046469213048428, Val PRAUC: 0.7732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19771240577363075: 100%|██████████| 2224/2224 [07:30<00:00,  4.93it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69, Training loss: 0.19771240577363075, Val PRAUC: 0.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.16152382769798265: 100%|██████████| 2224/2224 [07:28<00:00,  4.95it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70, Training loss: 0.16152382769798265, Val PRAUC: 0.7729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2193718015479725: 100%|██████████| 2224/2224 [07:32<00:00,  4.92it/s] \n",
      "100%|██████████| 544/544 [00:47<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71, Training loss: 0.2193718015479725, Val PRAUC: 0.7719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22373552037258046: 100%|██████████| 2224/2224 [07:30<00:00,  4.94it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72, Training loss: 0.22373552037258046, Val PRAUC: 0.7730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21471194562883106: 100%|██████████| 2224/2224 [07:29<00:00,  4.95it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73, Training loss: 0.21471194562883106, Val PRAUC: 0.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2610336570418934: 100%|██████████| 2224/2224 [07:29<00:00,  4.95it/s] \n",
      "100%|██████████| 544/544 [00:47<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74, Training loss: 0.2610336570418934, Val PRAUC: 0.7707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2026405506556108: 100%|██████████| 2224/2224 [07:31<00:00,  4.93it/s] \n",
      "100%|██████████| 544/544 [00:47<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75, Training loss: 0.2026405506556108, Val PRAUC: 0.7726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.22303605636571258: 100%|██████████| 2224/2224 [07:33<00:00,  4.90it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76, Training loss: 0.22303605636571258, Val PRAUC: 0.7731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.24921361295005828: 100%|██████████| 2224/2224 [07:41<00:00,  4.82it/s]\n",
      "100%|██████████| 544/544 [00:47<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77, Training loss: 0.24921361295005828, Val PRAUC: 0.7712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.1502805209450741: 100%|██████████| 2224/2224 [07:29<00:00,  4.95it/s] \n",
      "100%|██████████| 544/544 [00:45<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78, Training loss: 0.1502805209450741, Val PRAUC: 0.7723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.17816704615517293: 100%|██████████| 2224/2224 [07:25<00:00,  4.99it/s]\n",
      "100%|██████████| 544/544 [00:46<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79, Training loss: 0.17816704615517293, Val PRAUC: 0.7716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.26479352117914584: 100%|██████████| 2224/2224 [07:23<00:00,  5.02it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80, Training loss: 0.26479352117914584, Val PRAUC: 0.7727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.2518688727092115: 100%|██████████| 2224/2224 [07:30<00:00,  4.94it/s] \n",
      "100%|██████████| 544/544 [00:45<00:00, 11.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Training loss: 0.2518688727092115, Val PRAUC: 0.7723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.23481248585433223: 100%|██████████| 2224/2224 [07:25<00:00,  4.99it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, Training loss: 0.23481248585433223, Val PRAUC: 0.7723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.19270287113738618: 100%|██████████| 2224/2224 [07:22<00:00,  5.03it/s]\n",
      "100%|██████████| 544/544 [00:45<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83, Training loss: 0.19270287113738618, Val PRAUC: 0.7730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.21479746201523026:  55%|█████▍    | 1215/2224 [04:02<03:21,  5.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/pj20/experiment/gnn_impl copy.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39mdouble()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m train_loop(train_loader\u001b[39m=\u001b[39;49mtrain_loader, val_loader\u001b[39m=\u001b[39;49mval_loader, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, device\u001b[39m=\u001b[39;49mdevice, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/home/pj20/experiment/gnn_impl copy.ipynb Cell 25\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train_loader, val_loader, model, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_loop\u001b[39m(train_loader, val_loader, model, optimizer, device, epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         loss \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# y_true_all, y_prob_all = evaluate(model, device, train_loader)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m# train_pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"macro\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         y_true_all, y_prob_all \u001b[39m=\u001b[39m evaluate(model, device, val_loader)\n",
      "\u001b[1;32m/home/pj20/experiment/gnn_impl copy.ipynb Cell 25\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m pbar:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00mtraining_loss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsunlab-serv-03.cs.illinois.edu/home/pj20/experiment/gnn_impl%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/data.py:262\u001b[0m, in \u001b[0;36mBaseData.to\u001b[0;34m(self, device, non_blocking, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, device: Union[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39m*\u001b[39margs: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    259\u001b[0m        non_blocking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    260\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    263\u001b[0m         \u001b[39mlambda\u001b[39;49;00m x: x\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice, non_blocking\u001b[39m=\u001b[39;49mnon_blocking), \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/data.py:245\u001b[0m, in \u001b[0;36mBaseData.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m store \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstores:\n\u001b[0;32m--> 245\u001b[0m     store\u001b[39m.\u001b[39;49mapply(func, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    246\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/storage.py:183\u001b[0m, in \u001b[0;36mBaseStorage.apply\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies the function :obj:`func`, either to all attributes or only\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39mthe ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems(\u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 183\u001b[0m     \u001b[39mself\u001b[39m[key] \u001b[39m=\u001b[39m recursive_apply(value, func)\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/storage.py:679\u001b[0m, in \u001b[0;36mrecursive_apply\u001b[0;34m(data, func)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecursive_apply\u001b[39m(data: Any, func: Callable) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m--> 679\u001b[0m         \u001b[39mreturn\u001b[39;00m func(data)\n\u001b[1;32m    680\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mPackedSequence):\n\u001b[1;32m    681\u001b[0m         \u001b[39mreturn\u001b[39;00m func(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/kgc/lib/python3.8/site-packages/torch_geometric/data/data.py:263\u001b[0m, in \u001b[0;36mBaseData.to.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto\u001b[39m(\u001b[39mself\u001b[39m, device: Union[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m], \u001b[39m*\u001b[39margs: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    259\u001b[0m        non_blocking: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    260\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Performs tensor device conversion, either for all attributes or\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[39m    only the ones given in :obj:`*args`.\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice, non_blocking\u001b[39m=\u001b[39;49mnon_blocking), \u001b[39m*\u001b[39margs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "in_channels = train_set[0].x.shape[1]\n",
    "out_channels = len(train_set[0].label)\n",
    "\n",
    "device = torch.device('cuda:5' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GAT(in_channels=in_channels, out_channels=out_channels, hidden_channels=512, heads=2).to(device)\n",
    "model = GIN(in_channels=in_channels, out_channels=out_channels, hidden_channels=512).to(device)\n",
    "# model = HGT(in_channels=in_channels, out_channels=out_channels, hidden_channels=512, heads=2).to(device)\n",
    "\n",
    "model.double()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loop(train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer, device=device, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './exp_data/saved_weights_gat_mimic3_drugrec.pkl')\n",
    "torch.save(model.state_dict(), './exp_data/saved_weights_gin_mimic3_drugrec.pkl')\n",
    "# torch.save(model.state_dict(), './exp_data/saved_weights_hgt_mimic3_drugrec.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_all, y_prob_all = evaluate(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.metrics import multilabel_metrics_fn\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# len(y_true_all), len(y_prob_all)\n",
    "pr_auc = average_precision_score(y_true_all, y_prob_all, average=\"samples\")\n",
    "print(pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# graph_all_patients = np.array(triples_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./graph_mimic3_patients.pkl', 'wb') as f:\n",
    "#     pickle.dump(graph_all_patients, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('./graph_mimic3_patients.pkl', 'rb') as f:\n",
    "#     graph_all_patients = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./graphs/cond_proc/CCSCM_CCSPROC/id2ent.json', 'r') as file:\n",
    "#     id2ent = json.load(file)\n",
    "# with open('./graphs/cond_proc/CCSCM_CCSPROC/id2rel.json', 'r') as file:\n",
    "#     id2rel = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import dgl\n",
    "# import numpy as np\n",
    "\n",
    "# patient = graph_all_patients[0]\n",
    "# nodes = set()\n",
    "# for triple in patient:\n",
    "#     h, r, t = triple[0], triple[1], triple[2]\n",
    "#     nodes.add(h)\n",
    "#     nodes.add(t)\n",
    "\n",
    "# node_idx_ori2new = {node : idx for idx, node in enumerate(nodes)}\n",
    "# node_idx_new2ori = {idx: node for node, idx in node_idx_ori2new.items()}\n",
    "# heads = torch.tensor([node_idx_ori2new[head] for head in patient[:, 0]])\n",
    "# tails = torch.tensor([node_idx_ori2new[tail] for tail in patient[:, 2]])\n",
    "\n",
    "# g = dgl.graph((heads, tails), num_nodes=len(nodes))\n",
    "\n",
    "# g.edata['edge_ids'] = torch.from_numpy(patient[:, 1])\n",
    "# g.ndata['node_ids'] = torch.from_numpy(np.array([node_idx_new2ori[i] for i in range(len(nodes))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.ndata['node_ids'][165], g.edata['edge_ids'][0], g.ndata['node_ids'][147], patient[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.ndata['node_ids'][226], g.edata['edge_ids'][1], g.ndata['node_ids'][385], patient[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Torch modules for graph attention networks with fully valuable edges (EGAT).\"\"\"\n",
    "# # pylint: disable= no-member, arguments-differ, invalid-name\n",
    "# import torch as th\n",
    "# from torch import nn\n",
    "# from torch.nn import init\n",
    "\n",
    "# from dgl import function as fn\n",
    "# from dgl.nn.functional import edge_softmax\n",
    "# from dgl.base import DGLError\n",
    "# from dgl.utils import expand_as_pair\n",
    "\n",
    "# # pylint: enable=W0235\n",
    "# class EGATConv(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  in_node_feats,\n",
    "#                  in_edge_feats,\n",
    "#                  out_node_feats,\n",
    "#                  out_edge_feats,\n",
    "#                  num_heads,\n",
    "#                  bias=True):\n",
    "\n",
    "#         super().__init__()\n",
    "#         self._num_heads = num_heads\n",
    "#         self._in_src_node_feats, self._in_dst_node_feats = expand_as_pair(in_node_feats)\n",
    "#         self._out_node_feats = out_node_feats\n",
    "#         self._out_edge_feats = out_edge_feats\n",
    "#         if isinstance(in_node_feats, tuple):\n",
    "#             self.fc_node_src = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_node_feats * num_heads, bias=False)\n",
    "#             self.fc_ni = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_edge_feats*num_heads, bias=False)\n",
    "#             self.fc_nj = nn.Linear(\n",
    "#                 self._in_dst_node_feats, out_edge_feats*num_heads, bias=False)\n",
    "#         else:\n",
    "#             self.fc_node_src = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_node_feats * num_heads, bias=False)\n",
    "#             self.fc_ni = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_edge_feats*num_heads, bias=False)\n",
    "#             self.fc_nj = nn.Linear(\n",
    "#                 self._in_src_node_feats, out_edge_feats*num_heads, bias=False)\n",
    "\n",
    "#         self.fc_fij = nn.Linear(in_edge_feats, out_edge_feats*num_heads, bias=False)\n",
    "#         self.attn = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_edge_feats)))\n",
    "#         if bias:\n",
    "#             self.bias = nn.Parameter(th.FloatTensor(size=(num_heads * out_edge_feats,)))\n",
    "#         else:\n",
    "#             self.register_buffer('bias', None)\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         \"\"\"\n",
    "#         Reinitialize learnable parameters.\n",
    "#         \"\"\"\n",
    "#         gain = init.calculate_gain('relu')\n",
    "#         init.xavier_normal_(self.fc_node_src.weight, gain=gain)\n",
    "#         init.xavier_normal_(self.fc_ni.weight, gain=gain)\n",
    "#         init.xavier_normal_(self.fc_fij.weight, gain=gain)\n",
    "#         init.xavier_normal_(self.fc_nj.weight, gain=gain)\n",
    "#         init.xavier_normal_(self.attn, gain=gain)\n",
    "#         init.constant_(self.bias, 0)\n",
    "\n",
    "#     def forward(self, graph, nfeats, efeats, get_attention=False):\n",
    "#         with graph.local_scope():\n",
    "#             if (graph.in_degrees() == 0).any():\n",
    "#                 raise DGLError('There are 0-in-degree nodes in the graph, '\n",
    "#                                'output for those nodes will be invalid. '\n",
    "#                                'This is harmful for some applications, '\n",
    "#                                'causing silent performance regression. '\n",
    "#                                'Adding self-loop on the input graph by '\n",
    "#                                'calling `g = dgl.add_self_loop(g)` will resolve '\n",
    "#                                'the issue.')\n",
    "\n",
    "#             # calc edge attention\n",
    "#             # same trick way as in dgl.nn.pytorch.GATConv, but also includes edge feats\n",
    "#             # https://github.com/dmlc/dgl/blob/master/python/dgl/nn/pytorch/conv/gatconv.py\n",
    "#             if isinstance(nfeats, tuple):\n",
    "#                 nfeats_src, nfeats_dst = nfeats\n",
    "#             else:\n",
    "#                 nfeats_src = nfeats_dst = nfeats\n",
    "\n",
    "#             f_ni = self.fc_ni(nfeats_src)\n",
    "#             f_nj = self.fc_nj(nfeats_dst)\n",
    "#             f_fij = self.fc_fij(efeats)\n",
    "\n",
    "#             graph.srcdata.update({'f_ni': f_ni})\n",
    "#             graph.dstdata.update({'f_nj': f_nj})\n",
    "#             # add ni, nj factors\n",
    "#             graph.apply_edges(fn.u_add_v('f_ni', 'f_nj', 'f_tmp'))\n",
    "#             # add fij to node factor\n",
    "#             f_out = graph.edata.pop('f_tmp') + f_fij\n",
    "#             if self.bias is not None:\n",
    "#                 f_out = f_out + self.bias\n",
    "#             f_out = nn.functional.leaky_relu(f_out)\n",
    "#             f_out = f_out.view(-1, self._num_heads, self._out_edge_feats)\n",
    "#             # compute attention factor\n",
    "#             e = (f_out * self.attn).sum(dim=-1).unsqueeze(-1)\n",
    "#             graph.edata['a'] = edge_softmax(graph, e)\n",
    "#             graph.srcdata['h_out'] = self.fc_node_src(nfeats_src).view(-1, self._num_heads,\n",
    "#                                                              self._out_node_feats)\n",
    "#             # calc weighted sum\n",
    "#             graph.update_all(fn.u_mul_e('h_out', 'a', 'm'),\n",
    "#                              fn.sum('m', 'h_out'))\n",
    "\n",
    "#             h_out = graph.dstdata['h_out'].view(-1, self._num_heads, self._out_node_feats)\n",
    "#             if get_attention:\n",
    "#                 return h_out, f_out, graph.edata.pop('a')\n",
    "#             else:\n",
    "#                 return h_out, f_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dgl\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# # from dgl.nn.pytorch import EGATConv\n",
    "\n",
    "# class GNNModel(nn.Module):\n",
    "#     def __init__(self, node_feat_dim, edge_feat_dim, num_classes, node_feats, edge_feats, training):\n",
    "#         super(GNNModel, self).__init__()\n",
    "#         self.node_feat_dim = node_feat_dim\n",
    "#         self.edge_feat_dim = edge_feat_dim\n",
    "#         self.num_classes = num_classes\n",
    "#         self.training = training\n",
    "        \n",
    "#         # Define node and edge feature embeddings\n",
    "#         self.node_feat_emb = nn.Embedding.from_pretrained(torch.from_numpy(node_feats).double(), freeze=True)\n",
    "#         self.edge_feat_emb = nn.Embedding.from_pretrained(torch.from_numpy(edge_feats).double(), freeze=True)\n",
    "        \n",
    "#         # Define GNN layers with attention mechanism\n",
    "#         self.gnn_layers = nn.ModuleList()\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim, \n",
    "#                                         in_edge_feats=edge_feat_dim, \n",
    "#                                         out_node_feats=node_feat_dim, \n",
    "#                                         out_edge_feats=edge_feat_dim, \n",
    "#                                         num_heads=3\n",
    "#                                         ))\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim, \n",
    "#                                         in_edge_feats=edge_feat_dim, \n",
    "#                                         out_node_feats=node_feat_dim, \n",
    "#                                         out_edge_feats=edge_feat_dim, \n",
    "#                                         num_heads=3, \n",
    "#                                         ))\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim + edge_feat_dim + node_feat_dim,\n",
    "#                                         in_edge_feats=edge_feat_dim,\n",
    "#                                         out_node_feats=node_feat_dim,\n",
    "#                                         out_edge_feats=edge_feat_dim,\n",
    "#                                         num_heads=3,\n",
    "#                                         ))\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim, \n",
    "#                                         in_edge_feats=edge_feat_dim, \n",
    "#                                         out_node_feats=node_feat_dim, \n",
    "#                                         out_edge_feats=edge_feat_dim, \n",
    "#                                         num_heads=3, \n",
    "#                                         ))\n",
    "#         self.gnn_layers.append(EGATConv(in_node_feats=node_feat_dim, \n",
    "#                                         in_edge_feats=edge_feat_dim, \n",
    "#                                         out_node_feats=node_feat_dim, \n",
    "#                                         out_edge_feats=edge_feat_dim, \n",
    "#                                         num_heads=3, \n",
    "#                                         ))\n",
    "        \n",
    "#         # Define attention mechanism\n",
    "#         self.attn_node = nn.Linear(node_feat_dim, 1, bias=False)\n",
    "#         self.attn_edge = nn.Linear(edge_feat_dim, 1, bias=False)\n",
    "        \n",
    "#         # Define final output layer\n",
    "#         self.output_layer = nn.Linear(node_feat_dim, num_classes)\n",
    "    \n",
    "#     def forward(self, g, get_attention=False):\n",
    "#         # Get node and edge features\n",
    "#         node_feats = self.node_feat_emb(g.ndata['node_ids']).double()\n",
    "#         edge_feats = self.edge_feat_emb(g.edata['edge_ids']).double()\n",
    "#         print(node_feats.shape, edge_feats.shape)\n",
    "        \n",
    "#         # Pass node and edge features through GNN layers with attention\n",
    "#         attentions = []\n",
    "#         for i in range(len(self.gnn_layers)):\n",
    "#             if i == 2:\n",
    "#                 # Add edge features to node features and perform attention\n",
    "#                 node_feats = torch.cat([node_feats, edge_feats, node_feats], dim=-1)\n",
    "#                 node_feats = F.dropout(node_feats, p=0.5, training=self.training)\n",
    "#                 attn_scores = self.attn_edge(edge_feats) + self.attn_node(node_feats)\n",
    "#                 attn_scores = torch.softmax(attn_scores, dim=1)\n",
    "#                 attn_scores = F.dropout(attn_scores, p=0.2, training=self.training)\n",
    "#                 edge_feats = edge_feats * attn_scores\n",
    "#                 node_feats, edge_feats = self.gnn_layers[i](g, node_feats, edge_feats)\n",
    "#                 node_feats = node_feats + edge_feats\n",
    "#                 attentions.append(attn_scores)\n",
    "#             else:\n",
    "#                 # Perform GNN layer and attention\n",
    "#                 node_feats, edge_feats = self.gnn_layers[i](g, node_feats, edge_feats)\n",
    "#                 node_feats = F.dropout(node_feats, p=0.5, training=self.training)\n",
    "#                 attn_scores = self.attn_node(node_feats)\n",
    "#                 attn_scores = torch.softmax(attn_scores, dim=1)\n",
    "#                 attn_scores = F.dropout(attn_scores, p=0.2, training=self.training)\n",
    "#                 node_feats = node_feats * attn_scores\n",
    "#                 attentions.append(attn_scores)\n",
    "        \n",
    "#         # Compute final output probabilities with sigmoid activation\n",
    "#         output_feats = self.output_layer(node_feats)\n",
    "#         output_probs = torch.sigmoid(output_feats)\n",
    "        \n",
    "#         if get_attention:\n",
    "#             return output_probs, attentions\n",
    "#         else:\n",
    "#             return output_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./graphs/cond_proc/CCSCM_CCSPROC/entity_embedding.pkl', 'rb') as f:\n",
    "#     ent_emb = pickle.load(f)\n",
    "# with open('./graphs/cond_proc/CCSCM_CCSPROC/relation_embedding.pkl', 'rb') as f:\n",
    "#     rel_emb = pickle.load(f)\n",
    "\n",
    "# ent_emb.shape, rel_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_feat_dim = ent_emb.shape[-1]\n",
    "# edge_feat_dim = rel_emb.shape[-1]\n",
    "# num_classes = len(sample_dataset.get_all_tokens('drugs'))\n",
    "# node_feats = ent_emb\n",
    "# edge_feats = rel_emb\n",
    "\n",
    "# gnn = GNNModel(node_feat_dim, edge_feat_dim, num_classes, node_feats, edge_feats, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(dgl)\n",
    "\n",
    "# g = dgl.add_self_loop(g)\n",
    "# gnn.double()\n",
    "# gnn(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('kgc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d0509d9aa81f2882b18eeb72d4d23c32cae9029e9b99f63cde94ba86c35ac78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
